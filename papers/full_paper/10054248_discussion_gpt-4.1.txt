**Discussion**

The experiments and analyses conducted in this work yield several insights regarding the inclusion of forgetting mechanisms in collaborative filtering (CF) systems. The general aim was to address persistent challenges in scalability and predictive accuracy as systems contend with increasing volumes of user-item interaction data.

**Adaptivity and Responsiveness to Concept Drift**

Our results with synthetic datasets (ART1, ART2)—characterized by abrupt changes in the underlying distributions—clearly show that forgetting mechanisms (both sliding windows and fading factors) enable faster adaptation of CF algorithms to sudden concept drift. Without forgetting, algorithms that aggregate user interactions over the entire history remain insensitive to new patterns, leading to prolonged periods of diminished recall following an abrupt shift. Sliding windows and fading factors, by restricting the effective memory of the CF algorithms, ensure older, potentially obsolete interactions are discounted, thus facilitating more rapid alignment with current user preferences. The rate of adaptation can be controlled via the window size or the fading factor α, with smaller windows or lower α values inducing faster forgetting but potentially at the cost of lower long-term accuracy in the absence of concept drift.

**Impact on Natural Datasets**

The results with real-world datasets (ELEARN and MUSIC) are more nuanced. Moderate levels of forgetting (e.g., fading factors with α between 0.97 and 0.99) improved the system's responsiveness in the face of induced abrupt changes, particularly for user-based algorithms. On the other hand, for item-based algorithms and in more stable intervals, excessive forgetting tended to reduce recall, especially when the underlying user-item associations evolved slowly or remained relatively stationary. This highlights an important trade-off: while aggressive forgetting accelerates adaptation to genuine drifts, it may hamstring predictive performance when a significant portion of historical data remains relevant.

**Comparison of Forgetting Mechanisms**

Sliding windows offer a simple, interpretable mechanism for restricting the system's memory to recent interactions, but can lead to sudden loss of valuable information if the window is not carefully sized. Fading factors, by contrast, allow for a more gradual and tunable discounting of old data, often resulting in smoother transitions and, as observed, can be more flexibly tailored to the rate of expected change in user preferences. The simplicity of implementation, as well as the computational and memory efficiency, varies: sliding windows demand recurrent rebuilding of similarity matrices and storage of windowed session data, while fading factors—especially when coupled with incremental updates—limit both memory usage and per-update computational cost by capitalizing on the sparsity and gradual decay of matrix entries.

**Scalability**

A key finding, in line with prior work {{15830350}}, is that incremental algorithms significantly outperform nonincremental (batch) algorithms in scalability, especially when extended with forgetting mechanisms. Incremental approaches that utilize fading factors mitigate the cost of matrix updates by leveraging the sparser structures of current and decayed similarities/frequencies. As observed in experiments with simulated Netflix data, and as dataset scale increases, the computational advantages of forgetting become more pronounced. However, in relatively small or stationary datasets, such benefits are less visible.

**Practical Considerations and Limitations**

Selecting appropriate forgetting parameters (window size or fading factor) is highly data- and context-dependent. Overly aggressive forgetting may neglect stable, informative patterns, compromising long-term accuracy, whereas conservative forgetting risks poor adaptation to genuine preference shifts. Notably, much of the evaluation is limited to abrupt (rather than gradual) concept drift due to both data availability and experimental design. Real-world preference evolution is often gradual; future work should examine how dynamic adjustment of forgetting rates, or hybrid approaches, may optimize performance across a variety of drift scenarios.

One technical limitation observed was the challenge of memory usage and matrix sizes as item or user counts become very large—especially for item-based methods on datasets with high item diversity, leading to memory swapping and instability, as seen with the MUSIC dataset. Further optimizations in data structures (e.g., leveraging more advanced sparse matrix representations {{44515378}}) may ameliorate such issues.

**Conclusions and Future Directions**

Overall, our work confirms that forgetting mechanisms—particularly when integrated with incremental CF algorithms—effectively navigate the trade-off between scalability and adaptivity. They not only enable faster model updates and reduce memory demands, but also improve predictive performance in dynamic environments. Nevertheless, optimal implementation depends on dataset characteristics and requires careful parameter tuning. Future research should focus on (1) developing adaptive forgetting schemes that automatically adjust to data dynamics, (2) evaluating performance in scenarios of gradual drift, and (3) extending the scale of experiments, particularly leveraging more efficient sparse data representations and parallel computation frameworks.

These conclusions provide actionable guidance for practitioners seeking to deploy scalable, adaptive recommender systems in contexts where user interests and item pools evolve continuously.