**Discussion**

The results presented here establish that a classical polynomial-time verifier can efficiently and reliably verify measurement-based quantum computations performed by a set of polynomially many, spatially separated quantum provers, each limited to a single, simple measurement operation. This is accomplished through the integration of robust self-testing protocols for graph states and measurement observables, with the universal computational power of measurement-based quantum computation (MBQC) as developed in cluster- and graph-state models {{6197709}}, {{8839192}}.

**Context and Comparison to Prior Work**

Compared to classical interactive proofs for quantum computations, the problem considered here imposes both minimal quantum requirements per individual prover and minimal assumptions on their internal workings, while allowing the classical verifier to remain entirely classical aside from orchestrating the measurement choices and collecting outcomes. Prior work has demonstrated that general interactive proofs for BQP exist but typically require quantum or highly powerful (e.g., PSPACE-complete) provers or multi-prover models with entanglement {{32614901}}, {{15800770}}. Recent advances leveraging self-testing through Bell-type correlations or rigidity (e.g., Mayers-Yao tests, CHSH-type inequalities) have been crucial in verifying non-classical behavior in adversarial settings {{16531904}}, {{44939389}}. Reichardt et al.'s result with two non-communicating provers and a classical verifier establishes the possibility of verifying general BQP computations with constant number of provers, yet each running in polynomial time and requiring elaborate gate teleportation protocols.

In contrast, our construction demonstrates that one can trade a larger number of extremely simple, non-interacting provers (each performing a single measurement) for verifier simplicity and composability of the test. The use of MBQC as the underlying model offers a direct mapping from the quantum circuit being outsourced to a specific measurement pattern, performed in a straightforward manner on a resource state (a universal graph state, e.g., cluster or triangular lattice state). Our error analysis shows that the honest provers, holding the correct graph state and able to perform X-Z plane measurements, succeed in passing both computation and self-tests with high (constant) probability. Critically, the self-testing framework ensures that any sufficiently dishonest deviation by the provers (in state preparation or measurement implementation) leads to a statistically significant decrease in the verifier’s acceptance probability, thereby guaranteeing soundness for the interactive proof.

**Self-Testing, Error Bounds, and Robustness**

A pivotal contribution of this work is the development and refinement of robust self-testing methods for graph states and X-Z plane measurements, along with tight error bounds that guarantee only polynomially many repetitions are needed to achieve constant (and tunably small) completeness and soundness errors. The adaptation of self-testing to triangular cluster states (each vertex covered by a triangle) leverages their favorable structure, enabling efficient anti-commutation checks and high-precision estimation of stabilizer operator correlations. The extension to adaptive measurements—a necessity for universal MBQC—required a generalized analysis (Lemma 6) to bound how errors in individual measurement observables propagate through an adaptive measurement sequence; this analysis assures that errors remain controlled and polynomially suppressed, even for complex, adaptive protocols.

Furthermore, the error bounds established here are conservative and, as discussed in the text, amenable to further optimization. In practical terms, this means that the honest-prover acceptance probability can be made arbitrarily close to 1, while a polynomial blowup in the number of provers or verification rounds ensures a constant (usually small) probability of soundness error, all achievable within polynomial resources.

**Completeness, Soundness, and Parallelizability**

The presented interactive protocol achieves completeness and soundness errors that can be independently controlled. Furthermore, the protocol's modularity allows either sequential or parallel repetition (with fresh randomness per run), affording practical flexibility in implementation—whether in parallelized cloud computing or sequential device testing. Importantly, the structure of the protocol ensures that at no point do the provers receive information distinguishing between ‘test’ and ‘calculation’ queries, thus eliminating loopholes for targeted cheating strategies.

**Conceptual and Practical Implications**

From a conceptual perspective, this interactive proof system furnishes a method for a classical user (or verifier) to both leverage quantum computational advantage and empirically certify quantum hardware or services, even when they themselves cannot efficiently simulate quantum behavior. This has direct applicability to the development of quantum cloud computing infrastructure: clients with minimal trusted resources can reliably verify outsourced quantum computations performed by large-scale quantum processors, provided those processors can be appropriately partitioned and isolated for the purposes of the protocol.

Moreover, the protocol has significant experimental and foundational value. In principle, it serves as a falsifiable protocol for the quantum formalism in large systems—classical users (or experimentalists) can test non-classical predictions even in scenarios where classical simulation is utterly infeasible, provided they can enforce spatial separation and independence of simple quantum devices.

**Relation to Cryptographic Self-Testing and Composability**

The robust self-testing foundations employed here are closely related to those that underlie device-independent quantum cryptography (DIQKD) and other cryptographic certification tasks {{44939389}}, {{16531904}}. The equivalence, up to local isometries plus possible complex conjugation, provides strong composability properties: as in DIQKD, these self-tests guarantee not just that the devices behave as expected in the average case, but that they cannot deviate significantly in any direction not accounted for by transformations invisible to the classical verifier. The protocol’s independence from any fixed Hilbert space dimension, and its immunity to “side-channel” attacks enabled by local complex conjugation, are critical for security and scientific rigor.

**Limitations and Outlook**

While this protocol achieves broad universality with only polynomial overhead, several limitations and avenues for future improvement remain. The polynomial overhead in the number of provers and scale of the test, while far preferable to exponential scaling, still presents a challenge for near-term implementation, particularly for very large computations or experimental systems. Further tightening the error bounds and associated constants, as suggested in the text, would enhance practical feasibility.

Additionally, extending these self-testing methods to more general or arbitrary graph states, reducing the number of required provers, or adapting to scenarios where the independence or spatial separation between provers is not perfectly enforceable, are all important avenues for future work. As noted in the conclusion, adaptation of these protocols to two-prover settings or to bipartite graph states may close the gap with protocols requiring fewer but more powerful or more interactive provers.

Finally, questions remain regarding the optimal separation of completeness and soundness achievable in such proof systems, and the class of quantum computations that can be efficiently and reliably verified under different restrictions on the verifier and prover models.

**Conclusion**

In summary, this work advances the state of the art in interactive proofs for quantum computations by harnessing the full power of measurement-based quantum computing, robust self-testing of graph states and measurements, and careful composability of error and soundness analyses. It provides a practical and conceptually transparent path for classical polynomialsized verifiers to reliably certify the correctness of quantum computations performed by multiple simple, non-communicating quantum devices, and opens multiple promising directions for experimental application, theory development, and the foundations of quantum information science.