Predictive abilityUsing the ELEARN dataset, forgetting mechanisms provide clear improvements in recall immediately after a sudden drift, except with IBFF, where slight improvements are only obtained with high fading factors (slow forgetting).However, the same is not observable in any case with the MUSIC dataset.With this dataset, immediately after the occurrence of a drift, improvements are not observed, but relative degradation is also not present.This contradicts results obtained with synthesized datasets that suggest a better capability to adapt to sudden drifts.A number of factors can influence the behavior of the algorithms, namely the presence of natural drifts and the natural variability of the datasets.This motivates further research to understand how dataset inherent features such as natural variability and the occurrence of sudden and gradual drifts relate to forgetting parameters such as window length and fading factor values.

Update timeWith nonincremental algorithms (UBSW/UBGW and IBSW/IBGW), the first observation is that the sliding window algorithms tend to maintain an approximately constant time to rebuild the matrix, while with growing window algorithms time increases throughout the experiment.This is a natural consequence of the use of fixed length windows.The number of sessions to process, in the case of UBSW and IBSW, is fixed, which leads to approximately constant time.Gains in scalability are the most evident motivation for the use of sliding windows.

Also, with nonincremental algorithms, it is noticeable that time is steadier with the user-based versions than with the item-based versions.One explanation for this is that both datasets have a sequential unique session per user, which Time with the use of fading factors in incremental algorithms seems to be unaffected with ELEARN and MU-SIC, but using a longer dataset, it becomes more evident that fading factors also have the potential to improve scalability.With fading factors, older similarity values tend to zero, but it takes a large amount of sessions for similarities to actually become zero.The package spam optimizes sparse matrix storage by storing only values greater than 2.220446 × 10 −16 {{44515378}}.For example, with α = 0.97, a similarity value of 0.5, if never updated by recent data, only gets completely "forgotten" (i.e., becomes zero) after 1,170 sessions, which is more than the number of sessions in both ELEARN and MUSIC.

User-based algorithms produce smaller matrices when the number of users is lower than the number of items and larger matrices when the number of users exceeds the number of items.Time to rebuild and/or update these matrices is directly affected by the amount of data to process.This explains why user-based algorithms perform better with the MUSIC dataset, while item-based algorithms have better results with ELEARN.

