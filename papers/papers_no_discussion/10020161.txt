
Introduction

Alignments in parallel corpora provide a straightforward basis for the extraction of paraphrases by means of re-translating pivots and then ranking the obtained set of candidates. For example, if the German verb aufsteigen is aligned with the English pivot verbs rise and climb up, and the two English verbs are in turn aligned with the German verbs aufsteigen, ansteigen and hochklettern, then ansteigen and hochklettern represent two paraphrase candidates for the German verb aufsteigen. {{15728911}} were the first to apply this method to gather paraphrases for individual words and multi-word expressions, using translation probabilities as criteria for ranking the obtained paraphrase candidates.
This standard re-translation approach however suffers from a major re-translation sense problem, because the paraphrase candidates cannot distinguish between the various senses of the target word or phrase. Consequently, (i) the different senses of the original word or phrase are merged,  when the back translations of all pivot words are collected within one set of paraphrase candidates; and (ii) the ranking step does not guarantee that all senses of a target are covered by the top-ranked candidates, as more frequent senses amass higher translation probabilities and are favoured.
Recently, {{11942888}} proposed two approaches to distinguish between paraphrase senses (i.e., aiming to solve problem (i) above). In this paper, we address both facets (i) and (ii) of the re-translation sense problem, while focusing on an emprically challenging class of multi-word expressions, i.e., German particle verbs (PVs). German PVs can appear morphologically joint or separated (such as steigt . . . auf ), and are often highly ambiguous. For example, the 138 PVs we use in this paper have an average number of 5.3 senses according to the Duden 1 dictionary. Table 1 illustrates the re-translation sense problem for German PVs. It lists the 10 top-ranked paraphrases for the target verb ausrichten obtained with the standard method. Four synonyms in the 10 top-ranked candidates were judged valid according to the Duden, covering three out of five senses listed in the Duden. Synonyms for a fourth sense "to tell" (sagen,übermitteln, weitergeben) existed in the candidate list, but were ranked low.
Our approach to incorporate word senses into the standard paraphrase extraction applies a graph-based clustering to the set of paraphrase candidates, based on a method described in {{13272287}}{{718563}}. It divides the set of candidates into clusters by reducing edges in an originally fully-connected graph to those exceeding a dynamic similarity threshold. The resulting clusters are taken as paraphrase senses, and different parameters from the graphical clustering (such as connectedness in clusters; cluster centroid positions; etc.) are supposed to enhance the paraphrase ranking step. With this setting, we aim to achieve higher precision in the top-ranked candidates, and to cover a wider range of senses as the original re-translation method. {{15728911}} introduced the idea of extracting paraphrases with the retranslation method. Their work controls for word senses regarding specific test sentences, but not on the type level. Subsequent approaches improved the basic re-translation method, including {{2755801}} who restrict paraphrases by syntactic type; and Wittmann et al. (2014) who add distributional similarity between paraphrase candidate and target word as a ranking feature. Approaches that applied extracted paraphrases relying on the re-translation method include the evaluation of SMT {{16241846}} and query expansion in Q-A systems {{2713391}}.

Related Work

Most recently, {{11942888}} proposed two clustering algorithms to address one of the sense problems: They discriminate between target word senses, exploiting hierarchical graph factorization clustering and spectral clustering. The approaches cluster all words in the Paraphrase Database {{6067240}} and focus on English nouns in their evaluation.
A different line of research on synonym extraction has exploited distributional models, by relying on the contextual similarity of two words or phrases, e.g. {{not_in_s2orc}}, van der Plas and Tiedemann (2006), {{7747235}}, {{1588782}}. Typically, these methods do not incorporate word sense discrimination.
Synonym Extraction Pipeline

This section lays out the process of extracting, clustering and ranking synonym candidates.
Synonym Candidate Extraction

Following the basic approach for synonym extraction outlined by {{15728911}}, we gather all translations (i.e., pivots) of an input particle verb, and then re-translate the pivots. The back translations constitute the set of synonym candidates for the target particle verb.
In order to rank the candidates according to how likely they represent synonyms, each candidate is assigned a probability. The synonym probability p(e 2 |e 1 ) e2 =e1 for a synonym candidate verb e 2 given a target particle verb e 1 is calculated as the product of two translation probabilities: the pivot probability p(f i |e 1 ), i.e. the probability of the English pivot f i being a translation of the particle verb e 1 , and the return probability p(e 2 |f i ), i.e. the probability that the synonym candidate e 2 is a translation of the English pivot f i . The final synonym score for e 2 is the sum over all pivots f 1..n that re-translate into the candidate:
The translation probabilities are based on relative frequencies of the counts in a parallel corpus, cf. section 4.1.
Filtering We apply filtering heuristics at the pivot probability step and the return probability step: obviously useless pivots containing only stop-words (e.g. articles) or punctuation are discarded. In the back-translation step, synonym candidates that did not include a verb are removed. Furthermore, we removed pivots (pivot probability step) and synonym candidates (return probability step) consisting only of light verbs, due to their lack of semantic content and tendency to be part of multi-word expressions. If left unfiltered, light verbs often become super-nodes in the graphs later on (see section 3.2) due to their high distributional similarity with a large number of other synonym candidates. This makes it difficult to partition the graphs into meaningful clusters with the algorithm used here.
Distributional Similarity We add distributional information as an additional feature for the ranking of synonym candidates, because weighting the score from equation (1) by simple multiplication with the distributional similarity between the candidate and the target (as obtained from large corpus data, cf. section 4.1), has been found to improve the ranking {{1998986}}.
Properties of the clusters: C(#(cand)) number of synonym candidates in a cluster C(av-sim(cand,c)) average distributional similarity between synonym candidates in a cluster and the cluster centroid C(av(#(e))) average number of edges in the clusters of the cluster analyses C(#(e)) total number of edges in a cluster C(av-sim(cand,v)) average distributional similariy between synonym candidates in a cluster and the target PV C(av-sim(cand,gc)) average distributional similariy between all synonym candidates and the global centroid C(sim(c,v)) distributional similarity between a cluster centroid and the target PV C(con) connectedness of a cluster Properties of the synonym candidates: S(tr) translation probability of a synonym candidate S(#(e)) number of edges of a synonym candidate S(cl%(#(e))) proportion of cluster edges for a synonym candidate S(sim(cand,v)) distributional similarity between a synonym candidate and the target PV S(sim(cand,c)) distributional similarity between a synonym candidate and the cluster centroid S(sim(cand,gc)) distributional similarity between a synonym candidate and the global centroid Table 2: Properties of synonym candidates and clusters.
Graph-Based Clustering of Candidates

The clustering algorithm suggested by {{718563}} is adopted for clustering all extracted synonym candidates for a specific particle verb target. In a first step, a fully connected undirected graph of all synonym candidates is created as a starting point, with nodes corresponding to synonym candidates and edges connecting two candidates; edge weights are set according to their distributional similarity. In a second step, a similarity threshold is calculated, in order to delete edges with weights below the threshold. The threshold is initialized with the mean value between all edge weights in the fully connected graph. Subsequently, the threshold is updated iteratively:
1. The synonym candidate pairs are partitioned into two groups: P 1 contains pairs with similarities below the current threshold, and P 2 contains pairs with similarities above the current threshold and sharing at least one pivot.
A new threshold is set: T =

A P 1 +A P 2 2 , where A P i is the mean over all similarities in P i .
After convergence, the resulting graph consists of disconnected clusters of synonym candidates. Singleton clusters are ignored. The sub-graphs represent the cluster analysis to be used in the ranking of synonyms for the target particle verb.
Iterative Application of Clustering Algorithm

Because the resulting clusterings of the synonym candidates typically contain one very large (and many small) clusters, we extend the original algorithm and iteratively re-apply the clustering: After one pass of the clustering algorithm as described above (T 1 ), the resulting set of connected synonym candidates becomes the input to another iteration of the algorithm (T 2...n ). Each iteration of the algorithm results in a smaller and more strongly partitioned sub-graph of the initially fully connected graph because the similarity threshold for edges becomes successively higher.
Synonym Candidate Ranking

Assuming that clusters represent senses, we hypothesize that combining properties of individual synonym candidates with properties of the graphbased clusters of synonym candidates results in a ranking of the synonym candidates that overcomes both facets of the re-translation sense problem: Including synonym candidates from various clusters should ensure more senses of the target particle verbs in the top-ranked list; and identifying salient clusters should improve the ranking. Table 2 lists the properties of the individual synonym candidates S and the properties of the graph-based cluster analyses C that we consider potentially useful. For the experiments in section 4, we use all combinations of S and C properties.
Experiments, Results and Discussion

Data and Evaluation

For the extraction of synonym candidates, we use the German-English version of Europarl (1.5M parallel sentences) with GIZA++ word alignments for the extraction of synonym candidates. In the alignments, the German data is lemmatized and reordered in order to treat split occurrences of particle and verb as a single word {{441443}}{{1540169}} (2) S(tr) · S(sim(cand,v)) · C(av-sim(cand,gc)) 38.26 27.90 2.04 46.89 5 clustering + ranking (3) S(tr) · S(sim(cand,v)) 38.19 27.90 2.04 46.89 6 clustering + ranking (4) S(tr) · S(sim(cand,v)) · C(sim(cand,v)) 38.12 27.90 2.04 46.89 7 clustering + ranking (5) S(tr) · S(sim(cand,v)) · C(con) 37.97 27.83 2.03 46.65 Table 3: Evaluation of basic approaches and best five rankings: precision & no./proportion of senses.
The distributional similarity sim is determined by cosine similarities between vectors relying on co-occurrences in a window of 20 words. We use the German web corpus DECOW14AX {{7987482}}{{51844671}} containing 12 billion tokens, with the 10,000 most common nouns as vector dimensions. The feature values are calculated as Local Mutual Information (LMI), cf. {{not_in_s2orc}}.
Our dataset contains the same 138 German particle verbs from Europarl as in previous work {{1998986}}, all PVs with a frequency f ≥ 15 and at least 30 synonyms listed in the Duden dictionary. For the evaluation, we also rely on the Duden, which provides synonyms for the target particle verbs and groups the synonyms by word sense. We consider four evaluation measures, and compare the ranking formulas by macro-averaging each of the evaluation measures over all 138 particle verbs:
• Precision among the 10/20 top-ranked synonym candidates.
• Number and proportion of senses represented among the 10 top-ranked synonyms.

Results

The basic system (line 1 in table 3) only relies on the translation probabilities (S(tr)). It is extended by incorporating the distributional similarity between the target particle verb and the synonym candidates (line 2). Our five best rankings with one iteration of graphical clustering (T 1 ) are shown in lines 3-7. All of these include the translation probability and the distributional similarity between candidate and particle verb; only one makes use of cluster information. Thus, the simple distributional extension is so powerful that additional cluster information cannot improve the system any further. The most relevant cluster measure is the number of edges of the cluster C(#(e)), an indication of cluster size and connectedness.
While the best three clustering systems 2 outperform the extended basic system (line 2) in terms of top-10/top-20 precision, none of the improvements is significant. {{not_in_s2orc}} Also, the number and proportion of senses remain the same as in the basic approach with distributional extension. Further iterations of the clustering step (T 2...n ) up to n = 8 lead to increasingly worse precision scores and sense detection, cf. figure 1 for T 1...5 .


CITED_PAPERS:


11942888:Clustering Paraphrases by Word Sense


Introduction

Many natural language processing tasks rely on the ability to identify words and phrases with equivalent meaning but different wording. These alternative ways of expressing the same information are called paraphrases. Several research efforts have produced automatically generated databases of English paraphrases, including DIRT (Lin and Pantel, 2001), the Microsoft Research Paraphrase Phrase Tables (Dolan et al., 2004), and the Paraphrase Database (Ganitkevitch et al., 2013;Pavlick et al., 2015a). A primary benefit of these automatically generated resources is their enormous scale, which provides superior coverage compared to manually compiled resources like WordNet (Miller, 1995). But automatically generated paraphrase resources currently have the drawback that they group all senses of polysemous words together, and do not partition paraphrases into groups like WordNet does with its synsets. Thus a search for paraphrases of the noun bug would yield a single list of paraphrases that includes insect, glitch, beetle, error, microbe, wire, cockroach, malfunction, microphone, mosquito, virus, tracker, pest, informer, snitch, parasite, bacterium, fault, mistake, failure and many others. The goal of this work is to group these paraphrases into clusters that denote the distinct senses of the input word or phrase, as shown in Figure 1.
We develop a method for clustering the paraphrases from the Paraphrase Database (PPDB). PPDB contains over 100 million paraphrases generated using the bilingual pivoting method (Bannard and Callison-Burch, 2005), which posits that two English words are potential paraphrases of each other if they share one or more foreign translations. We apply two clustering algorithms, Hierarchical Graph Factorization Clustering (Yu et al., 2005;Sun and Korhonen, 2011) and Self-Tuning Spectral Clustering (Ng et al., 2001;Zelnik-Manor and Perona, 2004), and systematically explore different ways of defining the similarity matrix that they use as input. We exploit a variety of features from PPDB to cluster its paraphrases by sense, including its im-  Figure 2: SEMCLUST connects all paraphrases that share foreign alignments, and cuts edges below a dynamically-tuned cutoff weight (dotted lines). The resulting connected components are its clusters.
plicit graph structure, aligned foreign words, paraphrase scores, predicted entailment relations, and monolingual distributional similarity scores. Our goal is to determine which algorithm and features are the most effective for clustering paraphrases by sense. We address three research questions:
• Which similarity metric is best for sense clustering? We systematically compare different ways of defining matrices that specify the similarity between pairs of paraphrases.
• Are better clusters produced by comparing second-order paraphrases? We use PPDB's graph structure to decide whether mosquito and pest belong to the same sense cluster by comparing lists of paraphrases for the two words.
• Can entailment relations inform sense clustering? We exploit knowledge like beetle is-an insect, and that there is no entailment between malfunction and microbe.
Our method produces sense clusters that are qualitatively and quantitatively good, and that represent a substantial improvement to the PPDB resource.

Related Work

The paraphrases in PPDB are already partitioned by syntactic type, following the work of Callison-Burch (2008). He showed that applying syntactic constraints during paraphrase extraction via the pivot method improves paraphrase quality. This means that paraphrases of the noun bug are separated from paraphrases of the verb bug, which consist of verbs like bother, trouble, annoy, disturb, and others. However, organizing paraphrases this way still leaves the issue of mixed senses within a single part of speech. This lack of sense distinction makes it difficult to decide when a paraphrase in PPDB would be an appropriate substitute for a word in a given sentence. Some researchers resort to crowdsourcing to determine when a PPDB substitution is valid (Pavlick et al., 2015c). Our sense clustering work is closely related to the task of word sense induction (WSI), which aims to discover all senses of a target word from large corpora. One family of common approaches to WSI aims to discover the senses of a word by clustering the monolingual contexts in which it appears (Navigli, 2009). Another uncovers a word's senses by clustering its foreign alignments from parallel corpora (Diab, 2003). A more recent family of approaches to WSI represents a word as a feature vector of its substitutable words, i.e. paraphrases (Melamud et al., 2015;Yatbaz et al., 2012). In this paper we take inspiration from each of these families of approaches, and we explore them when measuring word similarity in sense clustering.
The work most closely related to ours is that of Apidianaki et al. (2014), who used a simple graphbased approach to cluster pivot paraphrases on the basis of contextual similarity and shared foreign alignments. Their method represents paraphrases as nodes in a graph and connects each pair of words sharing one or more foreign alignments with an edge weighted by contextual similarity. Concretely, for paraphrase set P , it constructs a graph G = (V, E) where vertices V = {p i ∈ P } are words in the paraphrase set and edges connect words that share foreign word alignments in a bilingual parallel corpus. The edges of the graph are weighted based on their contextual similarity (computed over a monolingual corpus). In order to partition the graph into clusters, edges in the initial graph G with contextual similarity below a threshold T are deleted. The connected components in the resulting graph G are taken as the sense clusters. The threshold is dynamically tuned using an iterative procedure (Apidianaki and He, 2010).
As evaluated against reference clusters derived from SEMEVAL 2007 Lexical Substitution gold data (McCarthy and Navigli, 2007), their method, which we call SEMCLUST, outperformed simple most-frequent-sense, one-sense-per-paraphrase, and random baselines. Apidianaki et al. (2014)'s work corroborated the existence of sense distinctions in the paraphrase sets, and highlighted the need for further work to organize them by sense. In this paper, we improve on their method using more advanced clustering algorithms, and by systematically exploring a wider range of similarity measures.

Graph Clustering Algorithms

To partition paraphrases by sense, we use two advanced graph clustering methods rather than using Apidianaki et al. (2014)'s edge deletion approach. Both of them allow us to experiment with a variety of similarity metrics.

Hierarchical Graph Factorization Clustering

The Hierarchical Graph Factorization Clustering (HGFC) method was developed by Yu et al. (2006) to probabilistically partition data into hierarchical clusters that gradually merge finer-grained clusters into coarser ones. Sun and Korhonen (2011) applied HGFC to the task of clustering verbs into Levin (1993)-style classes. Sun and Korhonen extended the basic HGFC algorithm to automatically discover the latent tree structure in their clustering solution and incorporate prior knowledge about semantic relationships between words. They showed that HGFC far outperformed agglomerative clustering methods on their verb data set. We adopt Sun and Korhonen's implementation of HGFC for our experiments. HGFC takes as input a nonnegative, symmetric adjacency matrix W = {w ij } where rows and columns represent paraphrases p i ∈ P , and entries w ij denote the similarity between paraphrases sim D (p i , p j ). The algorithm works by factorizing W into a bipartite graph, where the nodes on one side represent paraphrases, and nodes on the other represent senses. The output of HGFC is a set of clusterings of increasingly coarse granularity, which we can also represent with a tree structure. The algo-rithm automatically determines the number of clusters at each level. For our task, this has the benefit that a user can choose the cluster granularity most appropriate for the downstream task (as illustrated in Figure 5). Another benefit of HGFC is that it probabilistically assigns each paraphrase to a cluster at each level of the hierarchy. If some p i has high probability in multiple clusters, we can assign p i to all of them ( Figure 3c).

Spectral Clustering

The second clustering algorithm that we use is Self-Tuning Spectral Clustering (Zelnik-Manor and Perona, 2004). Like HGFC, spectral clustering takes an adjacency matrix W as input, but the similarities end there. Whereas HGFC produces a hierarchical clustering, spectral clustering produces a flat clustering with k clusters, with k specified at runtime. The Zelnik-Manor and Perona (2004)'s selftuning method is based on Ng et al. (2001)'s spectral clustering algorithm, which computes a normalized Laplacian matrix L from the input W , and executes K-means on the largest k eigenvectors of L. Intuitively, the largest k eigenvectors of L should align with the k senses in our paraphrase set.

Similarity Measures

Each of our clustering algorithms take as input an adjacency matrix W where the entries w ij correspond to some measure of similarity between words i and j. For the paraphrases in Figure 1, W is a 20x20 matrix that specifies the similarity of every pair of paraphrases like microbe and bacterium or microbe and malfunction. We systematically investigated four types of similarity scores to populate W .

Paraphrase Scores

Bannard and Callison-Burch (2005) defined a paraphrase probability in order to quantify the goodness of a pair of paraphrases, based on the underlying translation probabilities used by the bilingual pivoting method. More recently, (Pavlick et al., 2015a) used supervised logistic regression to combine a variety of scores so that they align with human judgements of paraphrase quality. PPDB 2.0 provides this score for each pair of words in the database. The PPDB 2.0 score is a nonnegative real number that   can be used directly as a similarity measure:
PPDB 2.0 does not provide a score for a word with itself, so we set P P DB 2.0 Score(i, i) to be the maximum P P DB 2.0 Score(i, j) such that i and j have the same stem.

Second-Order Paraphrase Scores

Work by Rapp (2003) and Melamud et al. (2015) showed that comparing words on the basis of their shared paraphrases is effective for WSI. We define two novel similarity metrics that calculate the similarity of words i and j by comparing their secondorder paraphrases. Instead of comparing microbe and bacterium directly with their PPDB 2.0 score, we look up all of the paraphrases of microbe and all of the paraphrases of bacterium, and compare those two lists. Specifically, we form notional word-paraphrase feature vectors v p i and v p j where the features correspond to words with which each is connected in PPDB, and the value of the k th element of v p i equals P P DB 2.0 Score(i, k). We can then calculate the cosine similarity or Jensen-Shannon divergence between vectors: The value of vector element v ij is P P DB 2.0 Score(i, j).
where JS(v p i , v p j ) is calculated assuming that the paraphrase probability distribution for word i is given by its normalized word-paraphrase vector v p i .

Similarity of Foreign Word Alignments

When an English word is aligned to several foreign words, sometimes those different translations indicate a different word sense (Yao et al., 2012). Using this intuition, Gale et al. (1992) trained an English WSD system on a bilingual corpus, using the different French translations as labels for the English word senses. For instance, given the English word duty, the French translation droit was a proxy for its tax sense and devoir for its obligation sense. PPDB is derived from bilingual coropra. We recover the aligned foreign words and their associated translation probabilities that underly each PPDB entry. For each English word in our dataset, we get each foreign word that it aligns to in the Spanish and Chinese bilingual parallel corpora used by Ganitkevitch and Callison-Burch (2014). We use this to define a novel foreign word alignment similarity metric, sim T RAN S (i, j) for two English paraphrases i and j. This is calculated as the cosine similarity of the word-alignment vectors v a i and v a j where each feature in v a is a foreign word to which i or j aligns, and the value of entry v a if is the translation probability p(f |i).

Monolingual Distributional Similarity

Lastly, we populate the adjacency with a distributional similarity measure based on WORD2VEC (Mikolov et al., 2013). Each paraphrase i in our data set is represented as a 300-dimensional WORD2VEC embedding v w i trained on part of the Google News dataset. Phrasal paraphrases that did not have an entry in the WORD2VEC dataset are represented as the mean of their individual word vectors. We use the cosine similarity between WORD2VEC embeddings as our measure of distributional similarity.

Determining the Number of Senses

The optimal number of clusters for a set of paraphrases will vary depending on how many senses there ought to be for an input word like bug. It is generally recognized that optimal sense granularity depends on the application (Palmer et al., 2001). WordNet has notoriously fine-grained senses, whereas most word sense disambiguation systems achieve better performance when using coarse-grained sense inventories (Navigli, 2009). Depending on the task, the sense clustering for query word coach in Figure 5b with k = 5 clusters may be preferable to the alternative with k = 3 clusters. An ideal algorithm for our task would enable clustering at varying levels of granularity to support different downstream NLP applications.
Both of our clustering algorithms can produce sense clusters at varying granularities. For HGFC this requires choosing which level of the resulting tree structure to take as a clustering solution, and for spectral clustering we must specify the number of clusters prior to execution. 1 To determine the optimal number of clusters, we use the mean Silhouette Coefficient (Rousseeuw, 1987) which balances optimal inter-cluster tightness and intra-cluster distance. The Silhouette Coefficient is calculated for each paraphrase p i as
where a(p i ) is p i 's average intra-cluster distance (average distance from p i to each other p j in the same cluster), and b(p i ) is p i 's lowest average intercluster distance (distance from p i to the nearest external cluster centroid). For each clustering algorithm, we choose as the 'solution' the clustering which produces the highest mean Silhouette Coefficient. The Silhouette Coefficient calculation takes as input a matrix of pairwise distances, so we simply use 1 − W where the adjacency matrix W is calculated using one of the similarity methods we defined. Pavlick et al. (2015b) added a set of automatically predicted semantic entailment relations for each entry in PPDB 2.0. The entailment types that they include are Equivalent, Forward Entailment, Reverse Entailment, Exclusive, and Independent. While a negative entailment relationship (Exclusive or Independent) does not preclude words from belonging to the same sense of some query word, a positive entailment relationship (Equivalent, Forward/Reverse Entailment) does give a strong indication that the words belong to the same sense.

Incorporating Entailment Relations

We seek a straightforward way to determine whether entailment relations provide information that is useful to the final clustering algorithm. Both of our algorithms take an adjacency matrix W as input, so we add entailment information by simply 1 For spectral clustering there has been significant study into methods for automatically determining the optimal number of clusters, including analysis of eigenvalues of the graph Laplacian, and finding the rotation of the Laplacian that brings it closest to block-diagonal (Zelnik-Manor and Perona, 2004). We experimented with these and other cluster analysis methods such as the Dunn Index (Dunn, 1973) in our work, but found that using the simple Silhouette Coefficient produced clusterings that were competitive with the more intensive methods, in far less time.  multiplying each pairwise entry by its entailment probability. Specifically, we set
where p ind (i, j) gives the PPDB 2.0 probability that there is an Independent entailment relationship between words i and j. Intuitively, this should increase the similarity of words that are very likely to be entailing like fault and failure, and decrease the similarity of non-entailing words like cockroach and microphone.

Experimental Setup

We follow the experimental setup of Apidianaki et al. (2014). We focus our evaluation on a set of query words drawn from the LexSub test data (McCarthy and Navigli, 2007), plus 16 additional handpicked polysemous words.

Gold Standard Clusters

One challenge in creating our clustering methodology is that there is no reliable PPDB-sized standard against which to assess our results. WordNet synsets provide a well-vetted basis for comparison, but only allow us to evaluate our method on the 38% of our PPDB dataset that overlaps it. We therefore evaluate performance on two test sets.
WordNet+ Our first test set is designed to assess how well our solution clusters align with WordNet synsets. We chose 185 polysemous words from the SEMEVAL 2007 dataset and an additional 16 handpicked polysemous words. For each we formed a paraphrase set that was the intersection of their PPDB 2.0 XXXL paraphrases with their WordNet synsets, and their immediate hyponyms and hypernyms. Each reference cluster consisted of a Word-Net synset, plus the hypernyms and hyponyms of words in that synset. On average there are 7.2 reference clusters per paraphrase set.
CrowdClusters Because the coverage of Word-Net is small compared to PPDB, and because Word-Net synsets are very fine-grained, we wanted to create a dataset that would test the performance of our clustering algorithm against large, noisy paraphrase sets and coarse clusters. For this purpose we randomly selected 80 query words from the SEMEVAL 2007 dataset and created paraphrase sets from their unfiltered PPDB2.0 XXL entries. We then iteratively organized each paraphrase set into reference senses with the help of crowd workers on Amazon Mechanical Turk. On average there are 4.0 reference clusters per paraphrase set. A full description of our method is included in the supplemental materials.

Evaluation Metrics

We evaluate our method using two standard metrics: the paired F-Score and V-Measure. Both were used in the 2010 SemEval Word Sense Induction Task (Manandhar et al., 2010) and by Apidianaki et al. (2014). We give our results in terms of weighted average performance on these metrics, where the score for each individual paraphrase set is weighted by the number of reference clusters for that query word.
Paired F-Score frames the clustering problem as a classification task (Manandhar et al., 2010). It gen-erates the set of all word pairs belonging to the same reference cluster, F (S), and the set of all word pairs belonging to the same automatically-generated cluster, F (K). Precision, recall, and F-score can then be calculated in the usual way, i.e. P = F (K)∩F (S)
, and F = 2·P ·R P +R . V-Measure assesses the quality of a clustering solution against reference clusters in terms of clustering homogeneity and completeness (Rosenberg and Hirschberg, 2007). Homogeneity describes the extent to which each cluster is composed of paraphrases belonging to the same reference cluster, and completeness refers to the extent to which points in a reference cluster are assigned to a single cluster. Both are defined in terms of conditional entropy. V-Measure is the harmonic mean of homogeneity h and completeness c; V-Measure = 2·h·c h+c .

Baselines

We evaluate the performance of HGFC on each dataset against the following baselines:
Most Frequent Sense (MFS) assigns all paraphrases p i ∈ P to a single cluster. By definition, the completeness of the MFS clustering is 1.
One Cluster per Paraphrase (1C1PAR) assigns each paraphrase p i ∈ P to its own cluster. By definition, the homogeneity of 1C1PAR clustering is 1.

Random (RAND)

For each query term's paraphrase set, we generate five random clusterings of k = 5 clusters. We then take F-Score and V-Measure as the average of each metric calculated over the five random clusterings.
SEMCLUST We implement the SEMCLUST algorithm (Apidianaki et al., 2014) as a state-ofthe-art baseline. Since PPDB contains only pairs of words that share a foreign word alignment, in our implementation we connect paraphrase words with an edge if the pair appears in PPDB. We adopt the WORD2VEC distributional similarity score sim DIST RIB for our edge weights.
8 Experimental Results Figure 6 shows the performance of the two advanced clustering algorithms against the baselines. Our best configurations 2 for HGFC and Spectral outperformed all baselines except 1C1PAR V-Measure, which his biased toward solutions with many small clusters (Manandhar et al., 2010), and performed only marginally better than SEMCLUST in terms of F-Score alone. The dominance of 1C1PAR V-Measure is greater for the WordNet+ dataset which has smaller reference clusters than CrowdClusters. Qualitatively, we find that methods that strike a balance between high F-Score and high V-Measure tend to produce the 'best' clusters by human judgement. If we consider the average of F-Score and V-Measure as a comprehensive performance measure, our methods outperform all baselines.  Table 1: Average performance and number of clusters produced by our different similarity methods.
On our dataset, the state-of-the-art SEMCLUST baseline tended to lump many senses of the query word together, and produced scores lower than in the original work. We attribute this to the fact that the original work extracted paraphrases from Eu-roParl, which is much smaller than PPDB, and thus created adjacency matrices W which were sparser than those produced by our method. Directly applied, SEMCLUST works well on small data sets, but does not scale well to the larger, noisier PPDB data. More advanced graph-based clustering methods produce better sense clusters for PPDB.
The first question we sought to address with this work was which similarity metric is the best for sense clustering. Table 1 reports the average F-Score and V-Measure across 40 test configurations for each similarity calculation method. 3 On average across test sets and clustering algorithms, the paraphrase similarity score (P P DB 2.0 Score) performs better than monolingual distributional similarity (sim DIST RIB ) in terms of F-Score, but the results are reversed for V-Measure. This is also shown in the best HGFC and Spectral configurations, where the two similarity scores are swapped between them.
Next, we investigated whether comparing secondorder paraphrases would produce better clusters than simply using P P DB 2.0 Score directly. Table 1 also compares the two methods that we had for computing the similarity of second order paraphrases -cosine similarity (sim P P DB.cos ) and Jensen-Shannon divergence (sim P P DB.JS ). On average across test sets and clustering algorithms, using the direct paraphrase score gives stronger V-Measure and F-score than the second-order methods. It also produces 3 Our Supplementary Materials file provides the full set of results for all 200 configurations that we tested. coarser clusters than the second-order PPDB similarity methods.
Finally, we investigated whether incorporating automatically predicted entailment relations would improve cluster quality, and we found that it did. All other things being equal, adding entailment information increases F-Score by .014 and V-Measure by .020 on average (Figure 7). Adding entailment information had the greatest improvement to HGFC methods with sim DIST RIB similarities, where it improved F-Score by an average of .03 and V-Measure by an average of .05.

Discussion and Future Work

We have presented a novel method for clustering paraphrases in PPDB by sense. When evaluated against WordNet synsets, the sense clusters produced by the Spectral Clustering algorithm give a 64% relative improvement in F-Score over the closest baseline, and those produced by the HGFC algorithm give a 50% improvement in F-Score. We systematically analyzed a variety of similarity metrics as input to HGFC and Spectral Clustering, and showed that incorporating predicted entailment relations from PPDB boosts the performance of sense clustering.
Our sense clustering provides a significant improvement to the PPDB resource that may improve its applicability to downstream NLP tasks. One possible application of sense-clustered PPDB entries is the lexical substitution task, which seeks to identify appropriate word substitutions. Given a target word in context, it would be reasonable to suggest substitutes from the target word's PPDB sense cluster most closely related to the target context. There are many possible ways to choose the best cluster for a given context, ranging from simply choosing the cluster whose members have highest average pointwise mutual information with the context, to a more complex approach based on training cluster representations using a pseudo-word approach as in Melamud et al. (2015). We leave this application for future work.

Software and Data Release

With publication of this paper we are releasing paraphrase clusters for all PPDB 2.0 XXL entries, clustering code, and an interface for crowdsourcing paraphrase clusters using Amazon Mechanical Turk.

Supplementary Material

Our Supplementary Material provides additional detail on our similarity metric calculation, clustering algorithm implementation, and CrowdCluster reference cluster data development. We also provide full evaluation results across the entire range of our experiments, a selection of sense clusters output by our methods, and example content of our WordNet+ and CrowdCluster paraphrase sets.

Figure 1 :

1Our goal is to partition paraphrases of an input word like bug into clusters representing its distinct senses.
graph for query word bug. Wider lines signify stronger similarity.

Figure 3 :

3The graph, corresponding adjacency matrix W , and bipartite graph created by the first iteration of HGFC for query word bug (n)

Figure 4 :

4Comparing second-order paraphrases for malfunction and fault based on word-paraphrase vectors.

Figure 5 :

5HGFC and Spectral Clustering results for coach (n). Our silhouette optimization sets k = 3.

Figure 6 :

6Hierarchical Graph Factorization Clustering and Spectral Clustering both significantly outperform all baselines except 1C1PAR V-Measure.

Figure 7 :

7Histogram of metric change by adding entailment information across all experiments.
(b) The corresponding adjacency matrix W .Darker cells signify stronger similarity.insect 
beetle 
mosquito 
cockroach
pest 
parasite 
microbe 
virus 
bacterium 
glitch 
error 
malfunction
fault 
mistake 
failure 
microphone
wire 
tracker 
informer 
snitch
insect 
beetle 
mosquito 
cockroach pest 
parasite microbe virus 
bacterium glitch error 
malfunction fault 
mistake failure 
microphone wire 
tracker 
informer snitch
insect
mosquito 
cockroach 
beetle 
parasite 
microbe 
bacterium 
virus 
glitch 
error 
failure 
fault 
mistake 
malfunction 
microphone 
wire 
tracker 
informer 
snitch
pest
insect, mosquito, pest 
cockroach 
beetle 
parasite 
microbe, bacterium 
virus 
glitch 
error, failure, fault, mistake 
malfunction 
microphone, wire 
wire, tracker, informer, snitch
(c) The bipartite graph induced by the 
first iteration of HGFC. Note wire is 
assigned to two clusters.
Our top-scoring Spectral method, Spectral*, uses entailments, P P DB2.0Score similarities, and simDIST RIB to choose k. Our best HGFC method, HGFC*, uses entailments, simDIST RIB similarities, and P P DB2.0Score to choose k.
AcknowledgmentsThis research was supported by the Allen Institute for Artificial Intelligence (AI2), the Human Language Technology Center of Excellence, and by gifts from the Alfred P. Sloan Foundation, Google, and Facebook. This material is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government.We would like to thank Marianna Apidianaki and Alex Harelick for sharing code used in this research, and Ellie Pavlick for her substantive input. We are grateful to our anonymous reviewers for their thoughtful and constructive comments.
An algorithm for cross-lingual sense clustering tested in a MT evaluation setting. Marianna Apidianaki, Yifan He, Proceedings of the 7th International Workshop on Spoken Language Translation (IWSLT-10). the 7th International Workshop on Spoken Language Translation (IWSLT-10)Marianna Apidianaki and Yifan He. 2010. An algorithm for cross-lingual sense clustering tested in a MT eval- uation setting. In Proceedings of the 7th International Workshop on Spoken Language Translation (IWSLT- 10).
Semantic clustering of pivot paraphrases. Marianna Apidianaki, Emilia Verzeni, Diana Mc-Carthy, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC. the Ninth International Conference on Language Resources and Evaluation (LRECMarianna Apidianaki, Emilia Verzeni, and Diana Mc- Carthy. 2014. Semantic clustering of pivot para- phrases. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014).
Paraphrasing with bilingual parallel corpora. Colin Bannard, Chris Callison-Burch, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 05). the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 05)Colin Bannard and Chris Callison-Burch. 2005. Para- phrasing with bilingual parallel corpora. In Proceed- ings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 05).
Syntactic constraints on paraphrases extracted from parallel corpora. Chris Callison, - Burch, Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing. the 2008 Conference on Empirical Methods in Natural Language ProcessingHonolulu, HawaiiOctober. Association for Computational LinguisticsChris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In Pro- ceedings of the 2008 Conference on Empirical Meth- ods in Natural Language Processing, pages 196-205, Honolulu, Hawaii, October. Association for Computa- tional Linguistics.
Word sense disambiguation within a multilingual framework. Mona Talat, Diab , University of MarylandPh.D. thesisMona Talat Diab. 2003. Word sense disambiguation within a multilingual framework. Ph.D. thesis, Uni- versity of Maryland.
Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. William Dolan, Chris Quirk, Chris Brockett, Proceedings of the International Conference of Computational Linguistics. the International Conference of Computational LinguisticsWilliam Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase cor- pora: Exploiting massively parallel news sources. In Proceedings of the International Conference of Com- putational Linguistics (COLING 2004).
A fuzzy relative of the isodata process and its use in detecting compact wellseparated clusters. C Joseph, Dunn, Joseph C Dunn. 1973. A fuzzy relative of the iso- data process and its use in detecting compact well- separated clusters.
Using bilingual materials to develop word sense disambiguation methods. William A Gale, Kenneth W Church, David Yarowsky, Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation. the Fourth International Conference on Theoretical and Methodological Issues in Machine TranslationWilliam A. Gale, Kenneth W. Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation.
The multilingual paraphrase database. Juri Ganitkevitch, Chris Callison-Burch, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014). the Ninth International Conference on Language Resources and Evaluation (LREC-2014)Reykjavik, IcelandJuri Ganitkevitch and Chris Callison-Burch. 2014. The multilingual paraphrase database. In Proceedings of the Ninth International Conference on Language Re- sources and Evaluation (LREC-2014), Reykjavik, Ice- land, pages 4276-4283.
PPDB: The paraphrase database. Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, Proceedings of NAACL-HLT. NAACL-HLTJuri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of NAACL-HLT 2013.
English verb classes and alternations: A preliminary investigation. Beth Levin, University of Chicago pressBeth Levin. 1993. English verb classes and alternations: A preliminary investigation. University of Chicago press.
Discovery of inference rules for question answering. Dekang Lin, Patrick Pantel, Natural Language Engineering. Dekang Lin and Patrick Pantel. 2001. Discovery of infer- ence rules for question answering. Natural Language Engineering.
SemEval-2010 Task 14: Word sense induction & disambiguation. Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach, Sameer Pradhan, Proceedings of the Fifth International Workshop on Semantic Evaluations. the Fifth International Workshop on Semantic EvaluationsSemEval-2010Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach, and Sameer Pradhan. 2010. SemEval-2010 Task 14: Word sense induction & disambiguation. In Proceed- ings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010).
Semeval-2007 task 10: English lexical substitution task. Diana Mccarthy, Roberto Navigli, Proceedings of the Fourth International Workshop on Semantic Evaluations. the Fourth International Workshop on Semantic EvaluationsDiana McCarthy and Roberto Navigli. 2007. Semeval- 2007 task 10: English lexical substitution task. In Pro- ceedings of the Fourth International Workshop on Se- mantic Evaluations (SemEval-2007).
Modeling word meaning in context with substitute vectors. Oren Melamud, Ido Dagan, Jacob Goldberger, Human Language Technologies: The. Oren Melamud, Ido Dagan, and Jacob Goldberger. 2015. Modeling word meaning in context with substitute vectors. In Human Language Technologies: The 2015
Annual Conference of the North American Chapter of the ACL. Annual Conference of the North American Chapter of the ACL.
Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, Proceedings of NIPS. NIPSTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS.
WordNet: a lexical database for English. A George, Miller, Communications of the ACM. 3811George A Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39-41.
Word sense disambiguation: A survey. Roberto Navigli, ACM Computing SurveysRoberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys.
On spectral clustering: Analysis and an algorithm. Advances in Neural Information Processing Systems. Andrew Ng, Michael Jordan, Y Weiss, Andrew Ng, Michael Jordan, and Y. Weiss. 2001. On spectral clustering: Analysis and an algorithm. Ad- vances in Neural Information Processing Systems.
Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Martha Palmer, Hoa Trang Dang, Christiane Fellbaum, Natural Language Engineering. Martha Palmer, Hoa Trang Dang, and Christiane Fell- baum. 2001. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering.
PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification. Ellie Pavlick, Johan Bos, Malvina Nissim, Charley Beller, Benjamin Van Durme, Chris Callison-Burch, Proceedings of the 53rd. the 53rdEllie Pavlick, Johan Bos, Malvina Nissim, Charley Beller, Benjamin Van Durme, and Chris Callison- Burch. 2015a. PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification. In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics (ACL 2015).
Adding semantics to data-driven paraphrasing. Ellie Pavlick, Johannes Bos, Malvina Nissim, Charley Beller, Chris Callison-Burch Benjamin Van Durme, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL. the 53rd Annual Meeting of the Association for Computational Linguistics (ACLEllie Pavlick, Johannes Bos, Malvina Nissim, Charley Beller, and and Chris Callison-Burch Benjamin Van Durme. 2015b. Adding semantics to data-driven paraphrasing. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis- tics (ACL 2015).
FrameNet+: Fast paraphrastic tripling of FrameNet. Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi, Chris Callison-Burch, Mark Drezde, Benjamin Van Durme, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015). the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)Beijing, ChinaAssociation for Computational LinguisticsEllie Pavlick, Travis Wolfe, Pushpendre Rastogi, Chris Callison-Burch, Mark Drezde, and Benjamin Van Durme. 2015c. FrameNet+: Fast paraphrastic tripling of FrameNet. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis- tics (ACL 2015), Beijing, China, July. Association for Computational Linguistics.
Word sense discovery based on sense descriptor dissimilarity. Reinhard Rapp, Proceedings of the Ninth Machine Translation Summit. the Ninth Machine Translation SummitReinhard Rapp. 2003. Word sense discovery based on sense descriptor dissimilarity. In Proceedings of the Ninth Machine Translation Summit, pages 315-322.
Vmeasure: A conditional entropy-based external cluster evaluation measure. Andrew Rosenberg, Julia Hirschberg, EMNLP-CoNLL. 7Andrew Rosenberg and Julia Hirschberg. 2007. V- measure: A conditional entropy-based external clus- ter evaluation measure. In EMNLP-CoNLL, volume 7, pages 410-420.
Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. J Peter, Rousseeuw, Journal of computational and applied mathematics. 20Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analy- sis. Journal of computational and applied mathemat- ics, 20:53-65.
Hierarchical verb clustering using graph factorization. Lin Sun, Anna Korhonen, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsLin Sun and Anna Korhonen. 2011. Hierarchical verb clustering using graph factorization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1023-1033. Association for Computational Linguistics.
Expectations of word sense in parallel corpora. Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, The 2012 Conference of the North American Chapter of the Association for Computational Linguistics. Montréal, CanadaAssociation for Computational LinguisticsXuchen Yao, Benjamin Van Durme, and Chris Callison- Burch. 2012. Expectations of word sense in parallel corpora. In The 2012 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics, pages 621-625, Montréal, Canada, June. As- sociation for Computational Linguistics.
Learning syntactic categories using paradigmatic representations of word context. Enis Mehmet Ali Yatbaz, Deniz Sert, Yuret, Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningAssociation for Computational LinguisticsMehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012. Learning syntactic categories using paradigmatic rep- resentations of word context. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natu- ral Language Processing and Computational Natural Language Learning, pages 940-951. Association for Computational Linguistics.
Soft clustering on graphs. Kai Yu, Shipeng Yu, Volker Tresp, Advances in neural information processing systems. Kai Yu, Shipeng Yu, and Volker Tresp. 2005. Soft clus- tering on graphs. In Advances in neural information processing systems, pages 1553-1560.
Self-tuning spectral clustering. Lihi Zelnik, - Manor, Pietro Perona, Advances in neural information processing systems. Lihi Zelnik-Manor and Pietro Perona. 2004. Self-tuning spectral clustering. In Advances in neural information processing systems, pages 1601-1608.


1588782:A Structured Vector Space Model for Word Meaning in Context


Introduction

Semantic spaces are a popular framework for the representation of word meaning, encoding the meaning of lemmas as high-dimensional vectors. In the default case, the components of these vectors measure the co-occurrence of the lemma with context features over a large corpus. These vectors are able to provide a robust model of semantic similarity that has been used in NLP (Salton et al., 1975;McCarthy and Carroll, 2003;Manning et al., 2008) and to model experimental results in cognitive science (Landauer and Dumais, 1997;McDonald and Ramscar, 2001). Semantic spaces are attractive because they provide a model of word meaning that is independent of dictionary senses and their much-discussed problems (Kilgarriff, 1997;McCarthy and Navigli, 2007).
In a default semantic space as described above, each vector represents one lemma, averaging over all its possible usages (Landauer and Dumais, 1997;Lund and Burgess, 1996). Since the meaning of words can vary substantially between occurrences (e.g., for polysemous words), the next necessary step is to characterize the meaning of individual words in context.
There have been several approaches in the literature (Smolensky, 1990;Schütze, 1998;Kintsch, 2001;McDonald and Brew, 2004;Mitchell and Lapata, 2008) that compute meaning in context from lemma vectors. Most of these studies phrase the problem as one of vector composition: The meaning of a target occurrence a in context b is a single new vector c that is a function (for example, the centroid) of the vectors: c = a b.
The context b can consist of as little as one word, as shown in Example (1). In (1a), the meaning of catch combined with ball is similar to grab, while in (1b), combined with disease, it can be paraphrased by contract. Conversely, verbs can influence the interpretation of nouns: In (1a), ball is understood as a spherical object, and in (1c) as a dancing event.
(1) a. catch a ball b. catch a disease c. attend a ball
In this paper, we argue that models of word meaning relying on this procedure of vector composition are limited both in their scope and scalability. The underlying shortcoming is a failure to consider syntax in two important ways. The syntactic relation is ignored. The first problem concerns the manner of vector composition, which ignores the relation between the target a and its context b. This relation can have a decisive influence on their interpretation, as Example (2) shows:
(2) a. a horse draws b. draw a horse In (2a), the meaning of the verb draw can be paraphrased as pull, while in (2b) it is similar to sketch. This difference in meaning is due to the difference in relation: in (2a), horse is the subject, while in (2b) it is the object. On the modeling side, however, a vector combination function that ignores the relation will assign the same representation to (2a) and (2b). Thus, existing models are systematically unable to capture this class of phenomena. Single vectors are too weak to represent phrases. The second problem arises in the context of the important open question of how semantic spaces can "scale up" to provide interesting meaning representations for entire sentences. We believe that the current vector composition methods, which result in a single vector c, are not informative enough for this purpose. One proposal for "scaling up" is to straightforwardly interpret c = a b as the meaning of the phrase a + b (Kintsch, 2001;Mitchell and Lapata, 2008). The problem is that the vector c can only encode a fixed amount of structural information if its dimensionality is fixed, but there is no upper limit on sentence length, and hence on the amount of structure to be encoded. It is difficult to conceive how c could encode deeper semantic properties, like predicateargument structure (distinguishing "dog bites man" and "man bites dog"), that are crucial for sentencelevel semantic tasks such as the recognition of textual entailment (Dagan et al., 2006). An alternative approach to sentence meaning would be to use the vector space representation only for representing word meaning, and to represent sentence structure separately. Unfortunately, present models cannot provide this grounding either, since they compute a single vector c that provides the same representations for both the meanings of a and b in context.
In this paper, we propose a new, structured vector space model for word meaning (SVS) that addresses these problems. A SVS representation of a lemma comprises several vectors representing the word's lexical meaning as well as the selectional preferences that it has for its argument positions. The meaning of word a in context b is computed by combining a with b's selectional preference vector specific to the relation between a and b, addressing the first problem above. In an expression a + b, the meanings of a and b in this context are computed as two separate vectors a and b . These vectors can then be combined with a representation of the structure's expression (e.g., a parse tree), to address the second problem discussed above. We test the SVS model on the task of recognizing contextually appropriate paraphrases, finding that SVS performs at and above the state-ofthe-art.
Plan of the paper. Section 2 reviews related work. Section 3 presents the SVS model for word meaning in context. Sections 4 to 6 relate experiments on the paraphrase appropriateness task.

Related Work

In this section we give a short overview over existing vector space based approaches to computing word meaning in context.
General context effects. The first category of models aims at integrating the widest possible range of context information without recourse to linguistic structure. The best-known work in this category is Schütze (1998). He first computes "first-order" vector representations for word meaning by collecting co-occurrence counts from the entire corpus. Then, he determines "second-order" vectors for individual word instances in their context, which is taken to be a simple surface window, by summing up all first-order vectors of the words in this context. The resulting vectors form sense clusters.
McDonald and Brew (2004) present a similar model. They compute the expectation for a word w i in a sequence by summing the first-order vectors for the words w 1 to w i−1 and showed that the distance between expectation and first-order vector for w i correlates with human reading times.
Predicate-argument combination. The second category of prior studies concentrates on contexts consisting of a single word only, typically modeling the combination of a predicate p and an argument a. Kintsch (2001) uses vector representations of p and a to identify the set of words that are similar to both p and a. After this set has been narrowed down in a self-inhibitory network, the meaning of the predicateargument combination is obtained by computing the centroid of its members' vectors. The procedure does not take the relation between p and a into account. Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p + a as a function f operating on four components:
R is the relation holding between p and a, and K additional knowledge. This framework allows sensitivity to the relation. However, the concrete instantiations that Mitchell and Lapata consider disregards K and R, thus sharing the other models' limitations. They focus instead on methods for the direct combination of p and a: In a comparison between component-wise addition and multiplication of p and a, they find far superior results for the multiplication approach.
Tensor product-based models. Smolensky (1990) uses tensor product to combine two word vectors a and b into a vector c representing the expression a+b.
The vector c is located in a very high-dimensional space and is thus capable of encoding the structure of the expression; however, this makes the model infeasible in practice, as dimensionality rises with every word added to the representation. Jones and Mewhort (2007) represent lemma meaning by using circular convolution to encode n-gram co-occurrence information into vectors of fixed dimensionality. Similar to Brew and McDonald (2004), they predict most likely next words in a sequence, without taking syntax into account.
Kernel methods. One of the main tests for the quality of models of word meaning in context is the ability to predict the appropriateness of paraphrases in given a context. Typically, a paraphrase applies only to some senses of a word, not all, as can be seen in the paraphrases "grab" and "contract" of "catch". Vector space models generally predict paraphrase appropriateness based on the similarity between vectors. This task can also be addressed with kernel methods, which project items into an implicit feature space for efficient similarity computation. Consequently, vector space methods and kernel methods have both been used for NLP tasks based on similarity, notably Information Retrieval and Textual Entailment. Nevertheless, they place their emphasis on different types of information. Current kernels are mostly tree kernels that compare syntactic structure, and use semantic information mostly for smoothing syntactic similarity (Moschitti and Quarteroni, 2008). In contrast, vector-space models focus on the interaction between the lexical meaning of words in composition.

A structured vector space model for word meaning in context

In this section, we define the structured vector space (SVS) model of word meaning.
The main intuition behind our model is to view the interpretation of a word in context as guided by expectations about typical events. For example, in (1a), we assume that upon hearing the phrase "catch a ball", the hearer will interpret the meaning of "catch" to match typical actions that can be performed with a ball. Similarly, the interpretation of "ball" will reflect the hearer's expectations about typical things that can be caught. This move to include typical arguments and predicates into a model of word meaning can be motivated both on cognitive and linguistic grounds.
In cognitive science, the central role of expectations about typical events for human language processing is well-established. Expectations affect reading times (McRae et al., 1998), the interpretation of participles (Ferretti et al., 2003), and sentence processing generally (Narayanan and Jurafsky, 2002;Padó et al., 2006). Expectations exist both for verbs and nouns (McRae et al., 1998;McRae et al., 2005).
In linguistics, expectations, in the form of selectional restrictions and selectional preferences, have long been used in semantic theories (Katz and Fodor, 1964;Wilks, 1975), and more recently induced from corpora (Resnik, 1996;Brockmann and Lapata, 2003). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (Mc-Carthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Recently, a vectorspaced model of selectional preferences has been proposed that computes the typicality of an argument simply through similarity to previously seen arguments (Erk, 2007;.
We first present the SVS model of word meaning Representing lemma meaning. We abandon the traditional choice of representing word meaning as a single vector. Instead, we encode each word as a combination of (a) one vector that models the lexical meaning of the word, and (b) a set of vectors, each of which represents the semantic expectations/selectional preferences for one particular relation that the word supports. 1 The idea is illustrated in Fig. 1. In the representation of the verb catch, the central square stands for the lexical vector of catch itself. The three arrows link it to catch 's preferences for its subjects (subj), its objects (obj), and for verbs for which it appears as a complement (comp −1 ). The figure shows the selectional preferences as word lists for readability; in practice, each selectional preference is a single vector (cf. Section 4). Likewise, ball is represented by one vector for ball itself, one for ball 's preferences for its modifiers (mod), one vector for the verbs of which it is a subject (subj −1 ), and one for the verbs of which is an object (obj −1 ).
This representation includes selectional preferences (like subj, obj, mod) exactly parallel to inverse selectional preferences (subj −1 , obj −1 , comp −1 ). To our knowledge, preferences of the latter kind have not been studied in computational linguistics. However, their existence is supported in psycholinguistics by priming effects from nouns to typical verbs (McRae et al., 2005).
Formally, let D be a vector space (the set of possi-  ble vectors), and let R be some set of relation labels.
In the structured vector space (SVS) model, we represent the meaning of a lemma w as a triple
where v ∈ D is a lexical vector describing the word w itself, R : R → D maps each relation label onto a vector that describes w's selectional preferences, and R −1 : R → D maps from role labels to vectors describing inverse selectional preferences of w. Both R and R −1 are partial functions. For example, the direct object preference would be undefined for intransitive verbs.
Computing meaning in context. The SVS model of lemma meaning permits us to compute the meaning of a word a in the context of another word b in a new way, via their selectional preferences. Let
be the representations of the two words, and let r ∈ R be the relation linking a to b. Then, we define the meaning of a and b in this context as a pair (a , b ) of vectors, where a is the meaning of a in the context of b, and b the meaning of b in the context of a:
where v 1 v 2 is a direct vector combination function as in traditional models, e.g. addition or componentwise multiplication. If either R a (r) or R −1 b (r) are not defined, the combination fails. Afterwards, the argument position r is considered filled, and is deleted from R a and R −1 b . Figure 2 illustrates this procedure on the representations from Figure 1. The dotted lines indicate that the lexical vector for catch is combined with the inverse object preference of ball. Likewise, the lexical vector for ball is combined with the object preference vector of catch.
Note that our procedure for computing meaning in context can be expressed within the framework of Mitchell and Lapata (Eq. (3)). We can encode the expectations of a and b as additional knowledge K. The combined representation c is the pair (a , b ) that is computed according to our model (Eq. (4)).
The SVS scheme we have proposed incorporates syntactic information in a more general manner than previous models, and thus addresses the issues we have discussed in Section 1. Since the representation retains individual selectional preferences for all relations, combining the same words through different relations can (and will in general) result in different adapted representations. For instance, in the case of Example (2), we would expect the inverse subject preference of horse ("things that a horse typically does") to push the lexical vector of draw into the direction of pulling, while its inverse object preference ("things that are done to horses") suggest a different interpretation.
Rather than yielding a single, joint vector for the whole expression, our procedure for computing meaning in context results in one context-adapted meaning representation per word, similar to the output of a WSD system. As a consequence, our model can be combined with any formalism representing the structure of an expression. (The formalism used then determines the set R of relations.) For example, combining SVS with a dependency tree would yield a tree in which each node is labeled by a SVS tuple that represents the word's meaning in context.

Experimental setup

This section provides the background to the following experimental evaluation of SVS, including parameters used for computing the SVS representations that will be used in the experiments.

Experimental rationale

In this paper, we evaluate the SVS model against the task of predicting, given a predicate-argument pair, how appropriate a paraphrase (of either the predicate or the argument) is in that context. We perform two experiments that both use the paraphrase task, but differ in their emphasis. Experiment 1 replicates an existing evaluation against human judgments. This evaluation uses synthetic dataset, limited to one particular construction, and constructed to provide maximally distinct paraphrase candidates. Experiment 2 considers a broader class of constructions along with annotator-generated paraphrase candidates that are not screened for distinctness. In both experiments, we compare the SVS model against the state-of-theart model by Mitchell and Lapata 2008 (henceforth M&L; cf. Sec. 2 for model details).

Parameter choices

Vector space. In our parameterization of the vector space, we largely follow M&L because their model has been rigorously evaluated and found to outperform a range of other models.
Our first space is a traditional "bag-of-words" vector space (BOW, (Lund and Burgess, 1996)). For each pair of a target word and context word, the BOW space records a function of their co-occurrence frequency within a surface window of size 10. The space is constructed from the British National Corpus (BNC), and uses the 2,000 most frequent context words as dimensions.
We also consider a "dependency-based" vector space (SYN, (Padó and Lapata, 2007)). In this space, target and context words have to be linked by a "valid" dependency path in a dependency graph to count as co-occurring. 2 This space was built from BNC dependency parses obtained from Minipar (Lin, 1993).
For both spaces, we used pre-experiments to compare two methods for the computation of vector components, namely raw co-occurrence counts, the standard model, and the pointwise mutual information (PMI) definition employed by M&L.
Selectional preferences. We use a simple, knowledge-lean representation for selectional preferences inspired by Erk (2007), who models selectional preference through similarity to seen filler vectors v a : We compute the selectional preference vector for word b and relation r as the weighted centroid of seen filler vectors v a . We collect seen fillers from the Minipar-parse of the BNC.
Let f (a, r, b) denote the frequency of a occurring in relation r to b in the parsed BNC, then
We call this base model SELPREF. We will also study two variants of SELPREF, based on two different hypotheses about what properties of the selectional preferences are particularly important for meaning adaption. The first model aims specifically at alleviating noise introduced by infrequent fillers, a common problem in data-driven approaches. It only uses fillers seen more often than a threshold θ. We call this model SELPREF-CUT: (6) Our second variant again aims at alleviating noise, but noise introduced by low-valued dimensions rather than infrequent fillers. It achieves this by taking each component of the selectional preference vector to the nth power. In this manner, dimensions with high counts are further inflated, while dimensions with low counts are depressed. 3 This model, SELPREF-POW, is defined as follows:
The inverse selectional preferences R −1 b are defined analogously for all three model variants. We instantiate the vector combination function as component-wise multiplication, following M&L.
Baselines and significance testing. All tasks that we consider below involve judgments for the meaning of a word a in the context of a word b. A first baseline that every model must beat is simply using the original vector for a. We call this baseline "target only". Since we assume that the selectional preferences of b model the expectations for a, we use b's selectional preference vector for the given relation as a second baseline, "selpref only". Differences between the performance of models were tested for significance using a stratified shuffling-based randomization test (Yeh, 2000). 4 .

Exp. 1: Predicting similarity ratings

In our first experiment, we attempt to predict human similarity judgments. This experiment is a replication of the evaluation of M&L on their dataset 5 .
Dataset. The M&L dataset comprises a total of 3,600 human similarity judgements for 120 experimental items. Each item, as shown in Figure 3, consists of an intransitive verb and a subject noun that are combined with a "landmark", a synonym of the verb that is chosen to be either similar or dissimilar to the verb in the context of the given subject.
The dataset was constructed by extracting pairs of subjects and intransitive verbs from a parsed version of the BNC. Each item was paired with two landmarks, chosen to be as dissimilar as possible according to a WordNet similarity measure. All nouns and verbs were subjected to a pretest, where only those with highly significant variations in human judgments across landmarks were retained.
For each item of the final dataset, judgements on a 7-point scale were elicited. For example, judges considered the compatible landmark "slouch" to be much more similar to "shoulder slumps" than the incompatible landmark "decline". In Figure 3, the column sim shows whether the experiment designers considered the respective landmark to have high or low similarity to the verb, and the column judgment shows a participant's judgments.
Experimental procedure. We used cosine to compute similarity to the lexical vector of the landmark. "Target only" compares the landmark against the lexical vector of the verb, and "selpref only" compares it to the noun's subj −1 preference. For the M&L model, the comparison is to the combined lexical vectors of verb and noun. For our models SELPREF, SELPREF-CUT and SELPREF-POW, we combine the verb's lexical vector with the subj −1 preference of the noun. We used a held-out dataset of 10% of the data to optimize the parameters of θ of SELPREF-CUT and n of SELPREF-POW. Vectors with PMI components could model the data, while raw frequency components could not; we report only the former. We use the same two evaluation scores as M&L: The first score is the average similarity to compatible landmarks (high) and incompatible landmarks (low). The second is Spearman's ρ, a nonparametric correlation coefficient. We compute ρ between individual human similarity scores and our predictions. Based on agreement between human judges, M&L estimate an upper bound ρ of 0.4 for the dataset.
Results and discussion. Table 1 shows the results of Exp. 1 on the test set. In the upper half (BOW), we replicate M&L's main finding that simple componentwise multiplication of the predicate and argument vectors results in a highly significant correlation of  Table 2: Experiment 1: Average similarity (and standard deviation) between the inverse subject preferences of a noun and (left) its lexical vector and (right) inverse object preferences vector (cosine similarity in SYN space) ρ = 0.2, significantly outperforming both baselines. It is interesting, though, that the subj −1 preference itself ("Selpref only") is already highly significantly correlated with the human judgments.
A comparison of the upper half (BOW) with the lower half (SYN) shows that the dependency-based space generally shows better correlation with human judgements. This corresponds to a beneficial effect of syntactic information found for other applications of semantic spaces (Lin, 1998;Padó and Lapata, 2007).
All instances of the SELPREF model show highly significant correlations. SELPREF and SELPREF-CUT show very similar performance. They do better than both baselines in the BOW space; however, in the cleaner SYN space, their performance is numerically lower than using selectional preferences only (ρ = 0.13 vs. 0.16). SELPREF-POW is always significantly better than SELPREF and SELPREF-CUT, and shows the best result of all tested models (ρ = 0.27, BOW space). The performance is somewhat lower in the SYN space (ρ = 0.22). However, this difference, and the difference to the best M&L model at ρ = 0.24, are not statistically significant.
The SVS model computes meaning in context by combining a word's lexical representation with the preference vector of its context. In this, it differs from previous models, including that by M&L, which used what we have been calling "direct combination". So it is important to ask to what extent this difference in method translate to a difference in predictions. We analyzed this by measuring the similarity by the nouns' lexical vectors, used by direct combination methods, and their inverse subject preferences, which SVS uses. The result is shown in the first column in Table 2, computed as mean cosine similarities and standard deviations between noun vectors and selectional preferences. The table shows that these vectors have generally low similarity, which is further reduced by applying cutoff and potentiation. Thus, the predictions of SVS will differ from those of direct combination models like M&L.
A related question is whether syntax-aware vector combination makes a difference: Does the model encode different expectations for different syntactic relations (cf. Example 2)? The second column of Table 2 explores this question by comparing inverse selectional preferences for the subject and object slots. We observe that the similarity is very high for raw preferences, but becomes lower when noise is eliminated. Since the SELPREF-POW model performed best in our evaluation, we read this as evidence that potentiation helps to suppress noise introduced by mis-identified subject and object fillers.
In Experiment 1, all experimental items were verbs, which means that all disambiguation was done through inverse selectional preferences. As inverse selectional preferences are currently largely unexplored, it is interesting to note that the evidence that they provide for the paraphrase task is as strong as that of the context nouns themselves.

Exp. 2: Ranking paraphrases

This section reports on a second, more NLP-oriented experiment whose task is to distinguish between appropriate and inappropriate paraphrases on a broader range of constructions.
Dataset. For this experiment, we use the SemEval-1 lexical substitution (lexsub) dataset (McCarthy and Navigli, 2007), which contains 10 instances each of 200 target words in sentential contexts, drawn from Sharoff's (2006) English Internet Corpus. Contextually appropriate paraphrases for each instance of each target word were elicited from up to 6 participants. Fig. 4 shows two instances for the verb to work. The distribution over paraphrases can be seen as a characterization of the target word's meaning in each context.
Experimental procedure. In this paper, we predict appropriate paraphrases solely on the basis of a single context word that stands in a direct predicateargument relation to the target word. We extracted all instances from the lexsub test data with such a relation. After parsing all sentences with verbal and nominal targets with Minipar, this resulted in three

Sentence

Substitutes By asking people who work there, I have since determined that he didn't. (# 2002) be employed 4; labour 1
Remember how hard your ancestors worked. (# 2005) toil 4; labour 3; task 1 Figure 4: Lexical substitution example items for "work" sets of sentences: (a), target intransitive verbs with noun subjects (V-SUBJ, 48 sentences); (b), target transitive verbs with noun objects (V-OBJ, 213 sent.); and (c), target nouns occurring as objects of verbs (N-OBJ, 102 sent.). 6 Note that since we use only part of the lexical substitution dataset in this experiment, a direct comparison with results from the SemEval task is not possible.
As in the original SemEval task, we phrase the task as a ranking problem. For each target word, the paraphrases given for all 10 instances are pooled. The task is to rank the list for each item so that appropriate paraphrases (such as "be employed" for # 2002) rank higher than paraphrases not given (e.g., "toil").
Our model ranks paraphrases by their similarity to the following combinations (Eq. (4)): for V-SUBJ, verb plus the noun's subj −1 preferences; for V-OBJ, verb plus the noun's obj −1 preferences; and for N-OBJ, the noun plus the verb's obj preferences. Our comparison model, M&L, ranks all paraphrases by their similarity to the direct noun-verb combination.
To avoid overfitting, we consider only the two models that performed optimally in in the SYN space in Experiment 1 (SELPREF-POW with n=30 and M&L). However, since we found that vectors with raw frequency components could model the data, while PMI components could not, we only report the former.
For evaluation, we adopt the SemEval "out of ten" precision metric P OOT . It uses the model's ten top-ranked paraphrases as its guesses for appropriate paraphrases. Let G i be the gold paraphrases for item i, M i the model's top ten paraphrases for i, and f (s, i) the frequency of s as paraphrase for i:
McCarthy and Navigli propose this metric for the 6 The specification of this dataset will be made available.  dataset for robustness. Due to the sparsity of paraphrases, a metric that considers fewer guesses leads to artificially low results when a "good" paraphrase was not mentioned by the annotators by chance but is ranked highly by a model.
Results and discussion. Table 6 shows the mean out-of-ten precision for all models. The behavior is fairly uniform across all three datasets. Unsurprisingly, "target only", which uses the same ranking for all instances of a target, yields the worst results. 7
M&L's direct combination model outperforms "target only" significantly (p < 0.05). However, on both the V-SUBJ and the N-OBJ the "selpref only" baseline does better than direct combination. The best results on all datasets are obtained by SELPREF-POW. The difference between SELPREF-POW and the "target only" baseline is highly significant (p < 0.01). The difference to M&L's model is significant at p = 0.05.
We interpret these results as encouraging evidence for the usefulness of selectional preferences for judging substitutability in context. Knowledge about the selectional preferences of a single context word can already lead to a significant improvement in precision. We find this overall effect even though the word is not informative in all cases. For instance, the subject of item 2002 in Fig. 4, "who", presumably helps little in determining the verb's context-adapted meaning.
It is interesting that the improvement of SELPREF-POW over "selpref only" is smallest for the N-OBJ dataset (1.9% P OOT ). N-OBJ uses selectional preferences for nouns that may fill the direct object position, , while V-SUBJ and V-OBJ use inverse selectional preferences for verbs (cf. the two graphs in Fig. 1).

Conclusion

In this paper, we have considered semantic space models that can account for the meaning of word occurrences in context. Arguing that existing models do not sufficiently take syntax into account, we have introduced the new structured vector space (SVS) model of word meaning. In addition to a vector representing a word's lexical meaning, it contains vectors representing the word's selectional preferences. These selectional preferences play a central role in the computation of meaning in context.
We have evaluated the SVS model on two datasets on the task of predicting the felicitousness of paraphrases in given contexts. On the M&L dataset, SVS outperforms the state-of-the-art model of M&L, though the difference is not significant. On the Lexical Substitution dataset, SVS significantly outperforms the state-of-the-art. This is especially interesting as the Lexical Substitution dataset, in contrast to the M&L data, uses "realistic" paraphrase candidates that are not necessarily maximally distinct.
The most important limitation of the evaluation that we have given in this paper is that we have only considered single words as context. Our next step will be to integrate information from multiple relations (such as both the subject and object positions of a verb) into the computation of context-specific meaning. Our eventual aim is a model that can give a compositional account of a word's meaning in context, where all words in an expression disambiguate one another according to the relations between them.
We will explore the usability of vector space models of word meaning in NLP applications, formulated as the question of how to perform inferences on them in the context of the Textual Entailment task (Dagan et al., 2006). Paraphrase-based inference rules play a large role in several recent approaches to Textual Entailment (e.g. Szpektor et al (2008)); appropriateness judgments of paraphrases in context, the task of Experiments 1 and 2 above, can be viewed as testing the applicability of these inferences rules.

Figure 1 :

1Structured meaning representations for noun ball and verb catch : lexical information plus expectations that integrates lexical information with selectional preferences. Then, we show how the SVS model provides a new way of computing meaning in context.

Figure 2 :

2Combining predicate and argument via relationspecific semantic expectations

Table 3 :

3Experiment 2: Mean "out of ten" precision (P OOT )
We do not commit to a particular set of relations; see the discussion at the end of this section.
More specifically, we used the minimal context specification and plain weight function. SeePadó and Lapata (2007).
Since we focus on the size-invariant cosine similarity, the use of this model does not require normalization.
The software is available at http://www.nlpado.de/ ∼ sebastian/sigf.html.5  We thank J. Mitchell and M. Lapata for providing their data.
"Target only" still does very much better than a random baseline, which performs at 22% POOT.
Acknowledgments. Many thanks for helpful discussion to Jason Baldridge, David Beaver, Dedre Gentner, James Hampton, Dan Jurafsky, Alexander Koller, Brad Love, and Ray Mooney.
Evaluating and combining approaches to selectional preference acquisition. C Brockmann, M Lapata, Proceedings of EACL. EACLC. Brockmann, M. Lapata. 2003. Evaluating and combin- ing approaches to selectional preference acquisition. In Proceedings of EACL, 27-34.
The PASCAL Recognising Textual Entailment Challenge. I Dagan, O Glickman, B Magnini, Machine Learning Challenges. SpringerI. Dagan, O. Glickman, B. Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Ma- chine Learning Challenges, Lecture Notes in Computer Science, 177-190. Springer.
A simple, similarity-based model for selectional preferences. K Erk, Proceedings of ACL. ACLK. Erk. 2007. A simple, similarity-based model for selec- tional preferences. In Proceedings of ACL, 216-223.
Thematic role focusing by participle inflections: evidence form conceptual combination. T Ferretti, C Gagné, K Mcrae, Journal of Experimental Psychology. 291T. Ferretti, C. Gagné, K. McRae. 2003. Thematic role fo- cusing by participle inflections: evidence form concep- tual combination. Journal of Experimental Psychology, 29(1):118-127.
Automatic labeling of semantic roles. D Gildea, D Jurafsky, Computational Linguistics. 283D. Gildea, D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245- 288.
Structural ambiguity and lexical relations. D Hindle, M Rooth, Computational Linguistics. 191D. Hindle, M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103- 120.
Representing word meaning and order information in a composite holographic lexicon. M Jones, D Mewhort, Psychological review. 114M. Jones, D. Mewhort. 2007. Representing word mean- ing and order information in a composite holographic lexicon. Psychological review, 114:1-37.
The structure of a semantic theory. J J Katz, J A Fodor, The Structure of Language. Prentice-HallJ. J. Katz, J. A. Fodor. 1964. The structure of a semantic theory. In The Structure of Language. Prentice-Hall.
1997. I don't believe in word senses. A Kilgarriff, Computers and the Humanities. 31A. Kilgarriff. 1997. I don't believe in word senses. Com- puters and the Humanities, 31(2):91-113.
. W Kintsch, Predication. Cognitive Science. 25W. Kintsch. 2001. Predication. Cognitive Science, 25:173-202.
A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. T Landauer, S Dumais, Psychological Review. 1042T. Landauer, S. Dumais. 1997. A solution to Platos prob- lem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psycho- logical Review, 104(2):211-240.
Principle-based parsing without overgeneration. D Lin, Proceedings of ACL. ACLD. Lin. 1993. Principle-based parsing without overgener- ation. In Proceedings of ACL, 112-120.
Automatic retrieval and clustering of similar words. D Lin, Proceedings of COLING-ACL. COLING-ACLD. Lin. 1998. Automatic retrieval and clustering of simi- lar words. In Proceedings of COLING-ACL, 768-774.
Producing high-dimensional semantic spaces from lexical co-occurrence. K Lund, C Burgess, Behavior Research Methods, Instruments, and Computers. 28K. Lund, C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behav- ior Research Methods, Instruments, and Computers, 28:203-208.
Introduction to Information Retrieval. C D Manning, P Raghavan, H Schütze, CUP. C. D. Manning, P. Raghavan, H. Schütze. 2008. Introduc- tion to Information Retrieval. CUP.
Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. D Mccarthy, J Carroll, Computational Linguistics. 294D. McCarthy, J. Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639-654.
SemEval-2007 Task 10: English Lexical Substitution Task. D Mccarthy, R Navigli, Proceedings of SemEval. SemEvalD. McCarthy, R. Navigli. 2007. SemEval-2007 Task 10: English Lexical Substitution Task. In Proceedings of SemEval, 48-53.
A distributional model of semantic context effects in lexical processing. S Mcdonald, C Brew, Proceedings of ACL. ACLS. McDonald, C. Brew. 2004. A distributional model of semantic context effects in lexical processing. In Proceedings of ACL, 17-24.
Testing the distributional hypothesis: The influence of context on judgements of semantic similarity. S Mcdonald, M Ramscar, Proceedings of CogSci. CogSciS. McDonald, M. Ramscar. 2001. Testing the distribu- tional hypothesis: The influence of context on judge- ments of semantic similarity. In Proceedings of CogSci, 611-616.
Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension. K Mcrae, M Spivey-Knowlton, M Tanenhaus, Journal of Memory and Language. 38K. McRae, M. Spivey-Knowlton, M. Tanenhaus. 1998. Modeling the influence of thematic fit (and other con- straints) in on-line sentence comprehension. Journal of Memory and Language, 38:283-312.
A basis for generating expectancies for verbs from nouns. K Mcrae, M Hare, J Elman, T Ferretti, Memory and Cognition. 337K. McRae, M. Hare, J. Elman, T. Ferretti. 2005. A basis for generating expectancies for verbs from nouns. Memory and Cognition, 33(7):1174-1184.
Vector-based models of semantic composition. J Mitchell, M Lapata, Proceedings of ACL. ACLJ. Mitchell, M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, 236- 244.
Kernels on linguistic structures for answer extraction. A Moschitti, S Quarteroni, Proceedings of ACL. ACLColumbus, OHA. Moschitti, S. Quarteroni. 2008. Kernels on linguistic structures for answer extraction. In Proceedings of ACL, 113-116, Columbus, OH.
A Bayesian model predicts human parse preference and reading time in sentence processing. S Narayanan, D Jurafsky, Proceedings of NIPS. NIPSS. Narayanan, D. Jurafsky. 2002. A Bayesian model predicts human parse preference and reading time in sentence processing. In Proceedings of NIPS, 59-65.
Dependency-based construction of semantic space models. S Padó, M Lapata, Computational Linguistics. 332S. Padó, M. Lapata. 2007. Dependency-based construc- tion of semantic space models. Computational Linguis- tics, 33(2):161-199.
Combining syntax and thematic fit in a probabilistic model of sentence processing. U Padó, F Keller, M W Crocker, Proceedings of CogSci. CogSciU. Padó, F. Keller, M. W. Crocker. 2006. Combining syn- tax and thematic fit in a probabilistic model of sentence processing. In Proceedings of CogSci, 657-662.
Flexible, corpus-based modelling of human plausibility judgements. S Padó, U Padó, K Erk, Proceedings of EMNLP/CoNLL. EMNLP/CoNLLS. Padó, U. Padó, K. Erk. 2007. Flexible, corpus-based modelling of human plausibility judgements. In Pro- ceedings of EMNLP/CoNLL, 400-409.
Selectional constraints: An informationtheoretic model and its computational realization. Cognition. P Resnik, 61P. Resnik. 1996. Selectional constraints: An information- theoretic model and its computational realization. Cog- nition, 61:127-159.
A vector-space model for information retrieval. G Salton, A Wang, C Yang, Journal of the American Society for Information Science. 18G. Salton, A. Wang, C. Yang. 1975. A vector-space model for information retrieval. Journal of the American So- ciety for Information Science, 18:613-620.
Automatic word sense discrimination. H Schütze, Computational Linguistics. 241H. Schütze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97-124.
Open-source corpora: Using the net to fish for linguistic data. S Sharoff, International Journal of Corpus Linguistics. 114S. Sharoff. 2006. Open-source corpora: Using the net to fish for linguistic data. International Journal of Corpus Linguistics, 11(4):435-462.
Tensor product variable binding and the representation of symbolic structures in connectionist systems. P Smolensky, Artificial Intelligence. 46P. Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connection- ist systems. Artificial Intelligence, 46:159-216.
Contextual preferences. I Szpektor, I Dagan, R Bar-Haim, J Goldberger, Proceedings of ACL. ACLColumbus, OHI. Szpektor, I. Dagan, R. Bar-Haim, J. Goldberger. 2008. Contextual preferences. In Proceedings of ACL, 683- 691, Columbus, OH.
Preference semantics. Wilks, Formal Semantics of Natural Language. CUP. Wilks. 1975. Preference semantics. In Formal Seman- tics of Natural Language. CUP.
More accurate tests for the statistical significance of result differences. A Yeh, Proceeedings of COLING. eeedings of COLINGA. Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceeedings of COLING, 947-953.
