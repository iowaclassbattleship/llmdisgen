
Introduction

Collaborative filtering (CF) has been successfully used in a large number of applications, such as e-commerce web-sites {{262736977}} and on-line communities in a series of domains {{2455777}}{{2616594}}{{5546580}}.However, some challenges are still offered.Most of these systems deal with very large amounts of data and frequently suffer from scalability problems.CF systems should be able to efficiently process data on-line as it arrives, in order to keep the system up-to-date.This poses two problems:
-Scalability: as new users and items enter the system, time and memory requirements increase.At some point, the data processing rate may fall below the data arrival rate.-Accuracy: as new data elements add up, the weight of each individual data element decreases.This causes the system to become less and less sensitive to recent information.
In order to overcome these problems, forgetting mechanisms can be implemented.When forgetting older data, it is possible to reduce processing time and memory usage and maintain the system's sensitivity to recent data.
In this work, we present two forgetting mechanisms: sliding windows and fading factors.We look at user activity as a data stream {{34783548}} in which data elements consist of individual user sessions, each containing a set of implicitly binaryrated items-seen items.Then we implement and evaluate forgetting mechanisms in nonincremental and incremental versions of CF algorithms.
The remainder of this paper is organized as follows.Section 2 refers to related work.In Sect.3, we introduce the forgetting approaches to CF. Section 4 describes the four algorithms used in the experiments.Evaluation and results are presented in Sect. {{not_in_s2orc}}. Conclusions and future work are presented in Sect.6.

Related work

Collaborative filtering has been an active research field in recent years, with many enriching advances.However, few work has focused on the study of temporal effects in CF.In {{1487925}} and {{not_in_s2orc}}, Ding et al. use time-weighted ratings to predict new ratings in an item-based CF algorithm.The authors incorporate a time function in the rating prediction function, thus giving more weight to recent ratings and less weight to older ratings.In {{15280943}}, Nasraoui et al. use their TECNO-STREAMS {{6513417}} stream clustering algorithm to learn from evolving usage data streams.Koren {{not_in_s2orc}} addresses the problem of time-varying usage data using a model that is able to separately deal with multiple changing concepts.
In the field of data stream processing {{34783548}}, several methods have been proposed to provide algorithms with mechanisms able to deal with concept drifts.Most of these are based on the idea of "forgetting" older data elements, whether by using sliding windows or decay functions based on fading factors.The FLORA system {{15558554}} tries to deal with concept drifts by using an adaptive time window, controlled by heuristics that track system performance.In {{31439485}}, techniques are proposed to maintain sequence-based windows-where the number of data elements in the window is fixed-and time-based windows-where data elements belong to a determined time interval.Gradual forgetting is studied in {{15355119}}, where the author uses a recommender system to study the proposed method in the context of drifting user preferences.
Incremental CF has been presented in {{15396909}}, where a userbased algorithm incrementally updates user similarities every time new data is available.In {{206867959}} and {{15830350}}, item-based and user-based incremental algorithms that use implicit binary ratings are proposed and evaluated.
The algorithms proposed in this article are based on the ones presented in {{206867959}} and {{15830350}}.We propose, implement, and evaluate forgetting mechanisms in incremental and nonincremental algorithms using binary usage data.Our research suggests that forgetting mechanisms have the potential to improve both the scalability and the predictive ability of user-based and item-based CF.
Collaborative filtering with forgetting mechanisms

Neighborhood-based collaborative filtering works by calculating similarities between users-user-based-or itemsitem-based.Users are similar if they share many preferred items.Similarity between two items is determined by the number of users that simultaneously share interest in both items.This information is obtained by inspecting user sessions.A user session s by user u contains a list of items i.These session items are the ones for which u has given positive feedback during a well defined, usually short, period in time.Based on this information, a similarity matrix S is built, containing pairwise user or pairwise item similarities.
Sliding windows

Forgetting can be performed using a sequence-based sliding window of size n that retains information about the n most recent user sessions.One direct way to implement sliding windows in a user-based or item-based CF system is to rebuild the similarity matrix using data of a fixed size sequence-based window holding the w latest sessions.Each time a new session s i is available, the window moves one session forward discarding session s i−n−1 and including s i .Then the new window is used as learning data to build a new similarity matrix.
Nonincremental algorithms can be easily adapted to use sliding windows, since they recalculate the whole similarity matrix each time new session data is available.The additional task is to move the window forward and use that window to rebuild the matrix.It is important to note that nonincremental algorithms process individual sessions several times-as many as the length of the window-which is not an ideal way to deal with data streams {{34783548}}.
Traditional nonincremental algorithms use all of the past data to recalculate S.This is strategy is hereby referred to as a growing window approach, since the data window continuously grows as it incorporates more sessions.
For incremental algorithms the adaptation is not so simple because the similarity matrix is not rebuilt from scratch {{15830350}}{{15396909}}.The similarity values corresponding to the items in each new session are updated, while other values are kept.In order to implement a sliding window approach in incremental algorithms it is necessary, at each update, to remove from the similarity matrix the information that was added by the oldest session in the window.This requires that every session is processed twice, and as in the nonincremental case, memory is required to hold session data for all sessions in the window.
One possible alternative to sequence-based windows is to use time-based windows {{31439485}}.In this case sessions are timestamped so that the system knows which sessions it should discard.This approach has the disadvantage to make the window size variable, depending on the data rate.The number of sessions in the window increases for fast data rates and decreases with slow rates.Large windows carry more information but require more time and memory to be processed.Small windows are easy to process but may contain insufficient information.
The sliding window approach basically considers the n most recent sessions to build the similarity matrix S. To illustrate, consider a sequence of the first n user sessions {s 1 , s 2 , . . ., s n }, each containing a set of items rated by one user.First, S is built from data in sessions {s 1 , . . ., s n }.
Then, for each new session s a , the model is rebuilt with data from sessions {s a−n , . . ., s a }, creating a window that slides through data as it arrives.Algorithm 1 (UBSW) is a classical user-based nonincremental algorithm adapted for using sliding windows.This algorithm takes in a sequence of user sessions L = {s 1 , s 2 , . ..}.The first n sessions are used to build an initial user vs. user similarity matrix S.Then, for each new user session in L, S is recalculated using a new window consisting of the latest n sessions.Item activation weights are then calculated and the Nrecs items with the highest weights are recommended.
IBSW (Algorithm 2) is an item-based version of the nonincremental algorithm, also using sliding windows.This algorithm is based on the item-based nonincremental algo-
-Recommend the Nrec items with the highest activation weight rithm in {{15830350}}.As with UBSW, the algorithm takes in a sequence of user sessions L and, for each new session, recalculates an item vs. item similarity matrix S using a window with the latest n sessions.Then the item weights are calculated and recommendations are provided accordingly.
Fading factors

Sliding windows provide an effective but abrupt way to forget older data.In many cases, however, past data is not necessarily obsolete, and can contain valuable information {{34783548}}{{15355119}}.Fading factors {{5767782}} provide a mechanism to gradually forget past data.Fading factors with incremental algorithms can be implemented by multiplying the similarity matrix by a factor α < 1 before each update.In user-based algorithms, the similarity matrix contains similarities measured between every pair of users in the system.In item-based algorithms, similarities are measured between every pair of items.In both cases, the fading factor causes similarities to continuously decrease through time unless they are reinforced by recent session data.If the similarity reaches a lower threshold value, it can be assumed to be zero.This method is simple to implement and requires a single scan of each session.
Figure 1 illustrates the session weight curve that is obtained at session index 500 using different factors.We can observe that the decay for recent sessions is higher than for older sessions.Using forgetting curves with different shapes requires a more complex approach.Because session history is not kept, there is no way to know how to apply forgetting to each similarity value.Keeping ordered session data in memory would allow us to use different decay curves, however, it would also introduce complexity in the update process.Each session would have to be processed at every update until its weight is zero.
Fading factors can also be implemented in nonincremental algorithms if all considered sessions are kept in memory in the same order by which they arrived.A function of the session index can then be applied when rebuilding the similarity matrix, giving less weight to older sessions.However, this poses the same problem of using sliding windows: each session is processed several times, which is undesirable.
Whereas with sliding windows old data is abruptly forgotten, the idea of fading factors is to slowly decrease the importance of sessions as they grow old.This can be achieved by manipulating the similarity matrix S. Incremental algorithms using fading factors simply multiply S by a factor α ≤ 1 before updating them with the active session data.With α < 1, at each new session, older sessions become less important.With α = 1, older data weight is maintained.To incrementally update S we also maintain a frequency matrix F with the number of items corated by each pair of users (user-based) or the number of users that corated each pair of items (item-based).The principal diagonal in F gives us the number of items evaluated by each user-in the user-based case-and the number of users that evaluated each itemin the item-based case.The matrix F contains all necessary data to calculate any similarity in S. The values in F are incremented by 1 for every pair of items that are contained in the same session (item-based), or for every pair of users that have seen the same item (user-based).Then only the similarities in S that are affected by changes in F are recalculated.
Forgetting is obtained by multiplying matrices S and F by a fading factor α < 1.When using α = 1 no forgetting occurs.It is important that both matrices S and F are multiplied by α.Because similarities in S are calculated directly from values in F , forgetting must be reflected also in F .Otherwise, every time rows and columns in S were updated, no forgetting would occur for them.Also, if only F is multiplied by α, nonupdated rows and columns in S would not be forgotten.
UBFF (Algorithm 3) is a modified version-using fading factors-of the user-based incremental algorithm originally
-Update the row/column of S corresponding to user u a :
-Determine the activation weight W i of each item i never seen before by u a (Eq.( 3)) -Recommend the Nrec items with the highest activation weight to u a described in {{15830350}}.A cache matrix F maintains the number of items covisited by every pair of users.Additionally, the database D of user sessions and session weights needs to be maintained.This database is required in the matrices update step in order to reflect the forgetting of user sessions and in the recommendation step to retrieve recommendable items from the nearest neighbors.Each element s, w in D is a pair containing session data s and session weight w.The initial weight of each new session is set to 1. Session weights are then multiplied by the fading factor α every time a new session is processed.This way, sessions loose weight as they grow older.
Values in F are calculated as the number of items simultaneously present in the active session and every other (past) session.In order to reflect the forgetting of the older sessions, this number needs to be multiplied by the weight of the oldest of the two sessions at each cell in the active session row/column in F .For example, let the first session s 1 of user u 1 be composed of 2 items i and j .Also, let the tenth session s 10 (of user u 10 ) be composed of the same two items i and j .This would make F u 1 ,u 10 = 2.However, at session s 10 , previous session weights are already lower.Specifically, the weight of s 9 is W D 9 = α, the weight of s 8 is W D 8 = α 2 , the weight of s 7 is W D 7 = α 3 , and so on.The first session Algorithm 4 IBFF Input: L, Nrecs, α, n Output: recommendation list -Initialize matrices S and F using sessions {s 1 , . . ., s n } ⊂ L -For each new session s a ∈ L, a > n (by user u a )
-Determine the activation weight W i of each item i never seen before by u a (Eq.( 4)) -Recommend the Nrec items with the highest activation weight to u a -Update S and F :
-Let I a be the set of items in session s a -Multiply all values in S and F by a fading factor α S = αS, F = αF (
-For each new item, add a row and column to F and to S -For each pair of items in (i, j ) in I a , update the corresponding row/column in F :
-For each item i a in I a update the corresponding row (column) of S:
F i a ,i a × F .,. {{not_in_s2orc}} s 1 has weight W D 1 = α 9 , so this must be reflected in the cache matrix as
The incremental item-based algorithm with fading factors (IBFF-Algorithm 4) is based on the incremental itembased algorithm in {{15830350}}.In order to incrementally update S we also need save in memory the auxiliary cache matrix F with the number of users that evaluate each pair of items.The principal diagonal gives us the number of users that evaluate each item.Forgetting is obtained the same way as in UBFF, but in this case, the user session database is not required.
One difference between UBFF and IBFF is that with the first, recommendations are performed after updating the model, while with the latter, recommendations are provided before updating the model.With UBFF, the session belonging to the active user needs to be processed before the recommendation step because similarities between the active user and other users may not yet be present in S. With IBFF, S already contains enough information to compute the recommendations before performing the update.UBFF and IBFF are the same algorithms used in previous work {{15830350}}, with only the changes that are strictly necessary to function with fading factors.This allows us to make comparisons between past and present results.
Algorithms

All algorithms take binary usage data as input.This data contains the set of items visited by each user, grouped in sessions and ordered by the session end time.A session is considered to be the set of items visited or rated by a single user in a certain time frame.For the purpose of this work, datasets contain only anonymous users, so each session corresponds to a unique user.Sessions containing a single item are removed.Datasets are processed to build a similarity matrix S that contains the similarities between all pairs of usersin the user-based version-or items-in the item-based version.Similarity between a pair of users (or items) is calculated using a simplified version of the cosine measure for binary ratings {{206867959}}{{15830350}}.If U and V are the sets of items that users u and v evaluated, then the user-based similarity between u and v is given by
For the item-based version, if I and J are the sets of users that evaluated items i or j , the similarity between i and j is given by
Evaluation and results

In this section, we present results obtained in experiments conducted to evaluate the impact of forgetting mechanisms in CF algorithms.Our main goal is to assess the potential of forgetting mechanisms to improve scalability and predictive ability.We also implement an evaluation methodology that is able to deal with usage data streams.This approach allows us to continuously monitor the behavior of the algorithms.
Datasets

Four datasets are used in the experiments.Table 1 describes each dataset.Sessions with less than 2 items were removed.In all datasets, every user performs exactly one session, meaning that each session corresponds to a different unique user.Datasets ART1 and ART2 are synthesized datasets with an abrupt change.Both ART1 and ART2 consist of identical sessions with 4 items.These sessions contain the items {a, b, c, d} at the beginning and then the item d is replaced by a new item e.This change occurs at session index 400 in ART1 and session 500 in ART2.
ELEARN and MUSIC are natural datasets extracted from web usage logs of an e-learning website (ELEARN) and listened tracks from a social network1 dedicated to nonmainstream music (MUSIC).
Evaluation methodology

In all experiments we have used the all-but-one protocol as described in {{2885948}}, but following a chronological ordering for sessions.First, the dataset is split in a training set and a test set.Sessions are not selected randomly to the training and test sets, but rather according to their order.This means that for a split of 0.2, for example, the training set is composed of the first 20 % sessions and the test set is composed of the remaining 80 %.For IBSW and UBSW, the training set is considered to be the first window.For IBFF and UBFF, an initial training set containing the first 10 % of sessions is used to build the initial matrices S and F .This initial training set is required in order to avoid cold-start problems {{not_in_s2orc}}.After splitting the dataset, an item is randomly hidden from each session in the test set.Then recommendations made to each user are evaluated as a hit if the hidden item is among the recommended items.
To evaluate recommendations, we use Precision and Recall, with the following definition: Precision = # hits # recommended items {{15355119}} Recall = # hits # hidden items {{262736977}} One other possible measure, that combines Precision and Recall, is the F1 measure:
Since one single item from each session is hidden, Recall is either 1 (hit) or 0 (miss), and Precision is obtained dividing Recall by the number of recommended items, which is a predefined parameter (see Sect. 5.2.1).For this reason, we present predictive ability using Recall only.Precision and/or F1 scores can be easily calculated from Recall and the number of recommended items.
Recall is calculated sequentially for each user.At the end, we obtain a sequence of hits and misses, and an overall average can be calculated.However, as this average may hide different behaviors through time, we study the evolution of Recall values through time for each experiment.A moving average of Recall is used to obtain values and graphics that illustrate how accuracy varies with time, as new sessions arrive.It is important to use this approach because we want to study how Recall evolves with and without implementing forgetting mechanisms.In the Recall graphics, a moving average consisting of arithmetic mean of the previous 40 Recall values (0 or 1) is used to draw the graphics.In practice, this represents the proportion of hits in the previous 40 recommendation requests.
Computational time spent building or updating the matrices is provided for natural datasets ELEARN and MUSIC.Time measurements allow us to empirically study the scalability of algorithms using different datasets and parameters.
Reaction to sudden changes in data, using natural datasets ELEARN and MUSIC, is studied introducing artificial changes in these datasets.For the purpose of our experiments, we randomly chose 50 % of the existing items and change their names from a certain session onwards, causing a sudden drift.The algorithms are then evaluated using these modified datasets.These new datasets keep all the characteristics of a natural dataset, only with a drift of 50 % of the items.
Evaluation parameters

The following parameters must be set to conduct the tests:
k: the maximum number of neighbors (users or items) to consider in S when computing recommendations;
-Nrec: the number of items to recommend at each recommendation request; w p : the window size in percentage of total sessions in the dataset (for IBSW and UBSW); α: the fading factor.In IBFF and UBFF, S and F are multiplied by this factor before updating with new data.
In the experiments with synthesized datasets (ART1 and ART2), values k = 2 and Nrec = 1 are used.These low values are chosen because synthesized datasets have a low number (4) of items.With all other datasets values k = 5 and Nrec = 5 are used.These values are chosen taking into account results obtained in {{not_in_s2orc}} and the computational resources required to run the experiments.
For the incremental algorithms, four values of α are tested.Values close to 1 are chosen so that the forgetting is not too abrupt.The nonforgetting factor α = 1 is used as reference to measure and compare the impact of forgetting with different factors.
To study the impact of forgetting, we compare UBSW and IBSW, which use sliding windows, with their nonforgetting versions that use growing windows.These are called UBGW and IBGW.With growing windows, at each session s i , all past sessions {s 1 , . . ., s i−1 } are used to build S. For UBGW and IBGW, w p is the percentage of sessions used to build the initial matrix S.
Implementation and hardware details All algorithms were implemented using R version 2.11.0 with the package spam version 0.21-0 to handle sparse matrices.The hardware used in the experiments was a machine with a dual 2.67 GHz core processor and 2Gb of RAM, running the Ubuntu 8.04 Linux OS.
Experiments with synthesized datasets

It is possible to observe in Fig. 2 that, on the synthesized datasets, the algorithms with forgetting mechanisms tend to recover faster from abrupt changes than nonforgetting algorithms.As older data is forgotten, the initial conditions are not considered, providing a faster adaptation to new situations.Results for IBFF in Fig. 2(b) illustrate the behavior of the algorithm using different fading factors: recovery is faster for lower values of α and slower for higher values of α.Without forgetting, none of the algorithms recover completely until the end of the test.For α = 0.97, Recall = 1 is recovered about 400 sessions after the change.For α = 0.98 the recovery occurs after 600 sessions, and for α = 0.99, almost 1500 sessions are necessary to recover.Without forgetting (α = 1), recovery does not happen during the 2,000 sessions in the dataset.With IBSW and IBGW, shown in Fig. 4, the experiments with the ELEARN dataset show a better recovery from change with IBSW.This only happens right after change occurs.Then, shortly after session 200, IBGW recovers and remains better than IBSW.At the end of the experiment, IBSW drops rapidly to values close to 0.4, while IBGW holds on to values around 0.7.
Incremental with fading factors

Figure 5 shows the reaction of UBFF to change.With the ELEARN dataset, there is a better reaction of the algorithm with lower fading factor.The best results are obtained with α = 0.97, from session 200 to around session 450.The second best results are achieved with α = 0.98.Analyzing the lines in Fig. 5(a), from session 200-where change is introduced-to around session 350, Recall is generally higher for lower fading factors.In that interval, the lower recall values are obtained with α = 1 (without forgetting).
With the MUSIC dataset (Fig. 5(b)) results are very similar for all 4 fading factors.UBFF with α = 1 seems to have a slightly better performance most of the time.
With the item-based version (IBFF), shown in Fig. 6, recall is generally lower with α < 1 than with α = 1, although it is possible to see a better response to change with the ELEARN dataset with α = 0.98 and α = 0.99 between sessions 200 and 350.With the MUSIC dataset, the highest recall is obtained without forgetting (α = 1).case.In the case of matrix rebuild time-for IBSW and UBSW-the similarity matrix S is rebuilt from scratch every time a new session is available.With IBFF and UBFF values in matrices S and F are selectively updated.In any case, these matrices are typically very large and tend to grow very fast as new users and items enter the system.In this section, we study the scalability of the algorithms by measuring the time needed to rebuild or update the similarity matrix.
Incremental CF algorithms, instead of rebuilding the entire similarity matrix, only update the similarity values that can potentially change with a specific session.As shown in {{15830350}}, this has a considerably lower complexity than a complete rebuild.By looking at the time scales in Figs. 9 (UBFF) and 10-IBFF, and comparing them with those of nonincremental algorithms (Figs. 7 and 8), we can verify that incremental algorithms-UBFF and IBFF-update times are much shorter than rebuild times by UBSW/UBGW and IBSW/IBGW.
Analyzing UBFF update times in Fig. 9, it is clear that time increases as more data is available.Additionally, there seem to be very little differences between runs with different fading factors, including α = 1 (no forgetting).With IBFF (Fig. 10), results are also very similar between different fading factors.
Looking at the combination between user-based/itembased and the ELEARN/MUSIC datasets, we observe that with the ELEARN dataset, IBFF performs better than UBFF, while with the MUSIC dataset the opposite occurs.This is an effect similar to the one observed with nonincremental al-Fig.7 Matrix rebuild time with nonincremental algorithms (ELEARN dataset).A moving average (n = 40) is used to smoothen the lines gorithms, and again is caused by the proportion of the number of items and users in the datasets.
Nonincremental with sliding windows

Nonincremental algorithms need to recalculate the whole similarity matrix S every time a new session occurs.Figure 7 illustrates the time required to rebuild the matrix as the ELEARN dataset is processed.Comparing the sliding window algorithms with their growing window versions, it is clear that both user-based and item-based versions using growing windows (UBGW and IBGW) time to recalculate S grows super-linearly with the number of sessions.The sliding window versions tend to maintain time.Comparing the user-based algorithms with the item-based ones, the first have a more stable record than the latter.This happens because with user-based algorithms, S has exactly the same number of rows and columns as the number of sessions in the window.With item-based algorithms, the number of items in the window is not fixed-with sliding window-nor it grows one by one-with growing windows-since the order of appearance of new items is not sequential.This leads to memory allocation and garbage collection processes to run frequently, causing extra time consumption in some iterations.
Figure 8 shows the time to rebuild S with the MUSIC dataset.As with the ELEARN dataset, growing window algorithms take increasingly more time to rebuild S while the sliding window algorithms tend to maintain the time required to rebuild S.However, two main differences between ELEARN and MUSIC are clear.First, the difference between item-based and user-based algorithms is much higher with MUSIC.Second, the item-based versions take longer than the user-based versions with MUSIC, which is the opposite behavior of ELEARN.This happens because the number of items in MUSIC (3121) is much higher than the number of users (785), leading to much larger matrices when using item-based algorithms.The item-based matrices are large enough to cause memory swapping, as the available RAM is not enough to store them.This causes very high fluctuations (Fig. 8(b)).
Update times with Netflix data

When using fading factors, the matrices S and F become sparser as similarities and cooccurrence frequencies are forgotten.The algorithms actually take advantage of this sparsity to optimize the data structures where S and F are stored, leading to a better scalability.This is not visible in ELEARN and MUSIC because these datasets are not large enough to completely forget similarities and frequencies.However, using a longer dataset it is possible to observe that fading factors improve scalability.To verify this, we calculate update times of IBFF with a dataset consisting of 5,000 sessions sampled the well-known Netflix Prize {{not_in_s2orc}} dataset.
Because this dataset holds ratings given by users to movies within a discrete scale of 1 (worse) to 5 (better), we eliminated all ratings below 4, retaining only ratings of 4 or 5.This ensures that only positive ratings are considered.Then we organized the dataset in sessions.Because timestamps in the dataset only contain the date (not the time), we consider one session to be the list of rated items by one user in the same day.We then sampled 5,000 random sessions from the whole data, maintaining the chronological order.With α < 1, the time spent updating S and F shows a similar behavior until around session 1600, and then it starts to deviate downward from the nonforgetting configuration.Furthermore, there seems to be a relation between values of α and time gains-lower values require less time.The downsize of this experiment was the overall poor accuracy of the algorithm, with an average recall of 0.021 with α = 1 and 0.01 with α = 0.99.We believe this is caused by the removal of information (ratings <4) and low representativeness of the sample.

Conclusions

We have implemented and evaluated the impact of forgetting mechanisms in nonincremental and incremental collaborative filtering algorithms.Our results suggest that nonincremental algorithms that use sliding windows, when compared to their nonforgetting versions using a growing window, reduce computational requirements while not negatively affecting-and in some situations improvingpredictive ability.Results also suggest that incremental algorithms benefit from the use of fading factors, although the fading factor approach has more subtle improvements in time requirements.It is also confirmed that incremental algorithms are more scalable than non-incremental algorithms.
This work studies the impact of forgetting mechanisms in an abrupt change scenario.Our experimental results were limited with respect to gradual drifts, either due to the data sets we have used or due to limitations of our approach.In the future, we intend to further evaluate forgetting mechanisms under both abrupt and gradual drifts.This will require researching how forgetting mechanisms relate to dataset intrinsic characteristics.A better understanding of datasets will allow the implementation of algorithms that are able to automatically adjust forgetting parameters to different situations.This will allow the implementation of dynamic forgetting, only when useful.We are also working on the implementation of the algorithms to allow larger scale experiments and the use of fading factors more sporadicallyevery k sessions.
PONORTE and by the ERDF-European Regional Development Fund through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT-Fundação para a Ciência e a Tecnologia (Portuguese Foundation for Science and Technology) within project «FCOMP-01-0124-FEDER-022701».















CITED_PAPERS:


44515378:spam: A Sparse Matrix R Package with Emphasis on MCMC Methods for Gaussian Markov Random Fields


Introduction

In many areas of scientific study, there is great interest in the analysis of datasets of ever increasing size and complexity.In the geosciences, for example, weather prediction and climate model experiments utilize datasets that are measured on the scale of terabytes.New statistical modeling and computational approaches are necessary to analyze such data, and approaches that can incorporate efficient storage and manipulation of both data and model constructs can greatly aid even the most simple of statistical computations.The focus of this work is on statistical models for spatial data that can utilize regression and correlation matrices that are sparse, i.e., matrices that have many zeros.rameters in the context of a GMRF is not the only application where efficient Cholesky factorizations are needed.To mention just a few: drawing multivariate random variables, calculating log-likelihoods, maximizing log-likelihoods, calculating determinants of covariance matrices, linear discriminant analysis, etc. Statistical calculations, which require solving a linear system or calculating determinants, usually also require pre-and post-processing of the data, visualization, etc.A successful integration of an efficient factorization algorithm not only calls for subroutines for the factorization and back-and forwardsolvers, but also is user friendly and easy to work with.As we show below, it is also important to provide access to the computational steps involved in the sparse factorization, and which are compatible with the sparse matrix storage scheme.R, often called GNU S, is the perfect environment for implementing such algorithms and functionalities in view of statistical applications, see Ihaka and Gentleman (1996); R Development Core Team (2010a).Therefore, spam has been conceived as a publicly available R package available from the Comprehensive R Archive Network at http://CRAN.R-project.org/package=spam.For reasons of efficiency many functions of spam are programmed in Fortran with the additional advantage of abundantly available good code.On the other hand, Fortran does not feature dynamic memory allocation.While there are several remedies, these could lead to a minor decrease in memory efficiency.
To be more specific about one of spam's main features, assume we need to calculate A −1 b with A a symmetric positive-definite matrix featuring some sparsity structure, which is usually accomplished by solving Ax = b.We proceed by factorizing A into R R, where R is an upper triangular matrix, called the Cholesky factor or Cholesky triangle of A, followed by solving R y = b and Rx = y, called forwardsolve and backsolve, respectively.To reduce the fill-in of the Cholesky factor R, we permute the columns and rows of A according to a (cleverly chosen) permutation P, i.e., U U = P AP, with U an upper triangular matrix.There exist many different algorithms to find permutations which are optimal for specific matrices or at least close to optimal with respect to different criteria.Note that R and U cannot be linked through P alone.Figure 1 illustrates the factorization with and without permutation.For solving a linear system the two triangular solves are performed after the factorization.The determinant of A is the squared product of the diagonal elements of its Cholesky factor R. Hence the same factorization can be used to calculate determinants (a necessary and computational bottleneck in the computation of the log-likelihood of a Gaussian model), illustrating that it is very important to have a very efficient integration (with respect to calculation time and storage capacity) of the Cholesky factorization.In the case of GMRF, the off-diagonal non-zero elements correspond to the conditional dependence structure.However, for the calculation of the Cholesky factor, the values themselves are less important than the sparsity structure, which is often represented using a graph with edges representing the non-zero elements, see Figure 1.
A typical Cholesky factorization of a sparse matrix consists of the steps illustrated in the following pseudo-code algorithm.
[1] Determine permutation and permute the input matrix A to obtain P AP
[2] Symbolic factorization, where the sparsity structure of U is constructed [3] Numeric factorization, where the elements of U are computed When factorizing matrices with the same sparsity structure Steps 1 and 2 do not need to be repeated.In MCMC algorithms, this is commonly the case, and exploiting this shortcut leads
1 0.5 0 0.5 0.5 0.5 1 0 0.5 0 0 0.5 1 0.5 0.5 0.5 0.5 0 1 0 0.5 0 0.5 0 1

Outline

This article is structured as follows.The next section outlines in more detail the integration of the Cholesky factorization.Section 3 discusses the sparse matrix implementation in spam.In Section 4 we illustrate the performance of spam with simulation results for GMRFs.Section 5 illustrates spam two specific real data examples.Discussion and the positioning of spam and the Cholesky factorization in a larger framework are given in Section 6.

The integration of the Cholesky factorization

In this section we discuss the individual steps and the actual integration of the Cholesky factorization in more details.The scope of this article prohibits a very detailed discussion, and we refer to George and Liu (1981) or Duff, Erisman, and Reid (1986) as general texts and to the more specific references cited below.spam uses a Fortran supernodal left-looking (constructing the lower triangular factor R column-wise) Cholesky factorization originally developed by E. Ng and B. Peyton at Oak Ridge National Laboratory in the early 1990s, see Ng and Peyton (1993a).The algorithm groups columns (via elimination trees, see Liu 1992, for a definition) that share the same sparsity structure into supernodes, see Figure 1 and, e.g., Liu, Ng, and Peyton (1993).The factorization cycles over the supernodes, performing block factorization within each supernode with appropriate updates derived from previous supernodes.The algorithm has been enhanced since its first implementation by exploiting the memory hierarchy: it splits supernodes into sub-blocks that fit into the available cache; and it unrolls the outer loop of matrix-vector products in order to reduce overhead processor instructions.Within spam the algorithm is used to construct the upper triangular factur R and, strictly speaking, becomes a supernodal "top-down" algorithm.
A more detailed pseudo algorithm of the Cholesky factorization of a symmetric positivedefinite matrix and explanations of some of the steps are given below.
[0] Create the adjacency matrix data structure
[1] Determine permutation and permute the matrix As for Step 1, there are many different algorithms to find a permutation, two are implemented in spam, namely, the multiple minimum degree (MMD) algorithm, (Liu 1985), and the reverse Cuthill-McKee (RCM) algorithm, (George 1971).Additionally, the user has the possibility to manually specify a permutation to be used for the Cholesky factorization.The resulting sparsity structure in the permuted matrix determines the sparsity structure of the Cholesky factor.As an illustration, Figure 2 shows the sparsity structure of the Cholesky factor resulting from an MMD, an RCM, and no permutation of a precision matrix induced by a second order neighbor structure of the US counties.The values z, w are the sizes of the sparsity structure and of the vector containing the column indices of the sparsity structure and s is the number of supernodes.Note that the actual number of non-zero elements of the Cholesky factor may be smaller than what the constructed sparsity structure indicates.How much fill-in with zeros is present depends on the permutation algorithm, in the example of Figure 2 there are 14111, 97565 and 398353 zero elements in the Cholesky factors resulting from the MMD, RCM, and no permutation, respectively.
Step 2a constructs the elimination tree and supernode elimination tree.From this tree a maximal supernode partition (i.e., the one with the fewest possible supernodes) is calculated.
In Step 2b, the children of each parent in the supernodal elimination tree is reordered to minimize the storage requirement (i.e., the last child has the maximum number of non-zeros in its column of the factor).Hence, the matrix is ordered a second time, and if passing the identity permutation to Step 1, the matrix may nevertheless be reordered in Step 2b.
Step 2c constructs the sparsity structure of the factor using the results of Gilbert, Ng, and Peyton (1994), which allow storage requirements to be determined in advance, regardless of the ordering strategy used.Note that the symbolic factorization subroutines are independent of any ordering algorithms.
The integration of the Cholesky factorization in spam preserves the computational order of the permutation and of the factorization of the underlying Fortran code.Further, the resulting precision in R is equivalent to the precision of the Fortran code.We refer to George and Liu (1981); Liu (1992), Ng and Peyton (1993a) and to Gould, Hu, and Scott (2005b,a) for a detailed discussion about the precision and efficiency of the algorithms by themselves and within the framework of a comparison of different solvers.

The sparse matrix implementation of spam

The implementation of spam is designed as a trade-off between the following competing philosophical maxims.It should be competitively fast compared to existing tools or approaches in R and it should be easy to use, modify and extend.The former is imposed to assure that the package will be useful and used in practice.The latter is necessary since statistical methods and approaches are often very specific and no single package could cover all potential tools.Hence, the user needs to understand quickly the underlying structure of the implementation of spam and to be able to extend it without getting desperate.(When faced with huge amounts of data, sub-sampling is one possibility; using spam is another.)This philosophical approach also suggests trying to assure S3 and S4 compatibility, (Chambers 1998; see also Lumley 2004).S4 has higher priority but there are only a handful of cases of S3 discrepancies, which do however not affect normal usage.
To store the non-zero elements, spam uses the "old Yale sparse format".In this format, a (sparse) matrix is stored with four elements (vectors), which are (1) the nonzero values row by row, (2) the ordered column indices of nonzero values, (3) the position in the previous two vectors corresponding to new rows, given as pointers, and (4) the column dimension of the matrix.We refer to this format as compressed sparse row (CSR) format.Hence, to store a matrix with z nonzero elements we thus need z reals and z + n + 2 integers compared to n × n reals.Section 3.2 describes the format in more details.
Much of the algebraic calculations in spam are programmed in Fortran.Some of the Fortran code is based directly on SPARSKIT, a basic tool-kit for sparse matrix computations (Saad 1994).Some subroutines are optimized and tailored functions from SPARSKIT and a last, large set consists of new functions.
The package spam provides two classes, first, spam representing sparse matrices and, second, spam.chol.NgPeyton representing Cholesky factors.A class definition specifies the objects belonging to the class, these objects are called slots in R and accessed with the @ operator, see Chambers (1998) for a more thorough discussion.The four vectors of the CSR representation are implemented as slots.In spam, all operations can be performed without a detailed knowledge about the slots.However, advanced users may want to work on the slots of the class spam directly because of computational savings (e.g., changing only the contents of a matrix while maintaining its sparsity structure, see Section 6.2).The Cholesky factor requires additional information (e.g., the used permutation) hence the class spam.chol.NgPeyton contains more slots, which are less intuitive.There are only very few, specific cases, where the user has to access these slots directly.Therefore, user-visibility has been disregarded for the sake of speed.The two classes are discussed in the more technical Section 3.2.

Methods for the sparse classes of spam

For both sparse classes of spam, standard methods like plot, dim, backsolve/forwardsolve, determinant (based on a Cholesky factor) are implemented and behave as in the case of full matrices.Print methods display the sparse matrix as a full matrix for small matrices and display only the non-zero values otherwise.The corresponding cutoff value, as well as other parameters, can be set and read via spam.options.
For the spam class additional methods are defined, such as rbind/cbind, dim<-, etc.The group generic functions from Math, Math2 and Summary are treated particularly since they operate only on the nonzero entries of the spam class.For example, for the matrix A presented in the introduction, range(A) is the vector c(0.5, 1); that is, the zeros are omitted from the calculation.The help files list further available methods and highlight the (dis-)similarities compared to regular matrices or arrays.
Besides the two sparse classes mentioned above, spam does not maintain different classes for different types of sparse matrices, such as symmetric or diagonal matrices.Doing so would result in some storage and computational gain for some matrix operations, at the cost of user visibility.Instead of creating more classes we consider additional specific operators.As an illustration, consider multiplying a diagonal matrix with a sparse matrix.The operator %d*% uses standard matrix multiplication if both sides are matrices or multiplies each column according the diagonal entry if the left hand side is a diagonal matrix represented by vector.

Slots of the sparse classes

This section describes the slots of the sparse classes in spam in more detail.The slots of the class spam consist of one z-vector of reals, and three vectors of integers of length z, n + 1 and 2, that correspond to the four elements of the CSR format.These are named:
R> slotNames(A)
[1] "entries" "colindices" "rowpointers" "dimension"
Notice that the row-dimension of A, i.e., A@dimension[1], is also determined by the length of A@rowpointers, i.e., length(A@rowpointers) -1.
The slots of the Cholesky factor spam.chol.NgPeyton can be separated into different groups.
The first is linked to storing the factor (i.e., entries and indices), the second group contains the permutation and its inverse, and the third and forth group contain relevant information relating to the factorization algorithm and auxiliary information:
R> slotNames(U)
[1] "entries" "colindices" "colpointers" "rowpointers" "dimension" [6] "pivot" "invpivot" "supernodes" "snmember" "memory" [11] "nnzA"
The slot U@dimension is again redundant.Similarly, only U@pivot or U@invpivot would be required.U@memory allows speed-up in the update process and U@nnzA contains the number of non-zero elements of the original matrix, which is used for calculating fill-in statistics of the factor.
For the Cholesky factor we use a slightly more complicated storage system which is a modification of the CSR format and is due to Sherman (1975).The rows of a supernode have a dense diagonal block and have identical remaining row structure, i.e., for each row of a supernode the column indices are obtained by leaving out the leftmost column index of the preceding row.This is not only exploited computationally (Ng and Peyton 1993b) but also by storing only the column indices of the first row of a supernode.For our example presented in the introduction, we have three supernodes (indicated by the horizontal lines in Figure 1) and the indices are coded as follows:
George and Liu (1981, Section 5.4.2) discuss the gain of this storage system for large matrices.With w and s from Figure 2, the difference between z and w + s + 1 is the gain when using the modified scheme.However, a more important gain is a much faster access to individual elements of the matrix, because U@rowpointers allows a very efficient line access compared to a triplet based (i, j, u ij ) format.
Notice that the class spam.chol.NgPeyton does not extent the class spam.chol.However, by considering only supernodes of size one, U@colpointers and U@rowpointers are identical, and U@colindices corresponds to the format of the spam class.In view of this, it would be straightforward to implement other factorization routines (not considering supernodes) leading to different classes for the Cholesky factor.Another possibility would be to define a virtual class spam.chol(also called superclass) and extending classes spam.chol.NgPeyton and spam.chol.someothermethod.

Simulation results for GMRF

In this simulation study, we illustrate Cholesky factorizations in the framework of GMRF.We use a lattice on a regular grid of different sizes and different neighbor structures as well as an irregular lattice, namely the counties of the contiguous USA.The county boundaries we use are from the maps package (Becker, Wilks, Brownrigg, and Minka 2010) providing 3082 counties.We consider that two counties are neighbors if they share at least one edge of their polygon description in maps.In spam adjacency matrices can be constructed using the function nearest.distfor regular grids or the function spam if the neighbors are available as indices pairs {i, j}.
For timing and memory usage, we use the R functions system.time and Rprof as in the following construct:
R> Rprof(memory.profiling= TRUE, interval = 0.0001) R> ressystime <-system.time(expression)R> Rprof(NULL) R> resRprof <-summaryRprof(memory = "both")$by.totalwhere expression is the R expression under investigation (e.g., to construct Figure 3 we use the expression { for(i in 1:100) ch1 <-chol(Qspam) } for different precision matrices Qspam).From ressystime, we retain the component user.selfand, from resRprof, we use mem.total of "system.time".The small time interval argument of Rprof (here set to 0.0001) helps (at least partially) to circumvent the issues in precisely measuring the memory amount with Rprof; see also R Development Core Team (2010b).However, our simulations show that the measurement of timing and memory usage varies and repeating the same simulation indicates a coefficient of variation of about 2% and 0.8%, respectively.The simulations are done with spam 0.22-0 and R 2.9.2 on an i686-pc-linux-gnu computer with a 2.66 GHz Intel Core2 Duo processor and 2 Gigabyte of RAM.
We first compare the total time and the memory required for Cholesky factorizations for different sizes of regular grids.In our MCMC framework, the sparsity structure of the precision matrix does not change and we can compare the time and memory requirements with one Cholesky factorization followed by numerical updates of the factor (Step 3). Figure 3 shows the total time (left) and memory usage (right) for 101 Cholesky factorization (solid) and one factorizations and 100 updates (dashed) of a precision matrix from different sizes L of regular L × L grids with a second order neighbor structure.We have chosen fixed but arbitrary values for the conditional dependence of the first and second order neighbors.The precision matrix from L = 200 has L 4 = 1.6 • 10 9 elements.The update is performed with the function update that takes as arguments a Cholesky factor and a symmetric positive-definite matrix with the same sparsity structure.The gain in using the update only decreases slightly as the size of the matrices increases.For matrices up to 50000 elements the update is about 10 times faster and uses less than 15 times the memory.
The package spam offers several options that can be used to increase speed and decrease memory allocation compared to the default values.Most of the options are linked to reduced input testing and validation, which can often be eliminated after preliminary testing or within an MCMC framework.Table 1 gives the relative speed-up of different options in the case of the two neighbor structure of a regular 50 × 50 grid and of the US counties.If the user knows that the matrix is symmetric, a test can be avoided with the flag cholsymmetrycheck = FALSE.Minor additional improvements consist in setting safemode = c(FALSE, FALSE, FALSE), specifying, for example, if elements of a sparse matrix should be tested for storage mode double or for the presence of NAs.The size of the Cholesky factor is determined during the symbolic factorization (Step 2c) but we need to allocate vectors in R of appropriate sizes for the Fortran call.There is a trade-off in reserving enough space to hold the factor and its structure versus computational efficiency.spam addresses this issue as follows.We have simple formulas that try to estimate the necessary sizes.If the estimated size is too small the Fortran routine returns an error to R, which allocates more space and calls the Fortran routine again.However, to save time and memory the user can also pass better estimates of the allocation sizes to chol with the argument memory = list(nnzR = ..., nnzcolindices = and one factorization and 100 updates (dashed) of a precision matrix resulting from a regular 50 × 50 grid as a function of the distance for which grid points are considered as neighbors.
The dotted line is the ratio between both curves.For distance 6 each grid point has up to 112 neighbors and the dependence structure requires at least 18 parameters.
...).The minimal sizes for a fixed sparsity structure can be obtained from a summary call.If the user specifies the permutation to be used in chol with pivot = ... the argument memory = list(nnzR = ..., nnzcolindices = ...) should be given to fully exploit the time gain of doing so.Further, the flag cholpivotcheck = FALSE improves the computational savings of manually specifying the permutation additionally.
As an illustration for the last two rows of Table 1, consider a precision matrix Qspam of class spam and perform a first decomposition Qfact <-chol(Qspam).Successive factorizations of a new precision matrix Qspamnew can be performed as follows.
R> tmp <-summary(Qfact) R> pivot <-ordering(ch1) R> spam.options(cholsymmetrycheck= FALSE, safemode = c(FALSE, FALSE, FALSE), + cholpivotcheck = FALSE) R> Qfactnew <-chol.spam(Qspamnew,pivot = pivot, + memory = list(nnzR = tmp$nnzR, nnzcolindices = tmp$nnzc))
Of course, all of the above could be also be done by the following single command.
R> Qfactnew <-update(Qfact, Qspamnew)
When approximating isotropic second order stationary Gaussian fields by GMRF (cf, Rue and Held 2005, Section 5.1), many neighbors need to be considered in the dependence structure.Figure 4 shows the total time and memory for 101 Cholesky factorizations and one factorization and 100 updates for a precision matrix resulting from a regular 50 × 50 grid as a function of the distance for which grid points are considered as neighbors.For distance 6 each grid point has up to 112 neighbors and the dependence structure requires at least 18 parameters.We refer to Rue and Held (2005) for a detailed discussion and issues arising from the approximation.
The results of this section are based on 101 Cholesky factorizations and computation time scales virtually linearly for multiples thereof.However, in a practical MCMC setting the factorization is only one part of each iteration and, additionally, the set of the valid parameters is often unknown.The first issue is addressed with competitive algorithms in spam but also needs to be considered when writing R code, see Section 6.2.A typical procedure for the second issue is to sample from a hypothetical parameter space and to use a trial-and-error approach by calling the update function and verifying if the resulting matrix is positive definite.(For simple examples, it may be possible to give bounds on the parameter space that can be used when sampling, see also Rue and Held (2005), Section 2.7.)In the cases of a non-admissible value, the functions hand back an error, a warning or the value NULL, depending on the value of a specific flag.Figure 5 illustrates the valid parameter space for the second order neighbor model of the US counties.The 'brute force' code used for Figure 5 is as follows.
R> spam.options("cholupdatesingular"= "null") On the aforementioned computer, about 50 tests are evaluated per second.Hence, it takes about 6 minutes to execute the above code.The bounds for theta.1 and theta.2 were empirically determined.

Data examples

In this section we illustrate the spam package by analyzing two datasets which are modeled using latent GMRF.Both examples are also discussed (without documenting code) in Rue and Held (2005), Sections 4.2.1 and 4.4.2, to which we refer for technical details.Rue and Held (2005) use in both cases a slightly different approach for the MCMC steps, here we illustrate spam with a conceptually simpler but computationally tougher version of the Gibbs sampler.
We assume that the observations y are conditionally independent given latent parameters η and additional parameters θ y
where π(• | •) denotes the conditional density of the first argument given the second argument.
The latent parameters η are part of a larger latent random field x, which is modeled as a GMRF with mean µ and precision matrix Q, both depending on parameters θ x ; that is,

Normal response model

Consider the R dataset "UKDriverDeaths", a time series giving the monthly totals of car drivers in Great Britain killed or seriously injured from January 1969 to December 1984 (n = 192).The series y i exhibits a strong seasonal component (denoted by s i ) and a (possibly) smooth trend (denoted by t i ).Here, we want to predict the pattern η i = s i + t i for the next m = 12 months.We assume that the square root responses are normal and conditionally independent:
We assume further that 11 j=0 s i+j , i = 1, . . ., n + 1, are independent normals with mean zero and precision κ s (an intrinsic GMRF model for seasonal variation, e.g., Rue and Held 2005, page 122) and t i − 2t i+1 − t i+2 , i = 1, . . ., n + m − 2, are independent normals with mean zero and precision κ t (an intrinsic second order random walk model).Hence,
where Q s and Q t are given by analogues of equations (3.59) and (3.40) of Rue and Held (2005).
Using independent Gamma priors for the three precisions, e.g., π(κ s ) ∝ κ αs−1
the full joint density is
.
The individual block precisions are, for example,
It is now straightforward to implement a Gibbs sampler based on the full conditionals π(s, t | κ, y) and π(κ | s, t, y).The R code to implement is as follows.We first load the data, calculate the square root counts and specify the hyperparameters of the prior for κ = (κ y , κ s , κ t ) as in Rue and Held (2005).
Note that m denotes the length of one season, the duration of our prediction.The individual block precisions are now constructed (based on unit precisions).
We construct now a "template" precision matrix of the GMRF characterized by π(s, t | κ, y) to obtain the structure of the Cholesky factor.The sparsity structure of the precision matrix and of its Cholesky factor are shown in Figure 6.
R> Qst_yk <-rbind(cbind(Qss + diag.spam(nm),Qst), + cbind(Qst, Qtt + diag.spam(nm)))R> struct <-chol(Qst_yk) The code from now on does not differ for sparse and non-sparse input matrices.We need to specify some parameters for the Gibbs sampler, initialize the arrays containing the posterior samples and starting values for κ.
R> burnin <-10 R> ngibbs <-500 R> totalg <-ngibbs + burnin R> set.seed( 14) R> spost <-tpost <-array(0, c(totalg, nm)) R> kpost <-array(0, c(totalg, 3)) R> kpost[1,] <-c(0.5, 28, 500) R> postshape <-priorshape + c(n / 2, (n + 1) / 2, (n + m -2) / 2)
The Gibbs loop is now as follows:
The loop takes a few seconds to run.After eliminating the burn-in, summary statistics can be calculated.For example, for the precisions we have: spam: MCMC Methods for Gaussian Markov Random Fields in R q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 50 100 150 200 1000 1500 2000 2500

Counts

Posterior median Quantiles of posterior sample Quantiles of predictive distribution q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 100 200 300 400 500 q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q qq q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q where q(κ , x | κ, x, y) = q(x | κ , x, y) π(κ | x, y).
We guide the reader through the R code of the Gibbs sampler, also given in demo("articlejss-example2").First we need to setup data and adjacency structure, provided in the spam package for convenience and also available from http://www.The next few commands construct templates of the individual block precision matrices as given in (2), and pre-calculate quantities, notably of (3) for i = 1.<-as.spam(diag.spam(c(rep(0, n)
The Gibbs sampler proceeds now with sampling κ and x and then calculating the acceptance probability (3) on a log scale.Note that some quantities only need to be recalculated if we accept the proposal, i.e., if (logU < logalpha) is true.

R> for (i in

After the loop, we eliminate the burn-in from the samples and proceed with the usual evaluation of the posterior sample.The right panel of Figure 8 shows the posterior median of the estimated relative risks, i.e., exp(u).Figure 9 gives several diagnostics plots for the samples of the posterior precisions κ u and κ v .Note that κ v exhibits a somewhat slow mixing.The proposal q(κ , x | κ, x, y) depends on the precision priors and when choosing substantially different priors, the acceptance rate may be much lower.
The Gibbs sampler as illustrated above takes about 5.9 seconds per 1000 iterations.Not passing the struct object to rmvnorm.canonicalincreases the total computation time by roughly a factor of 1.7 and when working with full matrices, by a factor 40.The code can be improved for slight gains in time but loosing somewhat its readability.

Discussion

This paper highlights some of the functionalities of the R package spam.However, for details we refer to the enclosed help pages.The package is based on stable and well tested code but unlikely to be entirely free of minor bugs.Also, as time evolves, we intend to enhance the package with more functionalities and more efficient algorithms or more efficient implementations thereof.The function todo() of spam sheds some insights into intended future directions.
We have motivated the need for spam and illustrated this paper with MCMC methods for GMRF.However, there are many other statistical tools that profit from the functionalities of spam, as outlined in the motivation, and many of them involve covariance matrices.Naturally, any sparse covariance matrix calls for the use of spam.Sparse covariance matrices arise from compactly supported covariance functions or from tapering (direct multiplication of a covariance function with a compactly supported one), cf.Furrer, Genton, and Nychka (2006).The R package fields (Furrer, Nychka, and Sain 2009), providing tools for spatial data, uses spam as a required package.
In contrast to the precision matrix of GMRF, the range parameter of the covariance function, which is directly related to the support, is often of interest and within an MCMC framework would be sampled as well.Changing the range changes the sparsity structure of the corresponding matrix and reusing the first steps in the factorization is not possible.However, often an upper bound of the range is known and a sparsity structure using this upper bound can be constructed.During individual factorizations, the covariance matrix is filled according to this structure and not according to the actual support of the covariance matrix.
The illustration of this paper have been done with spam 0.22-0 available from http://www.math.uzh.ch/furrer/software/spam/,where the R code is distributed under the GNU Public License and the file LICENCE contains the details of the license agreement for the Fortran code.Sources, binaries and documentation of spam are also available for download from the Comprehensive R Archive Network http://CRAN.R-project.org/package=spam.Once installed, the figures and tables of this article can be reproduced using demo("article-jss"), demo("article-jss-example1") and demo("article-jss-example2").
6.1.spam and other sparse matrix R packages spam is not the only R package for sparse matrix algebra.The packages SparseM (Koenker and Ng 2010) and Matrix (Bates and Maechler 2010) contain similar functionalities for handling sparse matrices, however, recall that both packages do not provide the possibility to split up the Cholesky factorization as discussed in this paper.We briefly discuss the major differences with respect to spam; for a detailed description see their manual.SparseM is also based on the Fortran Cholesky factorization of Ng and Peyton (1993a) using the MMD permutation and almost exclusively on SPARSKIT.It was originally designed for large least squares problems and later also ported to S4 but is in a few cases inconsistent with existing R methods.It supports different sparse storage systems.Hence, besides wrapping issues and minor Fortran optimization its computational performance is comparable to spam.
Matrix incorporates many classes for sparse and full matrices and is based on C. For sparse matrices, it uses different storage formats, defines classes for different types of matrices and uses a Cholesky factorization based on UMFPACK (Davis 2004).
It would also be interesting to compare spam and the sparse matrix routines of MATLAB, The MathWorks, Inc. ( 2007) (see Figure 6 of Furrer et al. 2006 for a comparison between SparseM and MATLAB).

More hints for efficient computation

In many settings, having a fast Cholesky factorization routine is essential but not sufficient.Compared with other sparse matrix packages, spam is very competitive with respect to sparse matrix operations.However, given the row-oriented storage scheme, some operations are inherently slow and should be used carefully.Of course, a storage format based on a column oriented scheme does not solve the problem and there is no clear advantage of one over the other (Saad 1994).In this section we give a few examples of slow operations and mention a few tips for more efficient computation.
The mentioned inefficiency is often a result of not being able to access individual elements of a matrix directly.For example, if A is a sparse matrix in spam, we do not have direct memory access to an arbitrary element a ij , but we need to search within the individual elements of the ith line, until we have reached the jth element or the position where it should be (because of the ordered column indices).
Similarly, it is much more efficient to access entire rows instead of columns.Hence, one should never subset a column of a symmetric matrix but using rows instead.Likewise, an inner product should always be calculated with x (Ax ) instead of (x A)x , the latter being equivalent to omitting the parentheses.
Finally, if A is a square matrix and D is a diagonal matrix of the same dimension, A <-D %*% (A %*% D) is be optimized as follows.
R> A@entries <-A@entries * D@entries[A@colindices] * + D@entries[rep_int(1:n, diff(A@rowpointers))]
If all R code optimization is still insufficient to enable the envisioned statistical analysis, as a last resort, there is always the possibility to implement larger blocks in Fortran or C directly.

Figure 1 :

Figure 1: The symmetric positive-definite n = 5 matrix A and the sparsity structure of A and P AP (top row).The graph associated to the matrix A and the Cholesky factors R and U of A and P AP respectively are given in the bottom row.The nodes of the graph are labeled according to A (upright) and P AP (italics).The dashed lines in U indicate the supernode partition, see Section 2 and 3.2.

Figure 2 :

Figure 2: Sparsity structure of the Cholesky factor with MMD, RCM and no permutation of a precision matrix induced by a second order neighbor structure of the US counties.The values z, w are the sizes of the sparsity structure and of the vector containing the column indices of the sparsity structure and s is the number of supernodes.

Figure 3 :

Figure 3: Total time (left) and memory usage (right) for 101 Cholesky factorizations (solid) and one factorization and 100 updates (dashed) of a precision matrix from different sizes L of regular L × L grids with a second order neighbor structure.The dotted line is the ratio between both curves.The precision matrix from L = 200 has L 4 = 1.6 • 10 9 elements.

Figure 4 :

Figure 4: Total time (left) and memory usage (right) for 101 Cholesky factorizations (solid)and one factorization and 100 updates (dashed) of a precision matrix resulting from a regular 50 × 50 grid as a function of the distance for which grid points are considered as neighbors.The dotted line is the ratio between both curves.For distance 6 each grid point has up to 112 neighbors and the dependence structure requires at least 18 parameters.

Figure 5 :

Figure 5: Valid parameter space for the second order neighbor model of the US counties.

Figure 6 :

Figure 6: The sparsity structure of the precision matrix of π(s, t | κ, y) and of its Cholesky factor.

Table 1 :

Relative (to a generic chol call) gain of time and memory usage with different options and arguments in the case of a second order neighbor structure of a regular 50 × 50 grid and of the US counties.The time and memory usage for the generic call chol are 2.1 seconds, 53.7 Megabytes and 5.2 seconds, 145.4 Megabytes, respectively.
r-inla.org/, or from http: //www.math.ntnu.no/~hrue/GMRF-book/germany.graph and http://www.math.ntnu.no/~hrue/GMRF-book/oral.txt.Next, we set the hyperparameters, define the parameters for the Gibbs sampler and allocate variables for the posterior, containing the starting values.
