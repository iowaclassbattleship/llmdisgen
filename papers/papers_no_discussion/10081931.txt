
Introduction

Regression models for scalar responses and functional predictors have become increasingly important tools for analyzing functional data. There are many applications in which these models can provide an adequate description of the data at hand. For instance, a classic example provided in {{not_in_s2orc}} uses a functional linear model to describe the association between total annual rainfall (scalar response of interest) and temperature over the course of the corresponding year (functional predictor).
Let Y i ∈ R be the scalar response of interest for observation i, i = 1, . . . , n and let X i be a random predictor process that is square integrable on a compact support I ⊂ R (i.e, I X 2 i (t)dt < ∞). The corresponding functional linear model (FLM) is given by:
where α is the scalar intercept and ε i is the error term such that ε i ∼ N (0, σ 2 ). ω is a square integrable coefficient function that relates the predictor process to the response. The magnitude of ω(t) indicates the relative importance of the predictor X i at a given value of t. If |ω(t 0 )| is large, this means that changes in the predictor process at t 0 are important in predicting the response.
We note that in model (1.1), the predictor is presented as though it is a 1-dimensional signal depending on scalar value t. However, X i could also be a 2-or higher-dimensional functional object such as an image. In the case of a 2-dimensional predictor image, t corresponds to an ordered pair and ω is a coefficient image that provides information about the association of various regions of the predictor image with the scalar outcome of interest {{19027132}}{{not_in_s2orc}}.
A variety of approaches have been developed for estimating the coefficient function in (1.1). Many of these approaches employ functional principal components regression {{not_in_s2orc}} as in {{6355943}}; {{not_in_s2orc}}; and {{15606264}}. {{51949254}} employ both penalization techniques and FPCR or functional partial least squares (FPLS). Spline-based approaches are also common for estimating ω {{28240175}}{{17912826}}. Other "highly flexible" approaches include those taken by {{1927777}} who propose a functional additive regression model and {{7837016}} who extend generalized linear models, generalized additive models, and projection pursuit regression to include functional predictors. Other more recent developments in estimating ω rely on combining the use of wavelets and sparse fitting procedures as in {{39181660}} and {{not_in_s2orc}}.
Although (1.1) can be appropriate for modeling the relationship between a scalar response and a functional predictor when the association between the response and predictor is the same for all observations, it is inadequate for settings in which the coefficient function differs across subgroups of the observations. If there are C different associations corresponding to C different coefficient functions then we can think of each observation as coming from one of C distinct subpopulations/components and would need C distinct FLMs to adequately describe the relationship between the response and the predictor. We are concerned with cases in which subpopulation membership is not observed and will need to be estimated along with the component-specific coefficient functions. As motivation for a model that accounts for heterogeneous association between a functional predictor and scalar response {{12793573}} describe a study of the association between the longevity (scalar response) and
reproductive trajectory over the first 20 days of life (functional predictor) in female Mediterranean flies. Their analysis showed evidence of the existence of two distinct subpopulations defined by two different kinds of association between reproductivity and longevity.
Several approaches have been proposed to model scenarios like that mentioned above in both classical regression (with scalar predictors and a scalar response) and scalar-on-function regression. Some rely on using a clustering algorithm followed by fitting linear models within the estimated clusters as done by {{202944608}} in the case of classical regression and {{14198165}} in the case of scalar-on-function regression. An alternative approach, which we adopt here, makes use of the theory of finite mixture regression models. When the coefficient function is thought to be different for mutually exclusive subgroups, it is natural to model the heterogeneity with a finite mixture of regressions. Finite mixture models have become an important statistical tool because of their ability to both approximate general distribution functions in a semi-parametric way and account for unobserved heterogeneity {{9503324}}. Furthermore, mixture models can provide insight into previously unknown mechanisms that relate the predictors to the response.
Although the underlying theory of finite mixture regression models and methods for estimating those models have been well-studied when the predictors are scalars {{not_in_s2orc}}{{not_in_s2orc}}, methods for finite mixture regression remain relatively undeveloped when the predictors are functions. To our knowledge, {{12793573}} are the only ones to investigate such an extension. They refer to the corresponding class of models that use the framework of finite mixture regression to model relationships between functional predictors and a scalar response in the presence of unobserved heterogeneity as functional mixture regression (FMR) models. In their approach, they first represent each functional predictor in terms of some suitably chosen number of functional principal components and apply standard mixture regression techniques in the new coordinate space.
The FMR model is given by
where C is the number of components or distinct subpopulations, α r is the rth componentspecific intercept, and ω r is the regression function for the rth group, r = 1, . . . , C.
In contrast to {{12793573}}, we propose to take a wavelet-based approach to FMR models. We focus here on the wavelet basis for several reasons. Wavelets are particularly well suited to handle many types of functional data, especially functional data that contain features on multiple scales. They have the ability to adequately represent global and local attributes of functions and can handle discontinuities and rapid changes.
Furthermore a large class of functions can be well represented by a wavelet expansion with relatively few non-zero coefficients. This is a desirable property from a computational point of view as it aids in achieving the goal of dimension reduction and will be of critical importance when the functional predictors are of very high dimension as is the case when the predictors are 2-or 3-dimensional images.
Once the model is represented in terms of an appropriate wavelet basis, we employ a lasso-type {{16162039}} 1 -penalized fitting procedure to estimate the wavelet and scaling coefficients corresponding to the component-specific coefficient functions in the mixture model.
The rest of the paper is organized as follows. In Section 2, we provide a brief discussion of wavelets and the wavelet-based functional linear model followed by specification of the wavelet-based (WB) functional finite mixture regression and adaptive wavelet-based (AWB) functional finite mixture regression models. In Section 3, we outline an EM-type algorithm for fitting WB models. Section 4 discusses the various tuning parameters in the WB and AWB models. Section 5 presents simulation results showing the performance of the WB and AWB methods and an application of our method to a real data set where we investigate the association between fractional anisotropy profiles from the corpus callosum and scores on a test of cognitive functioning in a sample of subjects with multiple sclerosis. In section 6 we conclude with a brief discussion.

Methodology

Wavelets and Wavelet Decomposition

Wavelet bases are sets of functions that can be employed in representing a wide range of functional data and have the ability to represent localized features of functions in a sparse way. Comprehensive treatment of wavelets and their applications in statistics can be found in {{not_in_s2orc}}; {{not_in_s2orc}}; and {{not_in_s2orc}}. Here, we provide a brief overview of some important aspects of wavelet bases.
In L 2 (R), a wavelet basis is generated by two kinds of functions: a father wavelet, φ(t), and a mother wavelet, ψ(t), with the following properties:
φ(t)dt = 1 and ψ(t)dt = 0.
Father wavelets, also referred to as scaling functions, serve to approximate the function of interest while mother wavelets serve to provide the detail not captured by this approximation.
Any particular wavelet basis consists of translated and dilated versions of its father and mother wavelets given by
where the integer j is the dilation index referring to the scale and k is an integer that serves as a translation index. Larger values of j correspond to scaling and wavelet functions that can provide more localized information about the function of interest.
In practice, the functional predictors and coefficient function associated with the FLM are not considered outside of a given interval. Without loss of generality, we take that interval to be [0,1]. Wavelet and scaling functions can be adapted via implementation of one of several boundary handling schemes to represent a given function on the unit interval. Hence if we assume that ω ∈ L 2 ([0, 1]) then we can represent ω in the wavelet domain by
where j 0 is an integer that determines the number of scaling functions used in the lowest scale representation. The coefficients β j 0 ,k and β j,k are the corresponding scaling and wavelet coefficients for the functions φ j 0 ,k and ψ j,k respectively and are given by
Each coefficient provides information about the characteristics of the function ω at a given scale and location.
We restrict ourselves to orthonormal wavelet families. By this we mean that φ j,k (t) and
In typical applications, the functional predictors are discretely sampled. We assume that we observe a dyadic length (N = 2 J ) vector of function values X i = (X i (t 1 ), . . . , X i (t N )) T where the arguments t 1 , . . . , t N , are equally spaced and the same for all observations. To obtain the wavelet and scaling coefficients corresponding to the functional predictors we use the discrete wavelet transform (DWT). The inverse DWT (IDWT) can be used to reconstruct a vector of functional observations from its corresponding wavelet and scaling coefficients.
Both the DWT and IDWT can be performed using a computationally fast pyramid algorithm {{2356353}}).
Wavelet Representation of the FLM

Before moving on to our WB model we first review the WBFLM proposed by {{39181660}}. In the this model, both the coefficient function, ω and the functional predictor X i from (1.1) are expressed in terms of an appropriate wavelet basis. ω(t) can be expressed as in (2.1) and X i (t) can be expressed as
where the scaling and wavelet coefficients are given respectively by
In practice, given N equally-spaced observations of X i , the corresponding wavelet and scaling coefficients can be calculated using the DWT. These coefficients can be be put into an (N + 1) × 1 vector denoted by Z i (here we include 1 as the first element of the vector which will correspond to the intercept in the regression model) having the form
where J = log 2 (N ) − 1 and k j = 2 j − 1.
Because of the orthonormality of the wavelet basis, (1.1) can be simply written as
since all of the cross-product terms comprising the product of the wavelet-domain representations of X i and ω integrate to zero {{39181660}}, or, in matrix notation:
vector containing the intercept α followed by the coefficients arranged in the same order as the vector Z i , and ε = (ε 1 , . . . , ε n ) T .
Thus we see that once the functions ω and X i from (1.1) have been represented in the wavelet domain, the scaling and wavelet coefficients corresponding to X i , namely the elements of Z i , become the predictors in the transformed space. In the wavelet representation of (1.1) there are N + 1 parameters to estimate (including the intercept).
Specification of the WB Functional Mixture Regression Model

If the pairs of functional predictors and scalar responses come from a heterogeneous population, where the subpopulations (or components) are determined by C distinct associations between the predictors and the response, then there is a unique coefficient function, ω r , r = 1, . . . , C corresponding to each subpopulation. Since, as noted above, the predictors are typically discretely sampled at N points, the model we consider is
Thus, the coefficient functions of interest are given by
and our goal is to find estimates for the β r,j 0 ,k 's and the β r,j,k 's.
In this setting, the model of interest is similar to that seen in classical finite mixture regression. We have that Y i |Z i independent for i = 1, . . . , n and
where β r is the component-specific coefficient vector for component r, with the same form as β in the model given by (2.3), σ 2 r is the corresponding component-specific error variance, and π r is the probability that observation i belongs to component r. Let ξ = (β 1 , . . . , β C , σ 1 , . . . , σ C , π 1 , . . . , π C−1 ) ∈ R C(N +1) × R C >0 × Π be the ((N + 3) · C − 1) × 1 vector of free parameters to be estimated from (2.4), where Π is the space of vectors of the form (π 1 , . . . , π C−1 ) such that π r > 0 for r = 1, . . . , C − 1, C−1 r=1 π r < 1, and π C = 1 − C−1 r=1 π r . In practice, X i may be densely sampled and so we may have N n. In this case, maximum likelihood estimation will provide inaccurate and unstable estimates for each β r and consequently poor estimates for each ω r . Since wavelets allow for sparse representation of each ω r , we may assume that most elements of β r are negligible and thus we consider a lasso-type procedure for estimating the C component-specific vectors of wavelet and scaling coefficient values. {{52062868}} proposed an 1 -penalized mixture regression procedure for model fitting with general high-dimensional predictors. We make use of this procedure here. We begin by first reparameterizing model (2.4) using the following:
Based on this new parameterization, model (2.4) can be written as:
There is a one-to-one mapping from ξ in (2.4) to a new parameter vector
The corresponding log-likelihood for model (2.5) is
To estimate the parameter vector θ in model (2.5), we propose to useθ
where ϕ r 1 is the 1 -norm of the vector ϕ r . Note that the penalty on each wavelet and scaling component coefficient vector ϕ r is proportional to the mixing probability π r . Including the mixing proportion in this manner corresponds to the common practice of relating the amount of penalty to the sample size, where, in the context of mixture regression, {{15895493}} note that the "virtual" sample size from the rth component is proportional to π r .
Further discussion of the tuning parameters is given in Section 4.
Estimation of ϕ r and ρ r rather than the direct estimation of β r and σ r is considered primarily for two reasons. The reparametrization, along with a lasso-type penalty allows for penalization of both the coefficient vectors of interest and the error variances within each component {{52062868}} while maintaining convexity of the optimization problem to be solved.
The Adaptive Model

We also consider an adaptive version of the estimator,θ λ , above. {{13998761}} proposed the adaptive lasso which allows for differing weights, adaptively chosen, to be assigned to the coefficients in the 1 penalty. Among other benefits, the use of such weights can serve to provide better performance with respect to variable selection in high dimensional settings.
In the two-stage adaptive lasso procedure, one first finds initial estimates for the coefficients of interest and then uses these estimates (or a transformation of them) as weights for the coefficients in the 1 penalty of the lasso procedure.
Our adaptive estimator is denoted byθ adapt;λ and minimizes the following criterion which involves a re-weighted 1 -norm penalty term:
where ϕ r,q is the qth element of the vector ϕ r and w r,q = 1/ |φ r,q | whereφ r,q is the qth element of the WB functional mixture regression estimate of component vector r.
Since it is possible that some theφ r,q values can be zero, which would cause the corresponding w r,q values to be infinite, we add a small constant (0.001) to eachφ r,q estimate in the fitting algorithm.
Note that we follow {{52062868}} and use estimates from the WB functional mixture regression in our weights, but other weighting strategies may also be considered.
Fitting WB Functional Mixture Models

Fitting of WB functional mixture regression models is carried out in three main steps: (1) Use the DWT to obtain the wavelet and scaling coefficients corresponding to the functional predictors, (2) use an EM-type algorithm for computing parameter estimates in the wavelet domain, and (3) use the IDWT to obtain estimates of the component-specific coefficient functions in the original domain from the corresponding wavelet and scaling coefficient estimates.
Each step is explained in detail below.
Step 1. Use the DWT to decompose the functional predictors and obtain the corresponding wavelet and scaling coefficients for each predictor. Here we must choose the wavelet family (e.g., Daubechies' least asymmetric wavelets), number of vanishing moments, lowest level of decomposition (j 0 ∈ {0, . . . , log 2 (N ) − 1}), and method for handling the boundaries (e.g., symmetric boundary handling).
The empirical wavelet and scaling coefficients for each predictor curve can be arranged into (N + 1) × 1 vectors, denoted Z i , i = 1, . . . , n, which have the same structure as (2.2).
We then form Z, an n × (N + 1) matrix with ith row Z i .
Step 2. We carry out an EM-type algorithm for our setting in a manner similar to that described in {{52062868}}. Consider the unobserved random indicator variable ∆ i,r which designates component membership:
Then the expected scaled complete negative log-likelihood and penalized negative loglikelihood are given by
respectively.
In the E-step of the fitting procedure, we replace each unobserved group membership indicator, ∆ i,r , with its expected valuê
C−1 ) corresponds to the current vector of values for the parameters at EM-iteration m. Hence we can compute Q(θ|θ (m) ).
The M-step of the fitting procedure is carried out in two stages. First, we fix each
π r ϕ r 1 with respect to π; π r > 0 for r = 1, . . . , C and C r=1 π r = 1 according to the procedure described in {{52062868}}. This yields an updated estimate of the π vector, π (m+1) .
In the second stage of the M-step, we improve with respect to ϕ and ρ. At this stage, we note that the optimization problem decouples into C distinct convex optimization problems where we seek to minimize each of
with respect to ϕ r and ρ r . To solve this set of optimization problems, we implement a coordinate descent algorithm that updates one coordinate at a time while holding the other coordinates fixed at their current values. The update for ρ r is given by {{52062868}} 
Here ·, · refers to the vector inner product and · is the Euclidean norm. Once the update for ρ r is computed, we calculate the update for the unpenalized component-specific intercept
where Z i,q is the qth element of the vector Z i .
The coordinate-wise updates for the remaining N coefficients in each ϕ r vector are computed as
whereZ ,q = ∆ i,r Z ,q with Z ,q being the qth column of Z and where we define S q by
r,s Z ,q ,Z ,s , for q = 2, . . . , N + 1. The E-and M-steps are iterated until some convergence criteria are satisfied which ensure that the relative improvement in λ (θ) and the relative change in the parameter vector are small. Specifically, the EM procedure stops when
where θ q refers to the qth element of the parameter vector θ and we set τ = 10 −6 .
Step 3. Use the IDWT to obtain estimatesω 1 , . . . ,ω C from the estimatesσ 1φ1 , . . . ,σ 1φC
respectively.
The EM procedure discussed in
Step 2 above requires that we provide initial values for the parameters being estimated. We use the following scheme for obtaining these initial values. We first assign a weight to each observation corresponding to each of the C distinct components. To do this, we randomly assign to each observation i a class, κ, from the set
Tuning Parameters and Their Selection

In using WBFMR we need to specify a number of tuning parameters, namely, the number of components, C, the lowest level of decomposition, j 0 , and the penalty parameter, λ. The choice of the values for each of these tuning parameters may be based on prior information, otherwise data-driven methods may be employed in their selection. Below, we discuss each tuning parameter and consider two possible data-driven methods for their selection.
If the number of components is known a priori or exploratory data analysis suggests a particular number of components, then C can be specified outright. However, we often employ the mixture modeling approach when the number of components is unknown or knowledge of component membership is unavailable.
The value of j 0 corresponds to the lowest level of decomposition and can range from 0 to log 2 (N )−1. Since the predictors are sampled at N points, the DWT provides a decomposition that uses a total of N wavelet and scaling functions. Among this set of N basis functions, 2 j 0 will be scaling functions and N − 2 j 0 will be wavelet functions. Hence, setting j 0 close or equal to 0 results in using fewer scaling functions to represent large-scale features and more wavelet functions to represent local details of the function of interest. Conversely, setting j 0 close or equal to log 2 (N ) − 1 results in using more scaling functions and fewer wavelet functions.
The value of λ directly determines the role that the penalty function will have in both estimating and selecting variables in the model. Large values of λ force elements of the estimated component coefficient vectors to zero while small values result in many non-zero estimates.
We will employ two methods for tuning parameter selection. First we consider selecting the parameters that minimize the cross-validated value − 2 (θ j 0 ,λ,C ; Y ).
(4.1)
Here, (·; ·) denotes the log-likelihood from (2.6) and the estimateθ j 0 ,λ,C depends on the values of the tuning parameters as indexed by the subscripts. We will refer to (4.1) as the "predictive loss". We also consider selecting the parameters that minimize a modified BIC criterion. We use the modified BIC measure, proposed by {{16239564}}, which is given by
where d e = (N + 3) · C − 1 − q 0 is the effective number of parameters with q 0 being the number of coefficients estimated to be zero in all of the components. We can compute this value over a grid of candidate values for all or a subset of the turning parameters.
The cross-validation procedure generally puts more emphasis on predictive ability and chooses a model that performs well in this regard. On the other hand, BIC focuses more on finding the "true" model and often chooses a simpler one. Compared to BIC, cross-validation is computationally demanding and can be prohibitive for large and/or high-dimensional data.
Simulations and Application

We present simulation results that demonstrate various aspects of the WBFMR and AWBFMR procedures and that draw comparisons to a functional principal components-based (FPC) method similar to that proposed by {{12793573}}. For each simulation discussed below, we generated observations consisting of a discretely sampled one-dimensional functional predictor signal, X i , and a scalar response, Y i whose association with X i depends on some known group membership.
Each functional predictor is a Brownian bridge stochastic process for t ∈ (0, 1) with an expected value of 0, covariance given by cov(X i (t), X i (s)) = s(1 − t) for s < t, and with X i (0) = X i (1) = 0. We consider various sampling densities for the functional predictors.
Specifically, we consider data sets where the functional predictors are sampled at N = 64, 128, 256, or 512 equally-spaced points. A sample of three of these predictors are given in the left panel of Figure 1.
The scalar outcomes corresponding to each functional predictor were generated using two distinct settings for the component-specific coefficient functions. The first pair of componentspecific coefficient functions are given by ω s1 (t) = −sin(2πt) and ω s2 (t) = sin(πt). The second pair of component-specific coefficient functions are given by ω b1 (t) = −3.257e −a(t−0.15) 2 + 4.886e −a(t−0.25) 2 −3.257e −a(t−0.5) 2 +2.606e −a(t−0.9) 2 and ω b2 (t) = 3.257e −a(t−0.1) 2 −4.886e −a(t−0.35) 2 + 3.257e −a(t−0.7) 2 where a = 20000/9. The middle panel of Figure 1 shows ω s1 and ω s2 which we will refer to as the "smooth" functions while the right panel shows ω b1 and ω b2 which we will refer to as the "bumpy" functions. In addition to considering different component-specific coefficient settings, we also consider different signal-to-noise ratio settings. Equal proportions of observations were generated in each component with σ 1 = σ 2 and the discrete approximation to R 2 = 2 r=1 π r var(Xω r )/ 2 r=1 π r (var(Xω r ) + σ 2 r ) takes on the desired value in a given setting. We consider settings with R 2 values of 0.9, 0.7, and 0.5 corresponding to "high", "medium", and "low" signal-to-noise ratios respectively.
We employ Daubechies' least asymmetric wavelets with eight vanishing moments in all simulations. The WaveThresh package in R {{not_in_s2orc}}) is used to perform the DWT and IDWT with the periodic boundary handling option.
Simulation 1: Comparison of FMR Methods

In The training set is used to fit a model for each combination of the tuning parameters. The validation set is then used to select the combination of tuning parameters that minimizes (4.1) among all combinations of tuning parameters. Finally, the model estimated from the validation set is applied to the test set and the corresponding predictive loss is computed.
We repeat this procedure 100 times for each setting.
In this first set of simulations, we treat the number of components as known, i.e., C = 2.
For the wavelet-based methods, we fix the lowest level of decomposition to j 0 = 0 in the smooth setting and to j 0 = 5 in the bumpy setting. In extensive prior simulations (not shown here), these decomposition levels tended to consistently minimize the predictive loss for each of the settings that we consider here. Additional support for these choices is provided by the results in Simulation 2 where we note that the plots in Figure 7 show little difference between the distribution of the losses when j 0 is set to the values that we selected and the distributions when j 0 is allowed to be chosen by either cross-validation or BIC. To choose an optimal value for λ in a given setting, we first fit the model based on the training data for the corresponding test data and the predictive loss was obtained. We use the FlexMix packge {{11031789}} in R to fit the functional principal components-based models.
We first consider how the three methods compare with respect to predictive loss based on the test sets. The boxplots in Figure 2 show the predictive loss in the test set for the 100 simulation runs for each of the three methods at the various settings. Lower loss values are preferred. In the smooth setting (top row), we note that the wavelet-based and adaptive wavelet-based methods perform comparably to the functional principal components-based method while in the bumpy setting (bottom row), the wavelet-based methods appear to do better, especially for higher values of R 2 .
Estimation performance is illustrated in Figures 3-6 for R 2 = 0.9 and 0.7 respectively. (Performance for R 2 = 0.5 is not shown but is similar to that of R 2 = 0.7). In all settings for which the true component coefficient functions are smooth, we note that the functional principal components-based method appears to do best while the wavelet-based methods perform similarly well when the functional predictors are densely sampled. Substantial gains in estimation performance by the wavelet-based methods are evident in Figures 5 and 6 which show the average estimation performance of the three methods in the bumpy setting for R 2 = 0.9 and 0.7 respectively. (Again, performance for R 2 = 0.5 is not shown but is similar to that of R 2 = 0.7). We note that the wavelet-based methods do very well in capturing the local features of the component coefficient functions and in estimating regions where there is no association between the functional predictors and the response while the functional principal components-based method struggles with both of these tasks.
It is interesting to note that the wavelet-based and adaptive wavelet-based methods perform nearly identically in the simulations discussed above. Comparison (not shown here) of the wavelet and scaling coefficient estimates given by the wavelet-based and corresponding
adaptive wavelet-based methods shows that the adaptive version is performing additional variable selection and producing different estimates from those given by the non-adaptive procedure, but these changes do not yield substantial gains in reducing either predictive loss or estimation error.
Simulation 2: Tuning Parameter Selection Methods

In the second set of simulations, we investigate selection methods for the tuning parameters in the wavelet-based model. We compare selection based on minimizing the 5-fold crossvalidated log-likelihood loss to that based on minimizing the modified BIC criteria given in (4.2). We consider three different scenarios for tuning parameter selection:
Scenario 1. Set C = 2 and j 0 = 0 (smooth setting) or 5 (bumpy setting); select λ.
Scenario 2. Set j 0 = 0 (smooth setting) or 5 (bumpy setting); select C ∈ {1, 2, 3} and λ.
Scenario 3. Set C = 2; select j 0 ∈ {0, . . . , log 2 (N ) − 1} and λ.     We restrict ourselves to a subset of four of the 24 settings from the first group of simulations discussed above. Specifically, we compare the three tuning parameter selection scenarios in the smooth and bumpy settings when the sampling density of the functional predictors is either 128 or 256. In all four settings we have R 2 = 0.9. For each of 100 simulation runs at each setting, the training set was used to determine the optimal tuning parameters that either minimized the 5-fold cross-validated predictive log-likelihood loss or minimized the modified BIC criteria. The corresponding test set was used to estimate the test loss in each scenario for both selection methods.
The boxplots showing these log-likelihood loss values for the test sets are provided in Figure 7. The three tuning parameter selection scenarios appear to be comparable with respect to predictive log-likelihood loss across the four settings.
In Scenario 2, we allowed the data to select the number of components, C. Table 1 shows the proportions of simulation runs at each of the four settings for which the number of components was chosen to be 1, 2, or 3. The table suggests that, relative to using the modified BIC for selecting the number of components, 5-fold cross validation has greater tendency to overfit by estimating more components than truly exist.
Application to DTI Data for Subjects with Multiple Sclerosis

We now analyze data from a diffusion tensor imaging (DTI) study, discussed in {{40445189}}, using our wavelet-based functional mixture regression approach. The data are from a longitudinal study investigating the cerebral white matter tracts of subjects with multiple sclerosis (MS) recruited from an outpatient neurology clinic and healthy controls who were recruited from the community. Here we focus on the baseline observations for the 100 MS subjects. In particular, we are interested in the relationship between the fractional anisotropy profile (FAP) from the corpus callosum (functional predictor) and the Paced Auditory Serial Addition Test (PASAT) score (scalar response).
The PASAT is an assessment tool that measures a subject's cognitive ability with respect to auditory information processing speed and flexibility and also provides information on calculation ability {{43840001}}. The PASAT score is the number of correct answers out of 60 questions and thus ranges from 0 to 60. Lower scores are generally taken to indicate some level of dysfunction. The functional predictor of interest is the FAP from the corpus callosum which is derived from DTI, a magnetic resonance imaging modality that is commonly used to track the diffusion of water in biological tissue. The FAP is a continuous summary of water diffusivity that is parametrized by the arc length along a curve. The tract profiles are estimated via an automated tract-probability-mapping scheme described in {{8051725}}. In the data set, the FAP predictors are recorded at 93 locations along the corpus callosum. In our analysis, we linearly interpolate the FAP curves at 128 equally spaced points before projecting them onto a wavelet basis. We used data from 99 of the 100 MS subjects since one subject had missing FAP values at several locations along the tract. Figure 8 shows the FAPs for all 99 MS subjects that we considered as well as those for three subjects with the lowest, median, and highest PASAT scores.
We were interested in conducting an analysis that inspects whether the regression relationship between corpus callosum FAPs and PASAT scores varies due to some unknown mechanism. In the top plot of Figure 8, we note that there is no obvious grouping in the  We apply our WB functional mixture regression approach in which we used the BIC from (4.2) to select the optimal tuning parameters. This approach suggests that there are two distinct groups with different coefficient functions describing the association between corpus callosum FAP and PASAT score. Figure 9 shows the estimated coefficient functions, ω 1 andω 2 , for each of the two groups. For illustration, Figure 9 also shows the FAPs that belong to the groups associated with those functions. To determine which group a subject's FAP belongs to, we use the estimated group membership indicators from the last iteration of the EM algorithm. The indicator with the the highest value (i.e., max ∆ i,1 ,∆ i,2 for subject i) was taken to correspond to the group from which the observation came. Using this assignment method, there are 52 subjects belonging to Group 1 and 47 belonging to Group 2.
From Figure 9 we note that the estimated coefficient function corresponding to Group 2 is identically zero at all locations along the profile suggesting no association between FAP and PASAT score among MS subjects belonging to this group whereas the estimated coefficient function for Group 1 suggests that higher fractional anisotropy values between profile locations of about 0.2 and 0.7 are associated with higher PASAT scores while higher values between profile locations of about 0.7 and 0.9 are associated with lower PASAT scores for those MS subjects belonging to Group 1. Figure 10 shows the PASAT scores corresponding to the two groups. This plot illustrates a distinctive split between the two groups with respect to PASAT score. Overall the model may suggest that, among MS subjects with better cognitive function, there is no association between corpus callosum FAP and PASAT score whereas among those with worse cognitive function, fractional anisotropy values in the middle region of the tract can discriminate among the PASAT scores and that greater fractional anisotropy corresponds to higher scores.
With respect to the the properties that characterize the estimated components, the application of our method to the DTI data resulted in findings that are similar to those found in an application of the FPC-based FMR method used in {{12793573}}. Here we are referring to the analysis of the association between early reproductivity (functional predictor) and longevity (scalar response) in Mediterranean fruit flies. As in our application, {{12793573}} found that their approach suggested there were two groups of flies corresponding to two different regression structures that characterized the association between early fertility and longevity. Furthermore, upon examining the distribution of the response within each group, they found that one group tended to consist of flies with greater longevity while the other group consisted of flies with shorter longevity; similar to how the estimated groups in our DTI example show a distinction by higher and lower PASAT score.
Finally, we compare the chosen wavelet-based function mixture regression model to the wavelet based FLM (selected to minimize BIC) with respect to leave-one-out cross-validated
is the predicted PASAT score for subject i from a model fit on data with subject i removed. To determine which estimated coefficient function to use to obtain the predicted PASAT score for subject i, we use the following ad hoc method similar to that used in {{12793573}}: if the observed PASAT score Y i is less than 50 then we use the coefficient function that is not identically zero at each profile location and if Y i is 50 or larger then we use the zero function.
For our model with 2 groups, the CVRPE is 0.0315 and 0.0723 for the wavelet based FLM.


CITED_PAPERS:


11031789:FlexMix: A general framework for finite mixture models and latent class regression in R


Introduction

Finite mixture models have been used for more than 100 years, but have seen a real boost in popularity over the last decade due to the tremendous increase in available computing power.The areas of application of mixture models range from biology and medicine to physics, economics and marketing.On the one hand these models can be applied to data where observations originate from various groups and the group affiliations are not known, and on the other hand to provide approximations for multi-modal distributions (Everitt and Hand 1981;Titterington, Smith, and Makov 1985;McLachlan and Peel 2000).
In the 1990s finite mixture models have been extended by mixing standard linear regression models as well as generalized linear models (Wedel and DeSarbo 1995).An important area of application of mixture models is market segmentation (Wedel and Kamakura 2001), where finite mixture models replace more traditional cluster analysis and cluster-wise regression techniques as state of the art.Finite mixture models with a fixed number of components are usually estimated with the expectation-maximization (EM) algorithm within a maximum likelihood framework (Dempster, Laird, and Rubin 1977) and with MCMC sampling (Diebolt and Robert 1994) within a Bayesian framework.
The R environment for statistical computing (R Development Core Team 2004) features several packages for finite mixture models, including mclust for mixtures of multivariate Gaussian distributions (Fraley and Raftery 2002b,a), fpc for mixtures of linear regression models (Hennig 2000) and mmlcr for mixed-mode latent class regression (Buyske 2003).
There are three main reasons why we have chosen to write yet another software package for EM estimation of mixture models:
• The existing implementations did not cover all cases we needed for our own research (mainly marketing applications).
• While all R packages mentioned above are open source and hence can be extended by the user by modifying the source code, we wanted an implementation where extensibility is a main design principle to enable rapid prototyping of new mixture models.
• We include a sampling-based variant of the EM-algorithm for models where weighted maximum likelihood estimation is not available.FlexMix has a clean interface between E-and M-step such that variations of both are easy to combine.
This paper is organized as follows: First we introduce the mathematical models for latent class regression in Section 2 and shortly discuss parameter estimation and identifiability.Section 3 demonstrates how to use FlexMix to fit models with the standard driver for generalized linear models.Finally, Section 4 shows how to extend FlexMix by writing new drivers using the well-known model-based clustering procedure as an example.

Latent class regression

Consider finite mixture models with K components of form
where y is a (possibly multivariate) dependent variable with conditional density h, x is a vector of independent variables, π k is the prior probability of component k, θ k is the component specific parameter vector for the density function f , and
and Equation (1) describes a mixture of standard linear regression models, also called latent class regression or cluster-wise regression (DeSarbo and Cron 1988).If f is a member of the exponential family, we get a mixture of generalized linear models (Wedel and DeSarbo 1995), known as GLIMMIX models in the marketing literature (Wedel and Kamakura 2001).For multivariate normal f and x ≡ 1 we get a mixture of Gaussians without a regression part, also known as model-based clustering.
The posterior probability that observation (x, y) belongs to class j is given by
The posterior probabilities can be used to segment data by assigning each observation to the class with maximum posterior probability.In the following we will refer to f (•|•, θ k ) as mixture components or classes, and the groups in the data induced by these components as clusters.

Parameter estimation

The log-likelihood of a sample of N observations {(x 1 , y 1 ), . . ., (x N , y N )} is given by
and can usually not be maximized directly.The most popular method for maximum likelihood estimation of the parameter vector ψ is the iterative EM algorithm (Dempster et al. 1977):
Estimate the posterior class probabilities for each observation pnk = P(k|x n , y n , ψ) using Equation ( 2) and derive the prior class probabilities as
pnk Maximize the log-likelihood for each component separately using the posterior probabilities as weights max
The E-and M-steps are repeated until the likelihood improvement falls under a pre-specified threshold or a maximum number of iterations is reached.
The EM algorithm cannot be used for mixture models only, but rather provides a general framework for fitting models on incomplete data.Suppose we augment each observation (x n , y n ) with an unobserved multinomial variable z n = (z n1 , . . ., z nK ), where z nk = 1 if (x n , y n ) belongs to class k and z nk = 0 otherwise.The EM algorithm can be shown to maximize the likelihood on the "complete data" (x n , y n , z n ); the z n encode the missing class information.If the z n were known, maximum likelihood estimation of all parameters would be easy, as we could separate the data set into the K classes and estimate the parameters θ k for each class independently from the other classes.
If the weighted likelihood estimation in Equation ( 4) is infeasible for analytical, computational, or other reasons, then we have to resort to approximations of the true EM procedure by assigning the observations to disjoint classes and do unweighted estimation within the groups: max
This corresponds to allow only 0 and 1 as weights.
Possible ways of assigning the data into the K classes are
• hard assignment to the class with maximum posterior probability p nk , the resulting procedure is called maximizing the classification likelihood by Fraley and Raftery (2002b).
Another idea is to do
• random assignment to classes with probabilities p nk , which is similar to the sampling techniques used in Bayesian estimation (although for the z n only).
Well known limitations of the EM algorithm include that convergence can be slow and is to a local maximum of the likelihood surface only.There can also be numerical instabilities at the margin of parameter space, and if a component gets to contain only a few observations during the iterations, parameter estimation in the respective component may be problematic.E.g., the likelihood of Gaussians increases without bounds for σ 2 → 0. As a result, numerous variations of the basic EM algorithm described above exist, most of them exploiting features of special cases for f .

Identifiability

An open question is still identifiability of many mixture models.A comprehensive overview of this topic is beyond the scope of this paper, however, users of mixture models should be aware of the problem:
Relabelling of components: Mixture models are only identifiable up to a permutation of the component labels.For EM-based approaches this only affects interpretation of results, but is no problem for parameter estimation itself.
Overfitting: If a component is empty or two or more components have the same parameters, the data generating process can be represented by a smaller model with fewer components.This kind of unidentifiability can be avoided by requiring that the prior weights π k are not equal to zero and that the component specific parameters are different.

Generic unidentifiability:

It has been shown that mixtures of univariate normal, gamma, exponential, Cauchy and Poisson distributions are identifiable, while mixtures of discrete or continuous uniform distributions are not identifiable.A special case is the class of mixtures of binomial and multinomial distributions which are only identifiable if the number of components is limited with respect to, e.g., the number of observations per person.See Everitt and Hand (1981), Titterington et al. (1985), Grün (2002) and references therein for details.
FlexMix tries to avoid overfitting because of vanishing prior probabilities by automatically removing components where the prior π k falls below a user-specified threshold.Automated diagnostics for generic identifiability are currently under investigation.Relabelling of components is in some cases more of a nuisance than a real problem ("component 2 of the first run may be component 3 in the second run"), more serious are interactions of component relabelling and categorical predictor variables, see Grün and Leisch (2004) for a discussion and how bootstrapping can be used to assess identifiability of mixture models.

Using FlexMix

The standard M-step FLXMRglm() of FlexMix is an interface to R's generalized linear modelling facilities (the glm() function).As a simple example we use artificial data with two latent classes of size 100 each:
with ϵ ∼ N (0, 9) and prior class probabilities π 1 = π 2 = 0.5, see the left panel of Figure 1.
We can fit this model in R using the commands gives the estimated prior probabilities πk , the number of observations assigned to the corresponding clusters, the number of observations where p nk > δ (with a default of δ = 10 −4 ), and the ratio of the latter two numbers.For well-seperated components, a large proportion of observations with non-vanishing posteriors p nk should also be assigned to the corresponding cluster, giving a ratio close to 1.For our example data the ratios of both components are approximately 0.7, indicating the overlap of the classes at the cross-section of line and parabola.Histograms or rootograms of the posterior class probabilities can be used to visually assess the cluster structure (Tantrum, Murua, and Stuetzle 2003), this is now the default plot method for "flexmix" objects (Leisch 2004a).Rootograms are very similar to histograms, the only difference is that the height of the bars correspond to square roots of counts rather than the counts themselves, hence low counts are more visible and peaks less emphasized.
Usually in each component a lot of observations have posteriors close to zero, resulting in a high count for the corresponing bin in the rootogram which obscures the information in the other bins.To avoid this problem, all probabilities with a posterior below a threshold are ignored (we again use 10 −4 ).A peak at probability 1 indicates that a mixture component is well seperated from the other components, while no peak at 1 and/or significant mass in the middle of the unit interval indicates overlap with other components.In our simple example the components are medium well separated, see Figure 2.
Tests for significance of regression coefficients can be obtained by   Both the summary table and the rootograms in Figure 3 clearly show that the clusters of the Poisson response have much more overlap.For our simple low-dimensional example data the overlap of the classes is obvious by looking at scatterplots of the data.For data in higher dimensions this is not an option.The rootograms and summary tables for "flexmix" objects work off the densities or posterior probabilities of the observations and thus do not depend on the dimensionality of the input space.While we use simple 2-dimensional examples to demonstrate the techniques, they can easily be used on high-dimensional data sets or models with complicated covariate structures.

Multiple independent responses

If the response y = (y 1 , . . ., y D ) ′ is D-dimensional and the y d are mutually independent the mixture density in Equation ( 1) can be written as
To specify such models in FlexMix we pass it a list of models, where each list element corresponds to one f d , and each can have a different set of dependent and independent variables.Note that now three model formulas are involved: An overall formula as first argument to function flexmix() and one formula per response.The latter ones are interpreted relative to the overall formula such that common predictors have to be specified only once, see help("update.formula")for details on the syntax.The basic principle is that the dots get replaced by the respective terms from the overall formula.The rootograms show that the posteriors of the two-response model are shifted towards 0 and 1 (compared with either of the two univariate models), the clusters are now well-separated.Note that convergence of the EM algorithm is much faster with grouping and the two clusters are now perfectly separated.

Repeated measurements


Control of the EM algorithm

Details of the EM algorithm can be tuned using the control argument of function flexmix().E.g., to use a maximum number of 15 iterations, report the log-likelihood at every 3rd step and use hard assignment of observations to clusters (cf.page 4) the call is R> m5 <-flexmix(yn ~x + I(x^2), data = NPreg, k = 2, + control = list(iter.max= 15, verbose = 3, classify = "hard"))
Classification: hard 3 Log-likelihood : -722.8041 5 Log-likelihood :
-722.5276 converged Another control parameter (minprior, see below for an example) is the minimum prior probability components are enforced to have, components falling below this threshold (the current default is 0.05) are removed during EM iteration to avoid numerical instabilities for components containing only a few observations.Using a minimum prior of 0 disables component removal.

Automated model search

In real applications the number of components is unknown and has to be estimated.Tuning the minimum prior parameter allows for simplistic model selection, which works surprisingly well in some situations:
Cluster sizes: 1 2 122 78 convergence after 103 iterations Although we started with four components, the algorithm converged at the correct two component solution.
A better approach is to fit models with an increasing number of components and compare them using AIC or BIC.As the EM algorithm converges only to the next local maximum of the likelihood, it should be run repeatedly using different starting values.The function stepFlexmix() can be used to repeatedly fit models, e.g.,

R> m7 <-stepFlexmix(yp ~x + I(x^2), data = NPreg


Extending FlexMix

One of the main design principles of FlexMix was extensibility, users can provide their own M-step for rapid prototyping of new mixture models.FlexMix was written using S4 classes and methods (Chambers 1998) as implemented in R package methods.
The central classes for writing M-steps are "FLXM" and "FLXcomponent".Class "FLXM" specifies how the model is fitted using the following slots: fit: A function(x,y,w) returning an object of class "FLXcomponent".defineComponent: Expression or function constructing the object of class "FLXcomponent".weighted: Logical, specifies if the model may be fitted using weighted likelihoods.If FALSE, only hard and random classification are allowed (and hard classification becomes the default).
formula: Formula relative to the overall model formula, default is .~.

name:

A character string describing the model, this is only used for print output.
The remaining slots of class "FLXM" are used internally by FlexMix to hold data, etc. and omitted here, because they are not needed to write an M-step driver.The most important slot doing all the work is fit holding a function performing the maximum likelihood estimation described in Equation ( 4).The fit() function returns an object of class "FLXcomponent" which holds a fitted component using the slots: logLik: A function(x,y) returning the log-likelihood for observations in matrices x and y.
predict: A function(x) predicting y given x.

df:

The degrees of freedom used by the component, i.e., the number of estimated parameters.
parameters: An optional list containing model parameters.In a nutshell class "FLXM" describes an unfitted model, whereas class "FLXcomponent" holds a fitted model.

Writing an M-step driver

Figure 5 shows an example driver for model-based clustering.We use function dmvnorm() from package mvtnorm for calculation of multivariate Gaussian densities.In line 5 we create a new "FLXMC" object named retval, which is also the return value of the driver.Class "FLXMC" extends "FLXM" and is used for model-based clustering.It contains an additional slot with the name of the distribution used.All drivers should take a formula as their first argument, this formula is directly passed on to retval.In most cases authors of new FlexMix drivers need not worry about formula parsing etc., this is done by flexmix itself.In addition we have to declare whether the driver can do weighted ML estimation (weighted=TRUE) and give a name to our model.
The remainder of the driver creates a fit() function, which takes regressors x, response y and weights w.For multivariate Gaussians the maximum likelihood estimates correspond to mean and covariance matrix, the standard R function cov.wt() returns a list containing estimates of the weighted covariance matrix and the mean for given data.Our simple example performs clustering without a regression part, hence x is ignored.If y has D columns, we estimate D parameters for the mean and D(D − 1)/2 parameters for the covariance matrix, giving a total of (3D + D 2 )/2 parameters (line 11).As an additional feature we allow the user to specify whether the covariance matrix is assumed to be diagonal or a full matrix.For diagonal=TRUE we use only the main diagonal of the covariance matrix (line 14) and the number of parameters is reduced to 2D.
In addition to parameter estimates, flexmix() needs a function calculating the log-likelihood of given data x and y, which in our example is the log-density of a multivariate Gaussian.In addition we have to provide a function predicting y given x, in our example simply the mean of the Gaussian.Finally we create a new "FLXcomponent" as return value of function fit().
Note that our internal functions fit(), logLik() and predict() take only x, y and w as arguments, but none of the model-specific parameters like means and covariances, although they use them of course.R uses lexical scoping rules for finding free variables (Gentleman and Ihaka 2000), hence it searches for them first in the environment where a function is defined.E.g., the fit() function uses the variable diagonal in line 24, and finds it in the environment where the function itself was defined, which is the body of function mymclust().Function logLik() uses the list para in lines 8 and 9, and uses the one found in the body of defineComponent().Function flexmix() on the other hand never sees the model parameters, all it uses are function calls of form fit(x,y,w) or logLik(x,y), which are exactly the same for all kinds of mixture models.In fact, it would not be necessary to even store the component parameters in the "FLXcomponent" object, they are there only for convenience such that users can easily extract and use them after flexmix() has finished.Lexical scope allows to write clean interfaces in a very elegant way, the driver abstracts all model details from the FlexMix main engine.The result can be seen in the left panel of Figure 6, the result is "wrong" because we forced the ellipses to be parallel to the axes.The overlap between three of the four clusters can also be inferred from the low ratio statistics in the summary table (around 0.5 for components 1, 3 and 4), while the much better separated upper left cluster has a much higher ratio of 0.85.Using the correct model with a full covariance matrix can be done by setting diagonal=FALSE in the call to our driver mymclust():

Example: Using the driver

R>

Summary and outlook

The primary goal of FlexMix is extensibility, this makes the package ideal for rapid development of new mixture models.There is no intent to replace packages implementing more specialized mixture models like mclust for mixtures of Gaussians, FlexMix should rather be seen as a complement to those.By interfacing R's facilities generalized linear models, FlexMix allows the user to estimate complex latent class regression models.
Using lexical scope to resolve model-specific parameters hides all model details from the programming interface, FlexMix can in principle fit almost arbitrary finite mixture models for which the EM algorithm is applicable.The downside of this is that FlexMix can in principle fit almost arbitrary finite mixture models, even models where no proper theoretical results for model identification etc. are available.
We are currently working on a toolset for diagnostic checks on mixture models to test necessary identifiability conditions for those cases where results are available.We also want to implement newer variations of the classic EM algorithm, especially for faster convergence.
Another plan is to have an interactive version of the rootograms using iPlots (Urbanek and Theus 2003) such that the user can explore the relations between mixture components, possibly linked to background variables.Other planned extensions include covariates for the prior probabilities and to allow to mix different distributions for components, e.g., to include a Poisson point process for background noise.

Figure 1 :

Figure 1: Standard regression example (left) and Poisson regression (right).

Function

refit() fits weighted generalized linear models to each component using the standard R function glm() and the posterior probabilities as weights, see help("refit") for details.The data setNPreg also includes a response from a generalized linear model with a Poisson distribution and exponential link function.The two classes of size 100 each have parameters Class 1: µ 1 = 2 − 0.2x Class 2: µ 2 = 1 + 0.1x and given x the response y in group k has a Poisson distribution with mean e µ k , see the right panel of Figure 1.The model can be estimated using R> m2 <-flexmix(yp ~x, data = NPreg, k = 2, + model = FLXMRglm(family = "poisson")) R> summary(m2) Call: flexmix(formula = yp ~x, data = NPreg, k = 2, model = FLXMRglm(family = "poisson"

Figure 3 :

Figure 3: plot(m2)

Figure 4 :

Figure 4: plot(m3)
runs flexmix() 5 times for k = 1, 2, . . ., 5 components, totalling in 25 runs.It returns a list with the best solution found for each number of components, each list element is simply an object of class "flexmix".To find the best model we can useR> getModel(m7, "BIC")Call: stepFlexmix(yp ~x + I(x^2), data = NPreg, control = list(verbose = 0and choose the number of components minimizing the BIC.

1

Figure 5: M-step for model-based clustering: mymclust is a simplified version of the standard FlexMix driver FLXmclust.
As a simple example we use the four 2-dimensional Gaussian clusters from data set Nclus.Fitting a wrong model with diagonal covariance matrix is done by R> data("Nclus") R> m1 <-flexmix(Nclus ~1, k = 4, model = mymclust()) R> summary(m1) Call: flexmix(formula = Nclus ~1, k = 4, model = mymclust()) prior size post>0 ratio Comp.1 0.

Figure 6 :

Figure 6: Fitting a mixture model with diagonal covariance matrix (left) and full covariance matrix (right).


15606264:Prediction in functional linear regression


Numerical implementation and simulation study.

1. Introduction. In the problem of functional linear regression we observe data {(X 1 , Y 1 ), . . . , (X n , Y n )}, where the X i 's are independent and identically distributed as a random function X, defined on an interval I, and the Y i 's are generated by the regression model, Here, a is a constant, denoting the intercept in the model, and b is a squareintegrable function on I, representing the slope function. The majority of attention usually focuses on estimating b, typically by methods based on functional principal components. See, for example, [28], Chapter 10, and [29].
In functional linear regression, perhaps as distinct from more conventional linear regression, there is significant interest in b in its own right. In particular, since b is a function rather than a scalar, then knowing where b takes large or small values provides information about where a future observation x of X will have greatest leverage on the value of I bx. Such information can be very useful for understanding the role played by the functional explanatory variable. Nevertheless, as this example suggests, the greatest overall interest lies, as in conventional linear regression, in using an estimatorb as an aid to predicting, either qualitatively or quantitatively, a future value of I bx.
Thus, while there is extensive literature on properties ofb, for example on convergence rates ofb to b (see, e.g., [11,13,15,20]), there is arguably a still greater need to understand the manner in whichb should be constructed in order to optimize the prediction of I bx, or of a + I bx. This is the problem addressed in the present paper.
Estimation of b is intrinsically an infinite-dimensional problem. Therefore, unlike slope estimation in conventional finite-dimensional regression, it involves smoothing or regularization. The smoothing step is used to reduce dimension, and the extent to which this should be done depends on the use to which the estimator of b will be put, as well as on the smoothness of b. It is in this way that the problem of estimating I bx is quite different from that of estimating b. The operation of integration, in computing Ib x fromb, confers additional smoothness, with the result that if we smoothb optimally for estimating b then it will usually be oversmoothed for estimating I bx.
Therefore the construction ofb, as a prelude to estimating I bx, should involve significant undersmoothing relative to the amount of smoothing that would be used if we wished only to estimate b itself. In fact, as we shall show, the degree of undersmoothing can be so great that it enables I bx to be estimated root-n consistently, even though b itself could not be estimated at such a fast rate.
However, root-n consistency is not always possible when estimating I bx. The optimal convergence rate depends on a delicate balance among the smoothness of b, the smoothness of x, and the smoothness of the autocovariance of the stochastic process X, all measured with respect to the same sequence of basis functions. In a qualitative sense, I bx can be estimated root-n consistently if and only if x is sufficiently smooth relative to the degree of smoothness of the autocovariance. If x is less smooth than this, then the optimal rate at which I bx can be estimated is determined jointly 3 by the smoothnesses of b, x and the autocovariance, and becomes faster as the smoothnesses of x and of b increase, and also as the smoothness of the covariance decreases.
These results are made explicitly clear in Section 4, which gives upper bounds to rates of convergence for specific estimators of I bx, and lower bounds (of the same order as the upper bounds) to rates of convergence for general estimators. Section 2 describes construction of the specific estimators of b, which are then substituted for b in the formula I bx. Practical choice of smoothing parameters is discussed in Section 3.
In this brief account of the problem we have omitted mention of the role of the intercept, a, in the prediction problem. It turns out that from a theoretical viewpoint the role is minor. Given an estimatorb of b, we can readily estimate a byâ =Ȳ − IbX , whereX andȲ denote the means of the samples of X i 's and Y i 's, respectively. Taking this approach, it emerges that the rate of convergence of our estimator of a + I bx is identical to that of our estimator of I bx, up to terms that converge to zero at the parametric rate n −1/2 . This point will be discussed in greater detail in Section 4.1.
The approach taken in this paper to estimating b is based on functional principal components. While other methods could be used, the PC technique is currently the most popular. It goes back to work of Besse and Ramsay [1], Ramsay and Dalzell [27], Rice and Silverman [31] and Silverman [32,33]. There are a great many more recent contributions, including those of Brumback and Rice [5], Cardot [7], Cardot, Ferraty and Sarda [8,9,10], Girard [19], James, Hastie and Sugar [23], Boente and Fraiman [3] and He, Müller and Wang [21].
Other recent work on regression for functional data includes that of Ferré and Yao [18], who introduced a functional version of sliced inverse regression; Preda and Saporta [26], who discussed linear regression on clusters of functional data; Escabias, Aguilera and Valderrama [14] and Ratcliffe, Heller and Leader [30], who described applications of functional logistic regression; and Ferraty and Vieu [16,17] and Masry [24], who addressed various aspects of nonparametric regression for functional data. Müller and Stadtmüller [25] introduced the generalized functional linear model, where the response Y i is a general smooth function of a + I bX i , plus an error. See also [22] and [12]. The methods developed in the present paper could be extended to this setting.
2. Model and estimators. We shall assume model (1.1), and suppose that the errors ε i are independent and identically distributed with zero mean and finite variance. It will be assumed too that the errors are independent of the X i 's and that I E(X 2 ) < ∞.
Conventionally, estimation of b is undertaken using a principal components approach, as follows. We take the covariance function of X to be positive definite, in which case it admits a spectral decomposition in terms of strictly positive eigenvalues θ j ,
where (θ j , φ j ) are (eigenvalue, eigenfunction) pairs for the linear operator with kernel K, the eigenvalues are ordered so that θ 1 > θ 2 > · · · (in particular, we assume there are no ties among the eigenvalues), and the functions φ 1 , φ 2 , . . . form an orthonormal basis for the space of all square-integrable functions on I.
Empirical versions of K and of its spectral decomposition are
Analogously to the case of K, (θ j ,φ j ) are (eigenvalue, eigenfunction) pairs for the linear operator with kernel K, ordered such that θ 1 ≥θ 2 ≥ · · · . Moreover,θ j = 0 for j ≥ n + 1. We take (θ j ,φ j ) to be our estimator of (θ j , φ j ). The function b can be expressed in terms of its Fourier series, as
where m, lying in the range 1 ≤ m ≤ n, denotes a "frequency cut-off" and b j is an estimator of b j .
To constructb j we note that b j = θ −1 j g j , where g j denotes the jth Fourier coefficient of g(u) = I K(u, v)b(v) dv. A consistent estimator of g is given byĝ
and so, for 1 ≤ j ≤ m, we takeb j =θ −1 jĝ j , whereĝ j = Iĝφ j . While the problem of estimating b is of intrinsic interest, it is arguably not of as much practical importance as that of prediction, that is, estimating
for a particular function x. To accomplish this task we require an estimator of a,â
Here,Ȳ andε are the respective means of the sequences Y i and ε i . Our estimator of p(x), for a given function x, iŝ
In Section 4 we shall introduce three parameters, α, β and γ, describing the smoothness of K, b and x, respectively. In each case, smoothness is measured in the context of generalized Fourier expansions in the basis φ 1 , φ 2 , . . . , and the larger the value of the parameter, the smoother the associated function. We shall show in Theorem 4.1 that if x is sufficiently smooth relative to K, specifically if γ > 1 2 (α + 1), then I bx can be estimated root-n consistently. For smaller values of γ, the optimal convergence rate is slower than n −1/2 .
There is a variety of possible approaches to empirical choice of the cut-off, m, although not all are directly suited to estimation of I bx. Potential methods include those based on simple least-squares, on the bootstrap or on cross-validation. In some instances where Ib x is root-n consistent for I bx, m can be chosen within a wide range without appreciably affecting the performance of the estimator. Only in relatively "unsmooth" cases, where either γ ≤ 1 2 (α + 1), or γ > 1 2 (α + 1) but γ is close to 1 2 (α + 1), is the choice of m rather critical. The empirical identification of unsmooth cases, and empirical choice of m in those instances, are challenging problems, and we shall not attempt to address them here. (See the last paragraph of Section 2 for discussion of α, β and γ.)
Instead, we shall give below a simple threshold-based algorithm for choosing m empirically in cases where x is sufficiently smooth. There, the algorithm guarantees root-n consistency. The order of magnitude of the empirically chosen m depends very much on selection of the threshold, but nevertheless the estimator Ib x remains root-n consistent in a very wide range of cases. Therefore, the effectiveness of the threshold algorithm underscores the robustness of the estimator against choice of m in cases where x is smooth.
To describe the threshold algorithm, let C > 0 and 0 < c ≤ 1 2 , and put I j = 1 ifθ j ≥ t ≡ Cn −c , with I j = 0 otherwise. Since the sequenceθ 1 ,θ 2 , . . . is nonincreasing andθ j = 0 for j ≥ n + 1, then I 1 , I 2 , . . . is a sequence of m, say, 1's, followed by an infinite sequence of 0's. Therefore the threshold 6 T. T. CAI AND P. HALL algorithm implicitly gives an empirical rule for choosing the cut-off, m. Our estimator of I bx is Ib x, whereb = 1≤j≤mb jφj . Note that the estimator
wherex j = I xφ j . This form is often easier to use in numerical calculations.
To appreciate the size of m chosen by this rule, let us suppose that θ j = const.j −α . It can be shown that, for the specified range of values of c,
It follows that the order of magnitude of m changes a great deal as we vary c.
It can be proved too that, under the conditions of Theorem 4.1, and assuming that α ≥ 2, γ ≥ 3 2 (α + 2) and β + γ ≥ (α/2c) + 1,
This result demonstrates the root-n consistency of the estimator on the lefthand side, for a range of different orders of magnitude of m. Of course, (3.1) continues to hold if the number of terms, m, is replaced by a deterministic quantity, say m ∼ const.n c/α . Note too that the conditions γ ≥ 3 2 (α + 2) and β + γ ≥ (α/2c) + 1 are both implied by γ ≥ max(3/2, 1/2c)α + 3, which asserts simply that the function x is sufficiently smooth relative to K.
The case where the functions X i are observed on a regular grid of k points with additive white noise may be treated similarly. Indeed, it can be proved that if continuous approximations to the X i 's are generated by passing a local-linear smoother through noisy, gridded data, and if we take c = 1 2 , then all the results discussed above remain true provided n = O(k). That is, k should be of the same order as, or of larger order than, n. Details are given in the Appendix of [6]. Similar results are obtained using smoothing methods based on splines or orthogonal series.
A simulation study was carried out to investigate the finite-sample performance of the thresholding procedure given above. The study considered the model (1.1) in two cases. In the first, the predictor X i was observed continuously without error. Specifically, random samples of size n = 100 were generated from the model (1.1), where I = [0, 1], the random functions X i were distributed as X = j Z j 2 1/2 cos(jπt), the Z j 's were independent and normal N(0, 4j −2 ), b = j j −4 2 1/2 cos(jπt), and the errors ε i were independent and normal N(0, 4). The future observation of X was taken to be x = j j −2 2 1/2 cos(jπt), in which case the conditional mean of y given X = x was 1.0141. The example in the second case was the same as that for the first, except that each X i was observed discretely on an equally-spaced grid of 200 points with additive N(0, 1) random noise. We used an orthogonal-series smoother to "estimate" each X i from the corresponding discrete data. Table 1 gives values of averaged squared error of the estimator of the conditional mean, computed by averaging 500 Monte Carlo simulations. It is clear from these results that the procedure is robust against discretization, random errors and choice of the threshold.
Earlier in this section we discussed the robustness ofb to choice of smoothing parameter in the prediction problem. This robustness is not shared in cases whereb is of interest in its own right, rather than a tool for prediction. To make this comparison explicit, and to compare the levels of smoothing appropriate for prediction and estimation, we extended the simulation study above. We selected X as before, but took b = 10 j j −2 2 1/2 cos(jπt) and x = j j −1.6 2 1/2 cos(jπt). In the case of noisy, discrete observations we took the noise to be N(0, 1) and the grid to consist of 500 points. Sample size was n = 100.
For the thresholds t = 0.001, 0.01, 0.05, 0.1, 0.15, 0.2 used to construct Table 1, mean squared prediction error was relatively constant; respective values were 0.013, 0.008, 0.007, 0.010, 0.015, 0.022. However, mean integrated squared error ofb was as high as 168 when t = 0.001, dropping to 6.67 at t = 0.01 and reaching its minimum, 0.639, at t = 0.1. Similar results were achieved in the case of noisy, discrete data; values of mean squared prediction error there were 0.014, 0.008, 0.009, 0.013, 0.019, 0.028 for the respective values of t, and mean integrated squared error ofb was elevated by about 30% across the range, the minimum again occurring when t = 0.1.
These results also indicate the advantages of undersmoothing when making predictions, as opposed to estimatingb in its own right. In particular, the numerical value of the optimal threshold for prediction is a little less than that for estimatingb. Discussion of theoretical aspects of this point will be given in Section 4.

Convergence rates.

4.1. Effect of the intercept, a. In terms of convergence rates, the problems of estimating a + I bx and I bx are not intrinsically different. To appreciate this point, define µ = E(X), let the functionals p andp be as in Section 2, and put q(
Provided only that E b − b 2 is bounded, the right-hand side of (4.1) equals O(n −1/2 ). Hence, (4.1) shows that, up to terms that converge to zero at the parametric rate n −1/2 , the rates of convergence ofp(x) to p(x) and ofq(x) to q(x) are identical. This result, and the fact that q(x) is identical to bx provided x is replaced by x − µ, imply that when addressing convergence rates in the prediction problem it is sufficient to treat estimation of I bx.

Estimation of bx.

Recall that our estimator of bx is b x. Suppose the eigenvalues θ j in the spectral decomposition (2.1) satisfy
For example, if θ j = Dj −α for a constant D > 0, then θ j −θ j+1 ∼ Dα −1 j −α−1 , and so (4.2) holds. The second part of (4.2) asks that the spacings among eigenvalues not be too small. Methods based on a frequency cut-off m can have difficulty when spacings equal zero, or are close to zero. To appreciate why, note that if θ j+1 = · · · = θ j+k then φ j+1 , . . . , φ j+k are not individually identifiable (although the set of these k functions is identifiable). In particular, individual functions cannot be estimated consistently. This can cause problems when estimating I bx if the frequency cut-off lies strictly between j and j + k.
Let Z have the distribution of a generic X i − E(X i ). Then we may write Z = j≥1 ξ j φ j , where ξ j = Zφ j is the jth principal component, or Karhunen-Loève coefficient, of Z. We assume that all the moments of X are finite, and more specifically that for each r ≥ 2 and each j ≥ 1, E|ξ j | 2r ≤ C(r)θ r j , where C(r) does not depend on j; and, for any sequence j 1 , . . . , j 4 , E(ξ j 1 . . . ξ j 4 ) = 0 unless each index j k is repeated. In particular, (4.3) holds if X is a Gaussian process. Let β > 1 and C 1 > 0, and let
We can interpret B(C 1 , β) as a "smoothness class" of functions, where the functions become smoother (measured in the sense of generalized Fourier expansions in the basis φ 1 , φ 2 , . . .) as β increases. We suppose too that the fixed function x satisfies
Again, x becomes smoother in the sense of generalized Fourier expansions as γ increases.
Define m 0 = m 0 (n) by
These explicit values serve to simplify our discussion and our proof of Theorem 4.1, and do not reflect the wider range of values of m, particularly in the case α + 1 < 2γ, for which our theory is valid. Discussion of this point has been given in Section 3.
Recall the definition ofb at (2.2). Given arbitrary positive constants C 3 , C 4 and C 5 , letb
where, for a function ψ on I, ψ 2 = I ψ 2 . This truncation ofb serves to ensure that all moments ofb are finite.
Theorem 4.1. Assume the eigenvalues θ j satisfy (4.2), that (4.3) holds and that all moments of the distribution of the errors ε i are finite. Let α, β and γ be as in (4.2), (4.4) and (4.5), respectively. Suppose that α > 1, β ≥ α+ 2 and γ > 1 2 , and that the ratio of m to m 0 is bounded away from zero and infinity as n → ∞. Then, for each given C, C 1 , . . . , C 5 > 0, as n → ∞, the estimatorb given in (4.7) satisfies
where τ = τ (n) is given by
if α + 1 = 2γ, n −2(β+γ−1)/(α+2β−1) , if α + 1 > 2γ.
(4.9)
The smoothing-parameter choices suggested by (4.6) are different from those that would be used if our aim were to estimate b rather than I bx.
In particular, to optimize the L 2 convergence rate ofb to b we would take m to be of size n 1/(α+2β) in each of the three settings addressed by (4.6). See, for example, [20]. In the critical cases where α + 1 ≥ 2γ, this provides an order of magnitude more smoothing than is suggested by (4.6). The intuition behind this result is that the integration step, in the definition Ib x, provides additional smoothing no matter what level is used when constructingb, and so less smoothing is needed forb.
The case α + 1 < 2γ is more difficult to discuss in these terms, since a variety of different orders of magnitude of m can lead to the same optimal mean-square convergence rate of n −1 . Further discussion of this issue is given in Section 3.
Of course, there are other related problems where similar phenomena are observed. Consider, for example, the problem of estimating a distribution function by integrating a kernel density estimator. In order to achieve the same parametric convergence rate as the empirical distribution function, we should, when constructing the density estimator, use a substantially smaller bandwidth than would be appropriate if we wanted a good estimator of the density itself. The operation of integrating the density estimator provides additional smoothing, over and above that accorded by the bandwidth, and so if the net result is not to be an oversmoothed distribution-function estimator then we should smooth less at the density estimation step. The same is true in the problem of prediction in functional regression; the operation of integratingbx provides additional smoothing, and so to get the right amount of smoothing in the end we should undersmooth when computing the slopefunction estimator. A curious feature of the regression prediction problem is that, unlike the distribution estimation one, it is not always parametric, and in some cases the optimal convergence rate lies strictly between that for the nonparametric problem of slope estimation and the parametric n −1/2 rate. 4.3. Lower bounds. We adopt notation from Sections 4.1 and 4.2, and in particular take x = j≥1 x j φ j to be a function and define B as at (4.4). Recall that the functions φ j form an orthonormal basis for square-integrable functions on I. Assume that, for a constant C 6 > 1,
Let T denote any estimator of T (b) = I bx, and define τ = τ (n) as at (4.9). Our main result in this section provides a lower bound to the convergence rate of T to T (b), complementing the upper bound given by Theorem 4.1 in the case T = Ib x, whereb is given by (4.7). We make relatively specific assumptions about the nature of the model, for example that X is a Gaussian process and the intercept, a, vanishes, bearing in mind that in the case of a lower bound, the strength of the result is increased, from some viewpoints, through imposing relatively narrow conditions. Theorem 4.2. Let α, β and γ be as in (4.2), (4.4) and (4.5), respectively, and assume α, β > 1 and γ > 1 2 . Suppose too that the process X is Gaussian and that the errors ε i in the model (1.1) are Normal with zero mean and strictly positive variance; and take a = 0. Then there exists a constant C 7 > 0 such that, for any estimatorT and for all sufficiently large n,
where τ = τ (n) is given as in (4.9).
A comparison of the lower bound given above with the upper bound given in Theorem 4.1 yields the result that the minimax risk of estimating bx satisfies
where, for positive sequences a n and b n , a n ≍ b n means that a n /b n is bounded away from zero and infinity as n → ∞.

5.

Proof of Theorem 4.1.

5.1.

Preliminaries. Define ∆ = K − K, |||∆||| 2 = I 2 ∆ 2 and δ j = min k≤j (θ k − θ k+1 ). It may be shown from results of Bhatia, Davis and McIntosh [2] that sup j≥1 |θ j − θ j | ≤ |||∆|||,
For simplicity in our proof we shall take m = m 0 , as defined in (4.6). Note that in this setting m ≤ n 1/(α+2β−1) in each of the three cases in (4.6). Expand x with respect to both the orthonormal series φ 1 , φ 2 , . . . and φ 1 ,φ 2 , . . . , obtaining x = j≥1 x j φ j = j≥1xjφj , where x j = I xφ j and x j = I xφ j . Putg j = I gφ j . In this notation
T. T. CAI AND P. HALL whence it follows that
). This quantity equals O{(n −1 log n) 1/2 } if α+1 = 2γ, equals O(n −(β+γ−1)/(α+2β−1) ) if α + 1 > 2γ and equals o(n −1/2 ) otherwise. We shall complete the derivation of Theorem 4.1 by obtaining bounds for second moments of the other three terms on the right-hand side of (5.2). Our analysis will show that the first and second terms determine the convergence rate, and that the third and fourth terms are asymptotically negligible. In the arguments leading to the bounds we shall use the notation "const." to denote a constant, the value of which does not depend on b ∈ B. In particular, the bounds we shall give are valid uniformly in b, although we shall not mention that property explicitly.

Bound for

It can be proved, using this result, (5.1), (5.4) and (5.5), that if E holds,
For each real number r, define
Standard moment calculations, noting that S 1 (g) ≡ j≤m (g j − g j )x j θ −1 j may be expressed as a sum of n independent and identically distributed random variables with zero mean, show that E{S 1 (g) 2 } ≤ const.n −1 t α−2γ (m), uniformly in g. Moreover, denoting by S 2 (g) the last term on the right-hand side of (5.7), we deduce that
, and by assumption, n ≥ m α+1 . Therefore, n −1 t 2α−γ+1 (m) ≤ const.t α−2γ (m). Hence, (5.8) implies that E{S 2 (g) 2 } ≤ const.n −1 t α−2γ (m). Combining this bound with that for E{S 1 (g) 2 }, and with (5.7), and writing I(F) for the indicator function of any subset F ⊆ E, we deduce that
Let p = g or x, and define π = α + β and π = γ in the respective cases. Let q 1 , q 2 , . . . denote constants satisfying |q j | ≤ const.j κ for each j, where κ = α − γ if p = g, and κ = −(α + β) if p = x. Given η > 0, consider the event
Comparing (5.6) and (5.13), and noting (4.2), we see that F ⊆ E. We shall show in Section 5.5 that, uniformly in 1 ≤ j ≤ const.n 1/(α+1) , (5.14) and also,
Next we use (5.15) to bound the first term on the right-hand side of (5.9):
To bound the second term, it can be proved from (5.14) that
Going back to the definition of F at (5.13), and taking η < {β −α−(3/2)}/(α+ 2β − 1), we deduce from (5.17) that
Results (5.9), (5.16) and (5.18) imply that
Noting that κ = −(α + β) when p = x, we may also use (5.15) and (5.14) to bound the expected values of the squares of the right-hand sides of (5.11) and (5.12), respectively, multiplied by I(F):
Noting that β ≥ α + 2 and E(g j − g j ) 2 ≤ const.n −1 θ j , we can show from (5.10) and (5.14) that  The proof of Theorem 4.1 will be complete if we show that the factor I(F) can be removed from the left-hand side. Since, in view of (4.7), our estimator b satisfies b ≤ C 4 n C 5 , then it suffices to prove that, for all D > 0, P (F) = 1 − O(n −D ). Now the first part of (5.1) and (5.13) imply that if we define
then G ⊆ F . Since m ≤ n 1/(α+2β−1) and 2(α + 1) < α + 2β − 1, then for some η ′ > 0, m −α−1 ≥ n η ′ −(1/2) . Therefore, if ζ > 0 is sufficiently small, there exists n 0 ≥ 1 such that, if we define H = {|||∆||| ≤ n ζ−(1/2) }, then for all n ≥ n 0 , H ⊆ G. Since we assumed all moments of the principal components ξ j and the errors ε i to be finite, then Markov's inequality is readily used to show that P (H) = 1 − O(n −D ) for all D > 0. It follows that P (F) = 1 − O(n −D ), and so (5.24) implies (4.8).

5.5.

Proof of (5.14) and (5.15). Define ∆ j bŷ
It may be proved that
from which it follows that
If F holds then so too does the event E and, in view of (4.2), |θ j − θ k | ≤ 2|θ j − θ k | for all 1 ≤ j ≤ m and all k = j. Therefore, writing p = j≥1 p j φ j and using (5.1), we deduce that
Since |p j | ≤ const.j −π for each j then, if d = 2 or 4,
We shall show in Section 5.6 that if η, in the definition of F at (5.13), is chosen sufficiently small, then whenever F holds, | I (φ j − φ j )φ j | ≤ C 0âj for 1 ≤ j ≤ m, where C 0 > 0 is a constant depending on neither j nor n, andâ j is a nonnegative random variable satisfying E(â 2 j ) ≤ n −2 j 4 .
(5.27)
Combining (5.26) and the results in this paragraph, we deduce that
When p = g we may substitute π = α + β into (5.28). Then we can deduce from (5.28) that, assuming α + 2 ≤ β as well as the bound j ≤ m ≤ n 1/(α+2β−1) , the right-hand side of (5.28) is bounded above by a constant multiple of n −1 j −α . Since β > 1 then this bound also applies to the righthand side of (5.29).
In the case p = x the fact that α + 2 ≤ β, as well as the bound j ≤ m ≤ n 1/(α+2β−1) , imply that the right-hand side of (5.28) is dominated by the right-hand side of (5.29). Hence, for both p = g and p = x the bound at (5.14) follows from (5.25), (5.28) and (5.29).
Observe too that by (5.28)
Therefore, if p = g then 3α + 2κ + 4 − 2π = 3α + 4 − 2(β + γ) < (α + 2β − 1) − 1, and 6α + 2κ + 6 − 2π = 2{3α + 3 − (β + γ)} < 2(α + 2β − 1) − 1. [We subtract the extra 1 to account for the factor m on the right-hand side of (5.30).] These two results, and the fact that m α+2β−1 ≤ n, imply that the terms in mn −2 t 3α+2κ+4−2π (m) and mn −3 t 6α+2κ+6−2π in (5.30) may be replaced by n −1 without affecting the validity of the bound when p = g. Furthermore, when p = g, α + 2κ + 1 = 3α − 2γ + 1 < (α + 2β − 1) − 1 and 2α + 2κ + 2 = 4α − 2γ + 2 < 2(α + 2β − 1) − 1, and so the terms in mn −2 t α+2κ+1 (m) and mn −3 t 2α+2κ+2 may also be replaced by n −1 . Therefore the right-hand of (5.30) may be replaced by n −1 when p = g. An identical argument shows this also to be the case when p = x. Hence, in either setting, 5.6. Proof of (5.27). It may be proved from (5.25)
and w jk = ∆φ j φ k . Since both φ j andφ j are of unit length thenv 2
If the event F obtains then |θ j − θ k | −1 ≤ 2|θ j − θ k | −1 for all j, k such that j = k and 1 ≤ j ≤ m. For the same range of values of j and k, |θ j − θ k | −1 ≤ Dθ −1 m m. Here D = C 2 , where C is as in (4.2). Definingx jk = ∆φ j φ k and y jk = ∆(φ j − φ j )φ k , we have w 2 jk ≤ 2(x 2 jk +ŷ 2 jk ), and hence, assuming F holds, we have for 1 ≤ j ≤ m,
where the constant does not depend on j, k or n. Moreover,
Therefore, E(Â j ) ≤ const.n −1 j 2 for 1 ≤ j ≤ m, and similar calculations show that
where D 1 > 0 depends on neither j nor n.
Combining (5.34) with the first part of (5.33) we deduce that if F holds,
for 1 ≤ j ≤ m. However, if c > 0 is given, and if η > 0 is chosen sufficiently small in the definition of F at (5.13), then for all sufficiently large m, F implies |||∆||| ≤ cm −1 θ m . Hence, by (5.36), if F holds, then for 1 ≤ j ≤ m,
Choosing c so small that 16D 2 c 2 ≤ 1 2 , we deduce that if F holds, then for 1 ≤ j ≤ m, φ j − φ j 2 ≤ 32Â j . Combining this result with (5.34), and noting the choice of c, we deduce that if F holds, then for 1 ≤ j ≤ m,û 2 j ≤ 16Â j . From this property and the second part of (5.33) we conclude that if F holds, then for 1 ≤ j ≤ m,
1Â j , where D 1 is as at (5.35), and letting C 0 = 32D 1 , we see that (5.27) follows from (5.35) and (5.37).
6. Proof of Theorem 4.2. We shall treat only the cases 2γ < α + 1 and 2γ = α + 1, since the third setting, 2γ > α + 1, is relatively straightforward. For notational simplicity we shall assume that C 1 , in the definition of B(C 1 , β), satisfies C 1 ≥ 1, and take θ j = j −α and x j = j −γ . More general cases are easily addressed.
Since X is Gaussian then we may write X i = j≥1 ξ ij φ j for i ≥ 1, where the variables ξ ij are independent and normal with zero mean and respective variances θ j for j ≥ 1. Define ν to be the integer part of n 1/(α+2β−1) , and let B 0 ≡ 0 and B 1 = ν+1≤j≤2ν j −β φ j ; both are functions in B(C 1 , β).
Note that T (B 0 ) = 0 and that for large n, T (B 1 ) ≥ const.n −(β+γ−1)/(α+2β−1) , (6.1)
where, here and below, "const." denotes a finite, strictly positive, generic constant. Write Ξ i = ν+1≤j≤2ν ξ ij j −β . The observed data are Y i = tΞ i + ε i for 1 ≤ i ≤ n, where t = 0 or 1 according as b = B 0 or b = B 1 , respectively. Denote by P t the joint distribution of the Y i 's for t = 0 or 1. Elementary calculations show that the chi-squared distance between P 0 and P 1 is given by d(P 0 , P 1 ) = (dP 1 ) 2 dP 0 = exp σ −2
where σ 2 denotes the variance of the error distribution. The variables Ξ i are independent and normally distributed with zero means and variance V n , where nV n = n ν+1≤j≤2ν j −α−2β → const. as n → ∞. Indeed, E 1 {d(P 0 , P 1 )} → const., (6.2) where E t denotes expectation in the model with b = B t , for t = 0 or 1. Let T be any estimator such that for some D > 0,
It follows from (6.1), (6.2) and the fact that T (B 0 ) = 0, that if D in (6.3) is chosen sufficiently small, ρ ≤ 1 2 . In this case,
where the first inequality follows from the constrained-risk lower bound of Brown and Low [4], and the second uses (6.1) and the property T (B 0 ) = 0. Consequently, writing E b for expectation when the slope function is b ∈ B, for any estimatorT The case 2γ = α+1 may be treated similarly, by taking ν = (n/ log n) 1/(α+2β−1) and replacing n by n/ log n in (6.1), (6.3) and (6.4).

5. 4 . 1 .

41Completion of the proof of Theorem 4.Combining

t=0, 1 E

1t { T − T (B t )} 2 ≥ const.n −2(β+γ−1)/(α+2β−1) .

Table 1

1Comparison of average squared errorsThreshold 
0.001 0.01 
0.05 
0.1 
0.15 
0.2 
X continuous 
0.026 0.019 0.015 0.014 0.013 0.015 
X discrete with noise 0.035 0.022 0.016 0.017 0.015 0.016
Acknowledgment. This work was done while Tony Cai was visiting the Mathematical Sciences Institute of the Australian National University.
Principal components analysis of sampled functions. P Besse, J O Ramsay, Psychometrika. 51Besse, P. and Ramsay, J. O. (1986). Principal components analysis of sampled functions. Psychometrika 51 285-311. MR0848110
Perturbation of spectral subspaces and solution of linear operator equations. R Bhatia, C Davis, A Mcintosh, Linear Algebra Appl. 52Bhatia, R., Davis, C. and McIntosh, A. (1983). Perturbation of spectral subspaces and solution of linear operator equations. Linear Algebra Appl. 52/53 45-67. MR0709344
Kernel-based functional principal components. G Boente, R Fraiman, Statist. Probab. Lett. 481771495Boente, G. and Fraiman, R. (2000). Kernel-based functional principal components. Statist. Probab. Lett. 48 335-345. MR1771495
A constrained risk inequality with applications to nonparametric functional estimation. L D Brown, M G Low, Ann. Statist. 241425965Brown, L. D. and Low, M. G. (1996). A constrained risk inequality with ap- plications to nonparametric functional estimation. Ann. Statist. 24 2524-2535. MR1425965
Smoothing spline models for the analysis of nested and crossed samples of curves (with discussion). B A Brumback, J A Rice, J. Amer. Statist. Assoc. 93Brumback, B. A. and Rice, J. A. (1998). Smoothing spline models for the analysis of nested and crossed samples of curves (with discussion). J. Amer. Statist. Assoc. 93 961-994. MR1649194
Prediction in functional linear regression. T T Cai, P Hall, Technical reportAvailable at stat.wharton.upenn.edu/˜tcai/paper/FLR-Tech-Report.pdfCai, T. T. and Hall, P. (2005). Prediction in func- tional linear regression. Technical report. Available at stat.wharton.upenn.edu/˜tcai/paper/FLR-Tech-Report.pdf.
Nonparametric estimation of smoothed principal components analysis of sampled noisy functions. H Cardot, J. Nonparametr. Statist. 12Cardot, H. (2000). Nonparametric estimation of smoothed principal components analysis of sampled noisy functions. J. Nonparametr. Statist. 12 503-538. MR1785396
Functional linear model. H Cardot, F Ferraty, P Sarda, Statist. Probab. Lett. 45Cardot, H., Ferraty, F. and Sarda, P. (1999). Functional linear model. Statist. Probab. Lett. 45 11-22. MR1718346
Étude asymptotique d'un estimateur spline hybride pour le modèle linéaire fonctionnel. H Cardot, F Ferraty, P Sarda, C. R. Acad. Sci. Paris Sér. I Math. 330Cardot, H., Ferraty, F. and Sarda, P. (2000).Étude asymptotique d'un estima- teur spline hybride pour le modèle linéaire fonctionnel. C. R. Acad. Sci. Paris Sér. I Math. 330 501-504. MR1756966
Spline estimators for the functional linear model. H Cardot, F Ferraty, P Sarda, MR1997162Statist. Sinica. 13Cardot, H., Ferraty, F. and Sarda, P. (2003). Spline estimators for the func- tional linear model. Statist. Sinica 13 571-591. MR1997162
Linear regression models for functional data. H Cardot, P Sarda, Unpublished manuscriptCardot, H. and Sarda, P. (2003). Linear regression models for functional data. Unpublished manuscript.
Estimation in generalized linear models for functional data via penalized likelihood. H Cardot, P Sarda, J. Multivariate Anal. 922102242Cardot, H. and Sarda, P. (2005). Estimation in generalized linear models for func- tional data via penalized likelihood. J. Multivariate Anal. 92 24-41. MR2102242
Linear functional regression: The case of fixed design and functional response. A Cuevas, M Febrero, R Fraiman, Canad. J. Statist. 30Cuevas, A., Febrero, M. and Fraiman, R. (2002). Linear functional regression: The case of fixed design and functional response. Canad. J. Statist. 30 285-300. MR1926066
Modeling environmental data by functional principal component logistic regression. M Escabias, A M Aguilera, M J Valderrama, Environ- metrics 16 95-107. MR2146901Escabias, M., Aguilera, A. M. and Valderrama, M. J. (2005). Modeling envi- ronmental data by functional principal component logistic regression. Environ- metrics 16 95-107. MR2146901
Dimension fractale et estimation de la régression dans des espaces vectoriels semi-normés. F Ferraty, P Vieu, C. R. Acad. Sci. Paris Sér. I Math. 3301745172Ferraty, F. and Vieu, P. (2000). Dimension fractale et estimation de la régression dans des espaces vectoriels semi-normés. C. R. Acad. Sci. Paris Sér. I Math. 330 139-142. MR1745172
The functional nonparametric model and application to spectrometric data. F Ferraty, P Vieu, MR1952697Comput. Statist. 17Ferraty, F. and Vieu, P. (2002). The functional nonparametric model and appli- cation to spectrometric data. Comput. Statist. 17 545-564. MR1952697
Nonparametric models for functional data, with application in regression, time-series prediction and curve discrimination. F Ferraty, P Vieu, J. Nonparametr. Statist. 162053065Ferraty, F. and Vieu, P. (2004). Nonparametric models for functional data, with application in regression, time-series prediction and curve discrimination. J. Nonparametr. Statist. 16 111-125. MR2053065
Functional sliced inverse regression analysis. L Ferré, A F Yao, Statistics. 372022235Ferré, L. and Yao, A. F. (2003). Functional sliced inverse regression analysis. Statistics 37 475-488. MR2022235
A nonlinear PCA based on manifold approximation. S Girard, Comput. Statist. 151794107Girard, S. (2000). A nonlinear PCA based on manifold approximation. Comput. Statist. 15 145-167. MR1794107
Methodology and convergence rates for functional linear regression. P Hall, J L Horowitz, Unpublished manuscriptHall, P. and Horowitz, J. L. (2004). Methodology and convergence rates for func- tional linear regression. Unpublished manuscript.
Functional canonical analysis for square integrable stochastic processes. G He, H.-G Müller, J.-L Wang, J. Multivariate Anal. 851978177He, G., Müller, H.-G. and Wang, J.-L. (2003). Functional canonical analy- sis for square integrable stochastic processes. J. Multivariate Anal. 85 54-77. MR1978177
Generalized linear models with functional predictors. G M James, J. R. Stat. Soc. Ser. B Stat. Methodol. 64James, G. M. (2002). Generalized linear models with functional predictors. J. R. Stat. Soc. Ser. B Stat. Methodol. 64 411-432. MR1924298
Principal component models for sparse functional data. G M James, T J Hastie, C A Sugar, Biometrika. 87James, G. M., Hastie, T. J. and Sugar, C. A. (2000). Principal component models for sparse functional data. Biometrika 87 587-602. MR1789811
Nonparametric regression estimation for dependent functional data: Asymptotic normality. E Masry, Stochastic Process. Appl. 1152105373Masry, E. (2005). Nonparametric regression estimation for dependent functional data: Asymptotic normality. Stochastic Process. Appl. 115 155-177. MR2105373
. T T Cai And P, Hall, T. T. CAI AND P. HALL
Generalized functional linear models. H.-G Müller, U Stadtmüller, Ann. Statist. 332163159Müller, H.-G. and Stadtmüller, U. (2005). Generalized functional linear models. Ann. Statist. 33 774-805. MR2163159
PLS approach for clusterwise linear regression on functional data. C Preda, G Saporta, Classification, Clustering, and Data Mining Applications. D. Banks, L. House, F. R. McMorris, P. Arabie and W. GaulBerlinSpringer2113607Preda, C. and Saporta, G. (2004). PLS approach for clusterwise linear regression on functional data. In Classification, Clustering, and Data Mining Applications (D. Banks, L. House, F. R. McMorris, P. Arabie and W. Gaul, eds.) 167-176. Springer, Berlin. MR2113607
Some tools for functional data analysis (with discussion). J O Ramsay, C J Dalzell, J. Roy. Statist. Soc. Ser. B. 53Ramsay, J. O. and Dalzell, C. J. (1991). Some tools for functional data analysis (with discussion). J. Roy. Statist. Soc. Ser. B 53 539-572. MR1125714
Functional Data Analysis. J O Ramsay, B W Silverman, SpringerNew YorkRamsay, J. O. and Silverman, B. W. (1997). Functional Data Analysis. Springer, New York.
Applied Functional Data Analysis: Methods and Case Studies. J O Ramsay, B W Silverman, Springer1910407New YorkRamsay, J. O. and Silverman, B. W. (2002). Applied Functional Data Analysis: Methods and Case Studies. Springer, New York. MR1910407
Functional data analysis with application to periodically stimulated foetal heart rate data. II. Functional logistic regression. S J Ratcliffe, G Z Heller, L R Leader, Statistics in Medicine. 21Ratcliffe, S. J., Heller, G. Z. and Leader, L. R. (2002). Functional data anal- ysis with application to periodically stimulated foetal heart rate data. II. Func- tional logistic regression. Statistics in Medicine 21 1115-1127.
Estimating the mean and covariance structure nonparametrically when the data are curves. J A Rice, B W Silverman, J. Roy. Statist. Soc. Ser. B. 531094283Rice, J. A. and Silverman, B. W. (1991). Estimating the mean and covariance structure nonparametrically when the data are curves. J. Roy. Statist. Soc. Ser. B 53 233-243. MR1094283
Incorporating parametric effects into functional principal components analysis. B W Silverman, J. Roy. Statist. Soc. Ser. B. 57Silverman, B. W. (1995). Incorporating parametric effects into functional principal components analysis. J. Roy. Statist. Soc. Ser. B 57 673-689. MR1354074
Smoothed functional principal components analysis by choice of norm. B W Silverman, Ann. Statist. 241389877Silverman, B. W. (1996). Smoothed functional principal components analysis by choice of norm. Ann. Statist. 24 1-24. MR1389877


42639:Regularization Paths for Generalized Linear Models via Coordinate Descent.


Introduction

The lasso (Tibshirani 1996) is a popular method for regression that uses an 1 penalty to achieve a sparse solution.In the signal processing literature, the lasso is also known as basis pursuit (Chen et al. 1998).This idea has been broadly applied, for example to generalized linear models (Tibshirani 1996) and Cox's proportional hazard models for survival data (Tibshirani 1997).In recent years, there has been an enormous amount of research activity devoted to related regularization methods:
1.The grouped lasso (Yuan and Lin 2007;Meier et al. 2008), where variables are included or excluded in groups.
2. The Dantzig selector (Candes and Tao 2007, and discussion), a slightly modified version of the lasso.
3. The elastic net (Zou and Hastie 2005) for correlated variables, which uses a penalty that is part 1 , part 2 .
4. 1 regularization paths for generalized linear models (Park and Hastie 2007a).
5. Methods using non-concave penalties, such as SCAD (Fan and Li 2005) and Friedman's generalized elastic net (Friedman 2008), enforce more severe variable selection than the lasso.
6. Regularization paths for the support-vector machine (Hastie et al. 2004).
7. The graphical lasso (Friedman et al. 2008) for sparse covariance estimation and undirected graphs.Efron et al. (2004) developed an efficient algorithm for computing the entire regularization path for the lasso for linear regression models.Their algorithm exploits the fact that the coefficient profiles are piecewise linear, which leads to an algorithm with the same computational cost as the full least-squares fit on the data (see also Osborne et al. 2000).
In some of the extensions above (items 2,3, and 6), piecewise-linearity can be exploited as in Efron et al. (2004) to yield efficient algorithms.Rosset and Zhu (2007) characterize the class of problems where piecewise-linearity exists-both the loss function and the penalty have to be quadratic or piecewise linear.
Here we instead focus on cyclical coordinate descent methods.These methods have been proposed for the lasso a number of times, but only recently was their power fully appreciated.
Early references include Fu (1998), Shevade and Keerthi (2003) and Daubechies et al. (2004).Van der Kooij (2007) independently used coordinate descent for solving elastic-net penalized regression models.Recent rediscoveries include Friedman et al. (2007) and Wu and Lange (2008).The first paper recognized the value of solving the problem along an entire path of values for the regularization parameters, using the current estimates as warm starts.This strategy turns out to be remarkably efficient for this problem.Several other researchers have also re-discovered coordinate descent, many for solving the same problems we address in this paper-notably Shevade and Keerthi (2003), Krishnapuram and Hartemink (2005), Genkin et al. (2007) and Wu et al. (2009).
In this paper we extend the work of Friedman et al. (2007) and develop fast algorithms for fitting generalized linear models with elastic-net penalties.In particular, our models include regression, two-class logistic regression, and multinomial regression problems.Our algorithms can work on very large datasets, and can take advantage of sparsity in the feature set.We provide a publicly available package glmnet (Friedman et al. 2009) implemented in the R programming system (R Development Core Team 2009).We do not revisit the wellestablished convergence properties of coordinate descent in convex problems (Tseng 2001) in this article.
Lasso procedures are frequently used in domains with very large datasets, such as genomics and web analysis.Consequently a focus of our research has been algorithmic efficiency and speed.We demonstrate through simulations that our procedures outperform all competitors -even those based on coordinate descent.
In Section 2 we present the algorithm for the elastic net, which includes the lasso and ridge regression as special cases.Section 3 and 4 discuss (two-class) logistic regression and multinomial logistic regression.Comparative timings are presented in Section 5.
Although the title of this paper advertises regularization paths for GLMs, we only cover three important members of this family.However, exactly the same technology extends trivially to other members of the exponential family, such as the Poisson model.We plan to extend our software to cover these important other cases, as well as the Cox model for survival data.
Note that this article is about algorithms for fitting particular families of models, and not about the statistical properties of these models themselves.Such discussions have taken place elsewhere.

Algorithms for the lasso, ridge regression and elastic net

We consider the usual setup for linear regression.We have a response variable Y ∈ R and a predictor vector X ∈ R p , and we approximate the regression function by a linear model
We have N observation pairs (x i , y i ).For simplicity we assume the x ij are standardized:
x 2 ij = 1, for j = 1, . . ., p. Our algorithms generalize naturally to the unstandardized case.The elastic net solves the following problem min
where
P α is the elastic-net penalty (Zou and Hastie 2005), and is a compromise between the ridgeregression penalty (α = 0) and the lasso penalty (α = 1).This penalty is particularly useful in the p N situation, or any situation where there are many correlated predictor variables.1
Ridge regression is known to shrink the coefficients of correlated predictors towards each other, allowing them to borrow strength from each other.In the extreme case of k identical predictors, they each get identical coefficients with 1/kth the size that any single one would get if fit alone.From a Bayesian point of view, the ridge penalty is ideal if there are many predictors, and all have non-zero coefficients (drawn from a Gaussian distribution).
Lasso, on the other hand, is somewhat indifferent to very correlated predictors, and will tend to pick one and ignore the rest.In the extreme case above, the lasso problem breaks down.The lasso penalty corresponds to a Laplace prior, which expects many coefficients to be close to zero, and a small subset to be larger and nonzero.
The elastic net with α = 1 − ε for some small ε > 0 performs much like the lasso, but removes any degeneracies and wild behavior caused by extreme correlations.More generally, the entire family P α creates a useful compromise between ridge and lasso.As α increases from 0 to 1, for a given λ the sparsity of the solution to (1) (i.e., the number of coefficients equal to zero) increases monotonically from 0 to the sparsity of the lasso solution.
Figure 1 shows an example that demonstrates the effect of varying α.The dataset is from (Golub et al. 1999), consisting of 72 observations on 3571 genes measured with DNA microarrays.The observations fall in two classes, so we use the penalties in conjunction with the logistic regression models of Section 3. The coefficient profiles from the first 10 steps (grid values for λ) for each of the three regularization methods are shown.The lasso penalty admits at most N = 72 genes into the model, while ridge regression gives all 3571 genes non-zero coefficients.The elastic-net penalty provides a compromise between these two, and has the effect of averaging genes that are highly correlated and then entering the averaged gene into the model.Using the algorithm described below, computation of the entire path of solutions for each method, at 100 values of the regularization parameter evenly spaced on the log-scale, took under a second in total.Because of the large number of non-zero coefficients for the ridge penalty, they are individually much smaller than the coefficients for the other methods.
Consider a coordinate descent step for solving (1).That is, suppose we have estimates β0 and β for = j, and we wish to partially optimize with respect to β j .We would like to compute the gradient at β j = βj , which only exists if βj = 0.If βj > 0, then
A similar expression exists if βj < 0, and βj = 0 is treated separately.Simple calculus shows (Donoho and Johnstone 1994) that the coordinate-wise update has the form
where ỹ(j) i = β0 + =j x i β is the fitted value excluding the contribution from x ij , and hence y i − ỹ(j) i the partial residual for fitting β j .Because of the standardization,
is the simple least-squares coefficient when fitting this partial residual to x ij .S(z, γ) is the soft-thresholding operator with value
The details of this derivation are spelled out in Friedman et al. (2007).
Thus we compute the simple least-squares coefficient on the partial residual, apply softthresholding to take care of the lasso contribution to the penalty, and then apply a proportional shrinkage for the ridge penalty.This algorithm was suggested by Van der Kooij (2007).

Naive updates

Looking more closely at (5), we see that
where ŷi is the current fit of the model for observation i, and hence r i the current residual.Thus 1
because the x j are standardized.The first term on the right-hand side is the gradient of the loss with respect to β j .It is clear from (8) why coordinate descent is computationally efficient.Many coefficients are zero, remain zero after the thresholding, and so nothing needs to be changed.Such a step costs O(N ) operations-the sum to compute the gradient.On the other hand, if a coefficient does change after the thresholding, r i is changed in O(N ) and the step costs O(2N ).Thus a complete cycle through all p variables costs O(pN ) operations.We refer to this as the naive algorithm, since it is generally less efficient than the covariance updating algorithm to follow.Later we use these algorithms in the context of iteratively reweighted least squares (IRLS), where the observation weights change frequently; there the naive algorithm dominates.

Covariance updates

Further efficiencies can be achieved in computing the updates in (8).We can write the first term on the right (up to a factor 1/N ) as
where x j , y = N i=1 x ij y i .Hence we need to compute inner products of each feature with y initially, and then each time a new feature x k enters the model (for the first time), we need to compute and store its inner product with all the rest of the features (O(N p) operations).We also store the p gradient components (9).If one of the coefficients currently in the model changes, we can update each gradient in O(p) operations.Hence with m non-zero terms in the model, a complete cycle costs O(pm) operations if no new variables become non-zero, and costs O(N p) for each new variable entered.Importantly, O(N ) calculations do not have to be made at every step.This is the case for all penalized procedures with squared error loss.

Sparse updates

We are sometimes faced with problems where the N × p feature matrix X is extremely sparse.A leading example is from document classification, where the feature vector uses the socalled "bag-of-words" model.Each document is scored for the presence/absence of each of the words in the entire dictionary under consideration (sometimes counts are used, or some transformation of counts).Since most words are absent, the feature vector for each document is mostly zero, and so the entire matrix is mostly zero.We store such matrices efficiently in sparse column format, where we store only the non-zero entries and the coordinates where they occur.
Coordinate descent is ideally set up to exploit such sparsity, in an obvious way.The O(N ) inner-product operations in either the naive or covariance updates can exploit the sparsity, by summing over only the non-zero entries.Note that in this case scaling of the variables will not alter the sparsity, but centering will.So scaling is performed up front, but the centering is incorporated in the algorithm in an efficient and obvious manner.

Weighted updates

Often a weight w i (other than 1/N ) is associated with each observation.This will arise naturally in later sections where observations receive weights in the IRLS algorithm.In this case the update step (5) becomes only slightly more complicated:
If the x j are not standardized, there is a similar sum-of-squares term in the denominator (even without weights).The presence of weights does not change the computational costs of either algorithm much, as long as the weights remain fixed.

Pathwise coordinate descent

We compute the solutions for a decreasing sequence of values for λ, starting at the smallest value λ max for which the entire vector β = 0. Apart from giving us a path of solutions, this scheme exploits warm starts, and leads to a more stable algorithm.
We have examples where it is faster to compute the path down to λ (for small λ) than the solution only at that value for λ.
When β = 0, we see from (5) that βj will stay zero if 1 N | x j , y | < λα.Hence N αλ max = max | x , y |.Our strategy is to select a minimum value λ min = λ max , and construct a sequence of K values of λ decreasing from λ max to λ min on the log scale.Typical values are = 0.001 and K = 100.

Other details

Irrespective of whether the variables are standardized to have variance 1, we always center each predictor variable.Since the intercept is not regularized, this means that β0 = ȳ, the mean of the y i , for all values of α and λ.
It is easy to allow different penalties λ j for each of the variables.We implement this via a penalty scaling parameter γ j ≥ 0. If γ j > 0, then the penalty applied to β j is λ j = λγ j .If γ j = 0, that variable does not get penalized, and always enters the model unrestricted at the first step and remains in the model.Penalty rescaling would also allow, for example, our software to be used to implement the adaptive lasso (Zou 2006).
Considerable speedup is obtained by organizing the iterations around the active set of featuresthose with nonzero coefficients.After a complete cycle through all the variables, we iterate on only the active set till convergence.If another complete cycle does not change the active set, we are done, otherwise the process is repeated.Active-set convergence is also mentioned in Meier et al. (2008) and Krishnapuram and Hartemink (2005).

Regularized logistic regression

When the response variable is binary, the linear logistic regression model is often used.Denote by G the response variable, taking values in G = {1, 2} (the labeling of the elements is arbitrary).The logistic regression model represents the class-conditional probabilities through a linear function of the predictors
Here we fit this model by regularized maximum (binomial) likelihood.Let p(x i ) = Pr(G = 1|x i ) be the probability (11) for observation i at a particular value for the parameters (β 0 , β), then we maximize the penalized log likelihood max
Denoting y i = I(g i = 1), the log-likelihood part of ( 13) can be written in the more explicit form
a concave function of the parameters.The Newton algorithm for maximizing the (unpenalized) log-likelihood (14) amounts to iteratively reweighted least squares.Hence if the current estimates of the parameters are ( β0 , β), we form a quadratic approximation to the log-likelihood (Taylor expansion about current estimates), which is
where
, (working response) ( 16)
and p(x i ) is evaluated at the current parameters.The last term is constant.The Newton update is obtained by minimizing Q .
Our approach is similar.For each value of λ, we create an outer loop which computes the quadratic approximation Q about the current parameters ( β0 , β).Then we use coordinate descent to solve the penalized weighted least-squares problem min
This amounts to a sequence of nested loops:
outer loop: Decrement λ.
middle loop: Update the quadratic approximation Q using the current parameters ( β0 , β).
inner loop: Run the coordinate descent algorithm on the penalized weighted-least-squares problem (18).
There are several important details in the implementation of this algorithm.
When p N , one cannot run λ all the way to zero, because the saturated logistic regression fit is undefined (parameters wander off to ±∞ in order to achieve probabilities of 0 or 1).Hence the default λ sequence runs down to λ min = λ max > 0.
Care is taken to avoid coefficients diverging in order to achieve fitted probabilities of 0 or 1.When a probability is within ε = 10 −5 of 1, we set it to 1, and set the weights to ε. 0 is treated similarly.
Our code has an option to approximate the Hessian terms by an exact upper-bound.This is obtained by setting the w i in (17) all equal to 0.25 (Krishnapuram and Hartemink 2005).
We allow the response data to be supplied in the form of a two-column matrix of counts, sometimes referred to as grouped data.We discuss this in more detail in Section 4.2.
The Newton algorithm is not guaranteed to converge without step-size optimization (Lee et al. 2006).Our code does not implement any checks for divergence; this would slow it down, and when used as recommended we do not feel it is necessary.We have a closed form expression for the starting solutions, and each subsequent solution is warm-started from the previous close-by solution, which generally makes the quadratic approximations very accurate.We have not encountered any divergence problems so far.

Regularized multinomial regression

When the categorical response variable G has K > 2 levels, the linear logistic regression model can be generalized to a multi-logit model.The traditional approach is to extend ( 12
Here β is a p-vector of coefficients.As in Zhu and Hastie (2004), here we choose a more symmetric approach.We model
This parametrization is not estimable without constraints, because for any values for the parameters {β 0 , β } K 1 , {β 0 − c 0 , β − c} K 1 give identical probabilities (20).Regularization deals with this ambiguity in a natural way; see Section 4.1 below.
We fit the model ( 20) by regularized maximum (multinomial) likelihood.Using a similar notation as before, let p (x i ) = Pr(G = |x i ), and let g i ∈ {1, 2, . . ., K} be the ith response.We maximize the penalized log-likelihood max
Denote by Y the N × K indicator response matrix, with elements y i = I(g i = ).Then we can write the log-likelihood part of ( 21) in the more explicit form
The Newton algorithm for multinomial regression can be tedious, because of the vector nature of the response observations.Instead of weights w i as in ( 17), we get weight matrices, for example.However, in the spirit of coordinate descent, we can avoid these complexities.
We perform partial Newton steps by forming a partial quadratic approximation to the loglikelihood ( 22), allowing only (β 0 , β ) to vary for a single class at a time.It is not hard to show that this is
where as before
Our approach is similar to the two-class case, except now we have to cycle over the classes as well in the outer loop.For each value of λ, we create an outer loop which cycles over and computes the partial quadratic approximation Q about the current parameters ( β0 , β).
Then we use coordinate descent to solve the penalized weighted least-squares problem min
This amounts to the sequence of nested loops:
outer loop: Decrement λ.
middle loop (outer): Cycle over ∈ {1, 2, . . ., K, 1, 2 . ..}.
middle loop (inner): Update the quadratic approximation Q using the current parameters { β0k , βk } K 1 .
inner loop: Run the co-ordinate descent algorithm on the penalized weighted-least-squares problem (26).

Regularization and parameter ambiguity

As was pointed out earlier, if {β 0 , β } K 1 characterizes a fitted model for (20), then {β 0 − c 0 , β − c} K 1 gives an identical fit (c is a p-vector).Although this means that the log-likelihood part of ( 21) is insensitive to (c 0 , c), the penalty is not.In particular, we can always improve an estimate {β 0 , β } K 1 (w.r.t. ( 21)) by solving min
This can be done separately for each coordinate, hence
Theorem 1 Consider problem ( 28) for values α ∈ [0, 1].Let βj be the mean of the β j , and β M j a median of the β j (and for simplicity assume βj ≤ β M j .Then we have
with the left endpoint achieved if α = 0, and the right if α = 1.
The two endpoints are obvious.The proof of Theorem 1 is given in Appendix A. A consequence of the theorem is that a very simple search algorithm can be used to solve (28).
The objective is piecewise quadratic, with knots defined by the β j .We need only evaluate solutions in the intervals including the mean and median, and those in between.We recenter the parameters in each index set j after each inner middle loop step, using the the solution c j for each j.
Not all the parameters in our model are regularized.The intercepts β 0 are not, and with our penalty modifiers γ j (Section 2.6) others need not be as well.For these parameters we use mean centering.

Grouped and matrix responses

As in the two class case, the data can be presented in the form of a N × K matrix m i of non-negative numbers.For example, if the data are grouped: at each x i we have a number of multinomial samples, with m i falling into category .In this case we divide each row by the row-sum m i = m i , and produce our response matrix y i = m i /m i .m i becomes an observation weight.Our penalized maximum likelihood algorithm changes in a trivial way.The working response ( 24) is defined exactly the same way (using y i just defined).The weights in (25) get augmented with the observation weight m i :
Equivalently, the data can be presented directly as a matrix of class proportions, along with a weight vector.From the point of view of the algorithm, any matrix of positive numbers and any non-negative weight vector will be treated in the same way.

Timings

In this section we compare the run times of the coordinate-wise algorithm to some competing algorithms.These use the lasso penalty (α = 1) in both the regression and logistic regression settings.All timings were carried out on an Intel Xeon 2.80GH processor.
We do not perform comparisons on the elastic net versions of the penalties, since there is not much software available for elastic net.Comparisons of our glmnet code with the R package elasticnet will mimic the comparisons with lars (Hastie and Efron 2007) for the lasso, since elasticnet (Zou and Hastie 2004) is built on the lars package.

Regression with the lasso

We generated Gaussian data with N observations and p predictors, with each pair of predictors X j , X j having the same population correlation ρ.We tried a number of combinations of N and p, with ρ varying from zero to 0.95.The outcome values were generated by
where β j = (−1) j exp(−2(j − 1)/20), Z ∼ N (0, 1) and k is chosen so that the signal-to-noise ratio is 3.0.The coefficients are constructed to have alternating signs and to be exponentially decreasing.
Table 1 shows the average CPU timings for the coordinate-wise algorithm, and the lars procedure (Efron et al. 2004).All algorithms are implemented as R functions.The coordinate-wise algorithm does all of its numerical work in Fortran, while lars (Hastie and Efron 2007) does much of its work in R, calling Fortran routines for some matrix operations.However comparisons in Friedman et al. (2007) showed that lars was actually faster than a version coded entirely in Fortran.Comparisons between different programs are always tricky: in particular the lars procedure computes the entire path of solutions, while the coordinate-wise procedure solves the problem for a set of pre-defined points along the solution path.In the orthogonal case, lars takes min(N, p) steps: hence to make things roughly comparable, we called the latter two algorithms to solve a total of min(N, p) problems along the path.Table 1 shows timings in seconds averaged over three runs.We see that glmnet is considerably faster than lars; the covariance-updating version of the algorithm is a little faster than the naive version when N > p and a little slower when p > N .We had expected that high correlation between the features would increase the run time of glmnet, but this does not seem to be the case.

Lasso-logistic regression

We used the same simulation setup as above, except that we took the continuous outcome y, defined p = 1/(1 + exp(−y)) and used this to generate a two-class outcome z with Pr(z = 1) = p, Pr(z = 0) = 1 − p.We compared the speed of glmnet to the interior point method l1logreg (Koh et al. 2007b,a), Bayesian binary regression (BBR, Madigan and Lewis 2007;Genkin et al. 2007), and the lasso penalized logistic program LPL supplied by Ken Lange (see Wu and Lange 2008).The latter two methods also use a coordinate descent approach.
The BBR software automatically performs ten-fold cross-validation when given a set of λ values.Hence we report the total time for ten-fold cross-validation for all methods using the Linear regression -Dense features Correlation 0 0.1 0.2 0.5 0.9 0.95  1: Timings (in seconds) for glmnet and lars algorithms for linear regression with lasso penalty.The first line is glmnet using naive updating while the second uses covariance updating.Total time for 100 λ values, averaged over 3 runs.
same 100 λ values for all.Table 2 shows the results; in some cases, we omitted a method when it was seen to be very slow at smaller values for N or p. Again we see that glmnet is the clear winner: it slows down a little under high correlation.The computation seems to be roughly linear in N , but grows faster than linear in p. Table 3 shows some results when the feature matrix is sparse: we randomly set 95% of the feature values to zero.Again, the glmnet procedure is significantly faster than l1logreg.

Real data

Table 4 shows some timing results for four different datasets.
Cancer (Ramaswamy et al. 2002): gene-expression data with 14 cancer classes.Here we compare glmnet with BMR (Genkin et al. 2007), a multinomial version of BBR.
Leukemia (Golub et al. 1999): gene-expression data with a binary response indicating type of leukemia-AML vs ALL.We used the preprocessed data of Dettling (2004).
InternetAd (Kushmerick 1999) NewsGroup (Lang 1995): document classification problem.We used the training set cultured from these data by Koh et al. (2007a).The response is binary, and indicates a subclass of topics; the predictors are binary, and indicate the presence of particular tri-gram sequences.The predictor matrix has 0.05% nonzero values.
All four datasets are available online with this publication as saved R data objects (the latter two in sparse format using the Matrix package, Bates and Maechler 2009).
For the Leukemia and InternetAd datasets, the BBR program used fewer than 100 λ values so we estimated the total time by scaling up the time for smaller number of values.The InternetAd and NewsGroup datasets are both sparse: 1% nonzero values for the former, 0.05% for the latter.Again glmnet is considerably faster than the competing methods.

Other comparisons

When making comparisons, one invariably leaves out someones favorite method.We left out our own glmpath (Park and Hastie 2007b) extension of lars for GLMs (Park and Hastie 2007a), since it does not scale well to the size problems we consider here.an earlier draft of this paper suggested two methods of which we were not aware.We ran a single benchmark against each of these using the Leukemia data, fitting models at 100 values of λ in each case.
OWL-QN: Orthant-Wise Limited-memory Quasi-Newton Optimizer for 1 -regularized Objectives (Andrew and Gao 2007a,b).The software is written in C++, and available from the authors upon request.
The R package penalized (Goeman 2009b,a), which fits GLMs using a fast implementation of gradient ascent.
Table 5 shows these comparisons (on two different machines); glmnet is considerably faster in both cases.

Selecting the tuning parameters

The algorithms discussed in this paper compute an entire path of solutions (in λ) for any particular model, leaving the user to select a particular solution from the ensemble.One general approach is to use prediction error to guide this choice.If a user is data rich, they can set aside some fraction (say a third) of their data for this purpose.They would then evaluate the prediction performance at each value of λ, and pick the model with the best performance.Mean Squared Error q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 99 99 97 95 93 75 54 21 12 5 2 1 1.4 log(Lambda) Deviance q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 100 98 97 88 74 55 30 9 7 3 2 Misclassification Error q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 100 98 97 88 74 55 30 9 7 3 2

Gaussian Family


Binomial Family


Binomial Family

Figure 2: Ten-fold cross-validation on simulated data.The first row is for regression with a Gaussian response, the second row logistic regression with a binomial response.In both cases we have 1000 observations and 100 predictors, but the response depends on only 10 predictors.
For regression we use mean-squared prediction error as the measure of risk.For logistic regression, the left panel shows the mean deviance (minus twice the log-likelihood on the left-out data), while the right panel shows misclassification error, which is a rougher measure.In all cases we show the mean cross-validated error curve, as well as a one-standard-deviation band.In each figure the left vertical line corresponds to the minimum error, while the right vertical line the largest value of lambda such that the error is within one standard-error of the minimum-the so called "one-standard-error" rule.The top of each plot is annotated with the size of the models.
Alternatively, they can use K-fold cross-validation (Hastie et al. 2009, for example), where the training data is used both for training and testing in an unbiased way.
Figure 2 illustrates cross-validation on a simulated dataset.For logistic regression, we sometimes use the binomial deviance rather than misclassification error, since the latter is smoother.We often use the "one-standard-error" rule when selecting the best model; this acknowledges the fact that the risk curves are estimated with error, so errs on the side of parsimony (Hastie et al. 2009).Cross-validation can be used to select α as well, although it is often viewed as a higher-level parameter and chosen on more subjective grounds.

Discussion

Cyclical coordinate descent methods are a natural approach for solving convex problems with 1 or 2 constraints, or mixtures of the two (elastic net).Each coordinate-descent step is fast, with an explicit formula for each coordinate-wise minimization.The method also exploits the sparsity of the model, spending much of its time evaluating only inner products for variables with non-zero coefficients.Its computational speed both for large N and p are quite remarkable.
An R-language package glmnet is available under general public licence (GPL-2) from the Comprehensive R Archive Network at http://CRAN.R-project.org/package=glmnet.Sparse data inputs are handled by the Matrix package.MATLAB functions (Jiang 2009) are available from http://www-stat.stanford.edu/~tibs/glmnet-matlab/.

Figure 1 :

Figure 1: Leukemia data: profiles of estimated coefficients for three methods, showing only first 10 steps (values for λ) in each case.For the elastic net, α = 0.2.

Table 2 :

Timings (seconds) for logistic models with lasso penalty.Total time for tenfold cross-validation over a grid of 100 λ values.

Table 3 :

: document classification problem with mostly binary features.The response is binary, and indicates whether the document is an advertisement.Only 1.2% nonzero values in the predictor matrix.Timings (seconds) for logistic model with lasso penalty and sparse features (95% zero).Total time for ten-fold cross-validation over a grid of 100 λ values.

Table 4 :

Timings (seconds, unless stated otherwise) for some real datasets.For the Cancer, Leukemia and InternetAd datasets, times are for ten-fold cross-validation using 100 values of λ; for NewsGroup we performed a single run with 100 values of λ, with λ min = 0.05λ max .

Table 5 :

Timings (seconds) for the Leukemia dataset, using 100 λ values.These timings were performed on two different platforms, which were different again from those used in the earlier timings in this paper.


52062868:ℓ1-penalization for mixture regression models


Introduction

In applied statistics, tremendous number of applications deal with relating a random response variable Y to a set of explanatory variables or covariates X = (X (1) , . . . , X (p) ) through a regression-type model. The homogeneity assumption that the regression coefficients are the same for different observations (Y 1 , X 1 ), . . . , (Y n , X n ) is often inadequate. Parameters may change for different subgroups of observations. Such heterogeneity can be modeled with a Finite Mixture of Regressions (FMR) model. Especially with highdimensional data, where the number of covariates p is much larger than sample size n, the homogeneity assumption seems rather restrictive: at least a fraction of covariates may exhibit a different influence on the response among various observations (i.e., subpopulations). Hence, addressing the issue of heterogeneity in high-dimensional data is important in many practical applications. We will empirically demonstrate with real data in Section 7.2 that substantial prediction improvements are possible by incorporating a heterogeneity structure to the model.
We propose here an ℓ 1 -penalized method, i.e., a Lasso-type estimator (Tibshirani, 1996), for estimating a high-dimensional Finite Mixture of Regressions (FMR) model where p ≫ n. Our procedure is related to the proposal in Khalili and Chen (2007). However, we argue that a different parameterization leads to more efficient computation in high-dimensional optimization for which we prove numerical convergence properties. Our algorithm can easily handle problems where p is in the thousands. Furthermore, regarding statistical performance, we present an oracle inequality which includes the setting where p ≫ n: this is very different from Khalili and Chen (2007) who use fixed p asymptotics in the low-dimensional framework. Our theory for deriving oracle inequalities in the presence of non-convex loss functions, as the negative log-likelihood in a mixture model is non-convex, is rather non-standard but sufficiently general to cover other cases than FMR models.
From a more general point of view, we show in this paper that high-dimensional estimation problems with non-convex loss functions can be addressed with high computational efficiency and good statistical accuracy. Regarding the computation, we develop a rather generic block coordinate descent generalized EM algorithm which is surprisingly fast even for large p. Progress in efficient gradient descent methods build on various developments by Tseng (2001) and Tseng and Yun (2008), and their use for solving Lassotype convex problems has been worked out by, e.g., Fu (1998), Friedman et al. (2007), Meier et al. (2008) and Friedman et al. (2010). We present in Section 7.3 some computation times for the more involved case with non-convex objective function using a block coordinate descent generalized EM algorithm. Regarding statistical theory, almost all results for high-dimensional Lasso-type problems have been developed for convex loss functions, e.g., the squared error in a Gaussian regression (Greenshtein and Ritov, 2004;Meinshausen and Bühlmann, 2006;Zhao and Yu, 2006;Bunea et al., 2007;Zhang and Huang, 2008;Meinshausen and Yu, 2009;Wainwright, 2009;Bickel et al., 2009;Cai et al., 2009b;Candès and Plan, 2009;Zhang, 2009) or the negative log-likelihood in a generalized linear model . We present a non-trivial modification of the mathematical analysis of ℓ 1 -penalized estimation with convex loss to non-convex but smooth likelihood problems.
When estimation is defined via optimization of a non-convex objective function, there is a major gap between the actual computation and the procedure studied in theory. The computation is typically guaranteed to find a local optimum of the objective function only, whereas the theory is usually about the estimator defined by a global optimum. Particularly in high-dimensional problems, it is difficult to compute a global optimum and it would be desirable to have some theoretical properties of estimators arising from local optima. We do not provide an answer to this difficult issue in this thesis. The beauty of, e.g., the Lasso or the Dantzig selector (Candès and Tao, 2007) in high-dimensional problems is the provable correctness or optimality of the estimator which is actually computed. A challenge for future research is to establish such provable correctness of estimators involv-ing non-convex objective functions. A noticeable exception is presented in Zhang (2010) for linear models, where some theory is derived for an estimator based on a local optimum of a non-convex optimization criterion.
The rest of this article is mainly focusing on Finite Mixture of Regressions (FMR) models. Some theory for high-dimensional estimation with non-convex loss functions is presented in Section 5 for more general settings than FMR models. The further organization of the paper is as follows: Section 2 describes the FMR model with an appropriate parameterization, Section 3 introduces ℓ 1 -penalized maximum-likelihood estimation, Sections 4 and 5 present mathematical theory for the low-and high-dimensional case, Section 6 develops some efficient generalized EM algorithm and describes its numerical convergence properties, and Section 7 reports on simulations, real data analysis and computational timings.

Finite mixture of Gaussian regressions model

Our primary focus is on the following mixture model involving Gaussian components:
Y i |X i independent for i = 1, . . . , n,
ξ = (β 1 , . . . , β k , σ 1 , . . . , σ k , π 1 , . . . , π k−1 ) ∈ R kp × R k >0 × Π, Π = {π; π r > 0 for r = 1, . . . , k − 1 and k−1 r=1 π r < 1}.
Thereby, X i ∈ R p are fixed or random covariates, Y i ∈ R is a univariate response variable and ξ = (β 1 , . . . , β k , σ 1 , . . . , σ k , π 1 , . . . , π k−1 ) denotes the (p + 2) · k − 1 free parameters and π k is given by π k = 1 − k−1 r=1 π r . The model in (1) is a mixture of Gaussian regressions, where every component r has its individual vector of regression coefficients β r and error variances σ 2 r . We are particularly interested in the case where p ≫ n.

Reparameterized mixture of regressions model

We prefer to work with a reparameterized version of model (1) whose penalized maximum likelihood estimator is scale-invariant and easier to compute. The computational aspect will be discussed in greater detail in Sections 3.1 and 6. Define new parameters φ r = β r /σ r , ρ r = σ −1 r , r = 1, . . . , k.
This yields a one-to-one mapping from ξ in (1) to a new parameter vector θ = (φ 1 , . . . , φ k , ρ 1 , . . . , ρ k , π 1 , . . . , π k−1 ), and the model (1) in reparameterized form then equals:
This is the main model we are analyzing and working with. The log-likelihood function of this model equals
Since we want to deal with the p ≫ n case, we have to regularize the maximum likelihood estimator (MLE) in order to obtain reasonably accurate estimates. We propose below some ℓ 1 -norm penalized MLE which is different from a naive ℓ 1 -norm penalty for the MLE in the non-transformed model (1). Furthermore, it is well known that the (log-) likelihood function is generally unbounded. We will see in Section 3.2 that our penalization will mitigate this problem.
3 ℓ 1 -norm penalized maximum likelihood estimator
We argue first for the case of a (non-mixture) linear model why the reparameterization above in Section 2.1 is useful and quite natural.

ℓ 1 -norm penalization for reparameterized linear models

Consider a Gaussian linear model
where X i are either fixed or random covariates. In short, we often write
with n×1 vectors Y and ε, p×1 vector β and n×p matrix X. In the sequel, · denotes the Euclidean norm. The ℓ 1 -norm penalized estimator, called the Lasso (Tibshirani (1996)), is defined asβ
Here λ is a non-negative regularization parameter. The Gaussian assumption is not crucial in model (4) but it is useful to make connections to the likelihood framework. The Lasso estimator in (5) is equivalent to minimizing the penalized negative log-likelihood n −1 ℓ(β; Y 1 , . . . , Y n ) as a function of the regression coefficients β and using the ℓ 1 -penalty β 1 = p j=1 |β j |: equivalence here means that we obtain the same estimator for a potentially different tuning parameter. But the Lasso estimator in (5) does not provide an estimate of the nuisance parameter σ 2 .
In mixture models, it will be crucial to have a good estimator of σ 2 and the role of the scaling of the variance parameter is much more important than in homogeneous regression models. Hence, it is important to take σ 2 into the definition and optimization of the penalized maximum likelihood estimator: we could proceed with the following estimator,
Note that we are penalizing only the β-parameter. However, the scale parameter estimatê σ 2 λ is influenced indirectly by the amount of shrinkage λ. There are two main drawbacks of the estimator in (6). First, it is not equivariant (Lehmann, 1983) under scaling of the response. More precisely, consider the transformation
which leaves model (4) invariant. A reasonable estimator based on transformed data Y ′ should lead to estimatorsβ ′ ,σ ′ which are related toβ,σ throughβ ′ = bβ andσ ′ = bσ. This is not the case for the estimator in (6). Secondly, and as important as the first issue, the optimization in (6) is non-convex and hence, some of the major computational advantages of Lasso for high-dimensional problems is lost. We address these drawbacks by using the penalty term λ β 1 σ leading to the following estimator:
This estimator is equivariant under the scaling transformation (7), i.e., the estimatorsβ ′ ,σ ′ based on Y ′ transform asβ ′ = bβ andσ ′ = bσ. Furthermore, it penalizes the ℓ 1 -norm of the coefficients and small variances σ 2 simultaneously which has some close relations to the Bayesian Lasso (Park and Casella, 2008). For the latter, a Bayesian approach is used with a conditional Laplace prior specification of the form
and a noninformative scale-invariant marginal prior p(σ 2 ) = 1/σ 2 for σ 2 . Park and Casella (2008) argue that conditioning on σ 2 is important because it guarantees a unimodal full posterior.
Most importantly, we can re-parameterize to achieve convexity of the optimization problem
This then yields the following estimator which is equivariant under scaling and whose computation involves convex optimization:
From an algorithmic point of view, fast algorithms are available to solve the optimization in (8). Shooting algorithms (Fu, 1998) with coordinate-wise descent are especially suitable, as demonstrated by, e.g., Friedman et al. (2007), Meier et al. (2008) or Friedman et al. (2010). We describe in Section 6.1 an algorithm for estimation in a mixture of regressions model, a more complex task than the optimization for (8). As we will see in Section 6.1, we will make use of the Karush-Kuhn-Tucker (KKT) conditions in the M-Step of a generalized EM algorithm. For the simpler criterion in (8) for a non-mixture model, the KKT conditions imply the following which we state without a proof. Denote by ·, · the inner product in n-dimensional Euclidean space and by X j the jth column vector of X.
Proposition 1. Every solution (φ,ρ) of (8) satisfies:
3.2 ℓ 1 -norm penalized MLE for mixture of Gaussian regressions
Consider the mixture of Gaussian regressions model in (2). Assuming that p is large, we want to regularize the MLE. In the spirit of the approach in (8), we propose for the unknown parameter θ = (φ 1 , . . . , φ k , ρ 1 , . . . , ρ k , π 1 , . . . , π k−1 ) the estimator:
where Π = {π; π r > 0 for r = 1, . . . , k − 1 and k−1 r=1 π r < 1} with π k = 1 − k−1 r=1 π r . The value of γ ∈ {0, 1/2, 1} parameterizes three different penalties.
The first penalty function with γ = 0 is independent of the component probabilities π r . As we will see in Sections 6.1 and 6.4, the optimization for computingθ (0) λ is easiest, and we establish a rigorous result about numerical convergence of a generalized EM algorithm. The penalty with γ = 0 works fine if the components are not very unbalanced, i.e., the true π r 's aren't too different. In case of strongly unbalanced components, the penalties with values γ ∈ {1/2, 1} are to be preferred, at the price of having to pursue a more difficult optimization problem. The value of γ = 1 has been proposed by Khalili and Chen (2007) for the naively parameterized likelihood from model (1). We will report in Section 7.1 about empirical comparisons with the three different penalties involving γ ∈ {0, 1/2, 1}.
All three penalty functions involve the ℓ 1 -norm of the component specific ratios φ r = βr σr and hence small variances are penalized. The penalized criteria therefore stay finite whenever σ r → 0: this is in sharp contrast to the unpenalized MLE where the likelihood is unbounded if σ r → 0; see, for example, McLachlan and Peel (2000).
Proposition 2. Assume that Y i = 0 for all i = 1, . . . , n. Then the penalized negative log-likelihood −n −1 ℓ (0) pen,λ (θ) is bounded from below for all values θ ∈ Θ from (11). A proof is given in Appendix C. Even though Proposition 2 is only stated and proved for the penalized negative log-likelihood with γ = 0, we expect that the statement is also true for γ = 1/2 or 1.
Due to the ℓ 1 -norm penalty, the estimator is shrinking some of the components of φ 1 , . . . , φ k exactly to zero, depending on the magnitude of the regularization parameter λ. Thus, we can do variable selection as follows. Denote by S = (r, j); 1 ≤ r ≤ k, 1 ≤ j ≤ p,φ r,j = 0 .
Here,φ r,j is the jth coefficient of the estimated regression parameterφ r belonging to mixture component r. The set S denotes the collection of non-zero estimated, i.e., selected, regression coefficients in the k mixture components. Note that no significance testing is involved, but, of course, S = S (γ) λ depends on the specification of the regularization parameter λ and the type of penalty described by γ.

Adaptive ℓ 1 -norm penalization

A two-stage adaptive ℓ 1 -norm penalization for linear models has been proposed by Zou (2006), called the adaptive Lasso. It is an effective way to address some bias problems of the (one-stage) Lasso which may employ strong shrinkage of coefficients corresponding to important variables.
The two-stage adaptive ℓ 1 -norm penalized estimator for a mixture of Gaussian regressions is defined as follows. Consider an initial estimate θ ini , for example, from the estimator in (9). The adaptive criterion to be minimized involves a re-weighted ℓ 1 -norm penalty term:
where γ ∈ {0, 1/2, 1}. The estimator is then defined aŝ
where Θ is as in (11).
The adaptive Lasso in linear models has better variable selection properties than the Lasso, see Zou (2006), Huang et al. (2008), van de Geer et al. (2010). We present some theory for the adaptive estimator in FMR models in Section 4. Furthermore, we report some empirical results in Section 7.1 indicating that the two-stage adaptive method often outperforms the one-stage ℓ 1 -penalized estimator.

Selection of the tuning parameters

The regularization parameters to be selected are the number of components k, the penalty parameter λ and we may also want to select the type of the penalty function, i.e., selection of γ.
One possibility is to use a modified BIC criterion which minimizes
over a grid of candidate values for k, λ and maybe also γ. Here,θ
λ,k denotes the estimator in (9) using the parameters λ, k, γ in (10), and −ℓ(·) is the negative log-likelihood. Furthermore, d e = k +(k −1)+ j=1,...,p;r=1,...,k 1 {φ r,j =0} is the effective number of parameters (Pan and Shen, 2007).
Alternatively, we may use a cross-validation scheme for tuning parameter selection minimizing some cross-validated negative log-likelihood.
Regarding the grid of candidate values for λ, we consider 0 ≤ λ 1 < · · · < λ M ≤ λ max , where λ max is given by
At λ max , all coefficientsφ j , (j = 1, . . . , p) of the one-component model are exactly zero. Equation (16) easily follows from Proposition 1.
For the adaptive ℓ 1 -norm penalized estimator minimizing the criterion in (13), we proceed analogously but replacingθ (γ) λ,k in (15) byθ (γ) adapt;λ in (14). As initial estimator in the adaptive criterion, we propose to use the estimate in (9) which is optimally tuned using the modified BIC or some cross-validation scheme.
4 Asymptotics for fixed p and k Following the penalized likelihood theory of Fan and Li (2001), we establish first some asymptotic properties of the estimator in (10). As in Fan and Li (2001), we assume in this section that the design is random and that the number of covariates p and the number of mixture components k are fixed as sample size n → ∞. Of course, this does not reflect a truly high-dimensional scenario, but the theory and methodology is much easier for this case. An extended theory for p potentially very large in relation to n is presented in Section 5.
Denote by θ 0 the true parameter.
Theorem 1. (Consistency) Consider model (2) with random design, fixed p and k. If λ = O(n −1/2 ) (n → ∞) then, under the regularity conditions (A)-(C) from Fan and Li (2001) on the joint density function of (Y, X), there exists a local minimizerθ
A proof is given in Appendix A. Theorem 1 can be easily misunderstood. It does not guarantee the existence of an asymptotically consistent sequence of estimates. The only claim is that a clairvoyant statistician (with pre-knowledge of θ 0 ) can choose a consistent sequence of roots in the neighborhood of θ 0 (van der Vaart, 2007). In the case where −n −1 ℓ (γ) pen,λ (θ) has a unique minimizer, which is the case for a FMR model with one component, the resulting estimator would be root-n consistent. But for a general FMR model with more than one component and typically several local minimizers, this does not hold anymore. In this sense, the preceding theorem might look better than it is.
Next, we present an asymptotic oracle result in the spirit of Fan and Li (2001) for the twostage adaptive procedure described in Section 3.3. Denote by S the population analogue of (12), i.e., the set of non-zero regression coefficients. Furthermore, let θ S = ({φ r,j ; (r, j) ∈ S}, ρ 1 , . . . , ρ k , π 1 , . . . , π k−1 ) be the sub-vector of parameters corresponding to the true nonzero regression coefficients (denoted by S) and analogously forθ S .
Theorem 2. (Asymptotic oracle result for adaptive procedure) Consider model (2) with random design, fixed p and k. If λ = o(n −1/2 ), nλ → ∞ and if θ ini satisfies θ ini − θ 0 = O P (n −1/2 ), then, under the regularity conditions (A)-(C) from Fan and Li (2001) on the joint density function of (Y, X), there exists a local minimizer θ
Fisher information knowing that θ S c = 0 (i.e., the submatrix of the Fisher information at θ 0 corresponding to the variables in S).
A proof is given in Appendix A. As in Theorem 1, the assertion of the theorem is only making a statement about some local optimum. Furthermore, this result only holds for the adaptive criterion with weights w r,j = 1 |φ ini r,j | coming from a root-n consistent initial estimator θ ini : this is a rather strong assumption given the fact that Theorem 1 only ensures existence of such an estimator. The non-adaptive estimator with the ℓ 1 -norm penalty as in (10) cannot achieve sparsity and maintain root-n consistency due to the bias problem mentioned in Section 3.3 (see also Khalili and Chen (2007)).

General theory for high-dimensional setting with nonconvex smooth loss

We present here some theory, entirely different from Theorems 1 and 2, which reflects some consistency and optimality behavior of the ℓ 1 -norm penalized maximum likelihood estimator for the potentially high-dimensional framework with p ≫ n. In particular, we derive some oracle inequality which is non-asymptotic. We intentionally present this theory for ℓ 1 -penalized smooth likelihood problems which are generally non-convex; ℓ 1penalized likelihood estimation in FMR models is then a special case discussed in Section 5.3. The following Sections 5.1 -5.2 introduce some mathematical conditions and derive auxiliary results and a general oracle inequality (Theorem 3); the interpretation of these conditions and of the oracle result is discussed for the case of FMR models at the end of Section 5.3.1.

The setting and notation

Let {f ψ ; ψ ∈ Ψ} be a collection of densities with respect to the Lebesgue measure µ on R (i.e., the range for the response variable). The parameter space Ψ is assumed to be a bounded subset of some finite-dimensional space, say
where we have equipped (quite arbitrarily) the space R d with the sup-norm ψ ∞ = max 1≤j≤d |ψ j |. In our setup, the dimension d will be regarded as a fixed constant (which still covers high-dimensionality of the covariates, as we will see). Then, equivalent metrics are, e.g., the ones induced by the ℓ q -norm ψ q = ( d j=1 |ψ j | q ) 1/q (q ≥ 1). We observe a covariate X in some space X ⊂ R p and a response variable Y ∈ R. The true conditional density of Y given X = x is assumed to be equal to
That is, we assume that the true conditional density of Y given x is depending on x only through some parameter function ψ 0 (x). Of course, the introduced notation also applies to fixed instead of random covariates.
The parameter {ψ 0 (x); x ∈ X } is assumed to have a nonparametric part of interest {g 0 (x); x ∈ X } and a low-dimensional nuisance part η 0 , i.e.,
In case of FMR models, g(
and η involves the parameters ρ 1 , . . . , ρ k , π 1 , . . . , π k−1 . More details are given in Section 5.3.
With minus the log-likelihood as loss function, the so-called excess risk
is the Kullback-Leibler information. For fixed covariates x 1 , . . . , x n , we define the average excess riskĒ
and for random design, we take the expectation E[E(ψ(X)|ψ 0 (X))].

The margin

Following Tsybakov (2004) and van de Geer (2008) we call the behavior of the excess risk E(ψ|ψ 0 ) near ψ 0 the margin. We will show in Lemma 1 that the margin is quadratic.
Denote by l ψ = log f ψ the log-density. Assuming the derivatives exist, we define the score function
and the Fisher information
Of course, we can then also look at I(ψ(x)) using the parameter function ψ(x).
In the sequel, we introduce some conditions (Conditions 1 -5). Their interpretation for the case of FMR models is given at the end of Section 5.3.1. First, we will assume boundedness of third derivatives.

Condition 1 It holds that

For a symmetric, positive semi-definite matrix A, we let Λ 2 min (A) be its smallest eigenvalue.
Condition 2 For all x ∈ X , the Fisher information matrix I(ψ 0 (x)) is positive definite and, in fact,
Further we will need the following identifiability condition.
Based on these three conditions, we have the following result: Lemma 1. Assume Conditions 1, 2, and 3. Then
A proof is given in Appendix B.

The empirical process

We now specialize to the case where
where (with some abuse of notation)
We also write
to make the dependence of the parameter function ψ(x) on ϑ more explicit.
We will assume that sup
This can be viewed as a combined condition on X and φ. For example, if X is bounded by a fixed constant this supremum (for fixed φ) is finite.
Our parameter space is now
Note thatΘ is, in principle, (pk + m)-dimensional. The true parameter ϑ 0 is assumed to be an element ofΘ.
Let us define
, and the empirical process for fixed covariates
We now fix some T ≥ 1 and λ 0 ≥ 0 and define the event

Oracle inequality for the Lasso for non-convex loss functions

For an optimality result, we need some condition on the design. Denote the active set, i.e., the set of non-zero coefficients, by
Further, let
Condition 4 (Restricted eigenvalue condition). There exists a constant κ ≥ 1 such that,
For ψ(·) T = (g(·) T , η T ), we use the notation
We also write for g(·) = (g 1 (·), . . . , g k (·)) T ,
and the bound in the restricted eigenvalue condition then reads
Bounding g φ 2 Qn in terms of φ S 2 2 can be done directly using, e.g., the Cauchy-Schwarz inequality. The restricted eigenvalue condition ensures a bound in the other direction which itself is needed for an oracle inequality. Some references about the restricted eigenvalue condition are provided at the end of Section 5.3.1.
We employ the Lasso-type estimator
We omit in the sequel the dependence ofθ on λ. Note that we consider here a global minimizer which may be difficult to compute if the empirical risk
which depends only on the estimateθ, and we denote by
Theorem 3. (Oracle result for fixed design). Assume fixed covariates x 1 , . . . , x n , Conditions 1-3 and 4, and that λ ≥ 2T λ 0 for the estimator in (19) with T and λ 0 as in (18). Then on T , defined in (18), for the average excess risk (average Kullback-Leibler loss),
where c 0 and κ are defined in Lemma 1 and Condition 4, respectively.
A proof is given in Appendix B. We will give an interpretation of this result in Section 5.3.1, where we specialize to FMR models. In the case of FMR models, the probability of the set T is large as shown in detail by Lemma 3 below.
Before specializing to FMR models, we present more general results for lower bounding the probability of the set T . We make the following assumption.
Condition 5 For the score function s ϑ (·) = s ψ ϑ (·), we have
for some function G 1 (·).
Condition 5 primarily has notational character. Later, in Lemma 2 and particularly in Lemma 3, the function G 1 (·) needs to be sufficiently regular to ensure small corresponding probabilities.
Define
As we will see, we usually choose M n ≍ log(n). Let P x denote the conditional probability given (X 1 , . . . , X n ) = (x 1 , . . . , x n ) = x, and with the expression l{·} we denote the indicator function.
Lemma 2. Assume Condition 5. We have for constants c 1 , c 2 and c 3 depending on k and K, and for all T ≥ c 1 ,
with P x probability at least
where (for i = 1, . . . , n)
Regarding the constants λ 0 and K, see (20) and (17), respectively.
A proof is given in Appendix B.

FMR models

In the finite mixture of regressions model from (2) with k components, the parameter is
are the inverse standard deviations in mixture component r and the π r are the mixture coefficients. For mathematical convenience and simpler notation, we consider here the log-transformed ρ and π parameters in order to have lower and upper bounds for ρ and π. Obviously, there is a one-to-one correspondence between ϑ and θ from Section 2.1.
Let the parameter space bẽ
and π k = 1 − k−1 r=1 π r . We consider the estimator
This is the estimator from Section 3.2 with γ = 0. We emphasize the boundedness of the parameter space by using the notationΘ. In contrast to Section 4, we focus here on any global minimizer of the penalized negative log-likelihood which is arguably difficult to compute.
In the following, we transform the estimatorθ λ toθ λ in the parameterization θ from Section 2.1. Using some abuse of notation we denote the average excess risk byĒ(θ λ |θ 0 ).

Oracle result for FMR models

We specialize now our results from Section 5.2 to FMR models.
Proposition 3. For fixed design FMR models as in (2) withΘ in (21), Conditions 1,2 and 3 are met, for appropriate C 3 , Λ min and {α ε }, depending on k and K. Also Condition 5 holds with G 1 (y) = e K |y| + K.
Proof. This follows from straightforward calculations.
In order to show that the probability for the set T is large, we invoke Lemma 2 and the following result.
Lemma 3. For fixed design FMR models as in (2) withΘ in (21), for some constants c 4 , c 5 and c 6 , depending on k, and K, and for M n = c 4 √ log n and n ≥ c 6 , the following holds:
and G 1 (·) is as in Proposition 3.
A proof is given in Appendix B.
Hence, the oracle result in Theorem 3 for our ℓ 1 -norm penalized estimator in the FMR model holds on a set T , summarized in Theorem 4, and this set T has large probability due to Lemma 2 and Lemma 3 as described in the following corollary.
Corollary 1. For fixed design FMR models as in (2) withΘ in (21), we have for constants c 2 , c 4 , c 7 , c 8 depending on k and K,
where T is defined with λ 0 = M n log n log(p∨n) n and M n = c 4 √ log n. Theorem 4. (Oracle result for FMR models). Consider a fixed design FMR model as in (2) withΘ in (21). Assume Condition 4 (restricted eigenvalue condition) and that λ ≥ 2T λ 0 for the estimator in (22). Then on T , which has large probability as stated in Corollary 1, for the average excess risk (average Kullback-Leibler loss),
where c 0 and κ are defined in Lemma 1 and Condition 4, respectively.
The oracle inequality of Theorem 4 has the following well-known interpretation. First, we obtainĒ
That is, the average Kullback-Leibler risk is of the order O(sλ 2 0 ) = O(s log 3 n log(p ∨ n)/n) (take λ = 2T λ 0 , use definition (20) and the assumption on M n in Lemma 3 above) which is up to the factor log 3 n log(p ∨ n) the optimal convergence rate if one would know the s non-zero coefficients. As a second implication we obtain φ S c 1 ≤ 4(λ + T λ 0 )c 2 0 κ 2 s, saying that the noise components in S c have small estimated values (e.g., its ℓ 1 -norm converges to zero at rate O(sλ 0 )).
Note that the Conditions 1, 2, 3 and 5 hold automatically for FMR models, as described in Proposition 3. We do require a restricted eigenvalue condition on the design, here Condition 4. In fact, for the Lasso or Dantzig selector in linear models, restricted eigenvalue conditions (Koltchinskii, 2009;Bickel et al., 2009) are considerably weaker than coherence conditions (Bunea et al., 2007;Cai et al., 2009a) or assuming the restricted isometry property (Candès and Tao, 2005;Cai et al., 2009b); for an overview among the relations, see van de Geer and Bühlmann (2009).

High-dimensional consistency of FMR models

We finally give a consistency result for FMR models under weaker conditions than the oracle result from Section 5.3.1. Denote by θ 0 the true parameter vector in a FMR model. In contrast to Section 4, the number of covariates p can grow with the number of observations n. Therefore, also the true parameter θ 0 depends on n. To guarantee consistency we have to assume some sparsity condition, i.e., the ℓ 1 -norm of the true parameter can only grow with o( n/(log 3 n log(p ∨ n))).
Theorem 5. (Consistency). Consider a fixed design FMR model (2) withΘ in (21) and fixed k. Moreover, assume that φ 0 1 = k r=1 φ 0,r 1 = o( n/(log 3 n log(p ∨ n))) (n → ∞).
If λ = C log 3 n log(p ∨ n)/n for some C > 0 sufficiently large, then any (global) minimizerθ λ as in (22) satisfiesĒ
A proof is given in Appendix B. The (restricted eigenvalue) Condition 4 on the design is not required; this is typical for a high-dimensional consistency result, see Greenshtein and Ritov (2004) for the Lasso in linear models.

Numerical optimization

We present a generalized EM (GEM) algorithm for optimizing the criterion in (10) in Section 6.1. In Section 6.2 and 6.3, we give further details on speeding-up and on initializing the algorithm. Finally, we discuss numerical convergence properties in Section 6.4. For the convex penalty (γ = 0) function, we prove convergence to a stationary point.

GEM algorithm for optimization

Maximization of the log-likelihood of a mixture density is often done using the traditional EM algorithm of Dempster et al. (1977). Consider the complete log-likelihood
Here (
and the expected complete (scaled) penalized negative log-likelihood is

E-

Step: Compute Q(θ|θ (m) ), or equivalently for r = 1, . . . , k and i = 1, . . . , n
Generalized M-Step: Improve Q pen (θ|θ (m) ) w.r.t θ ∈ Θ. a) Improvement with respect to π = (π 1 , . . . , π k ):
fix φ at the present value φ (m) and improve
with respect to the probability simplex {π; π r > 0 for r = 1, . . . , k and k r=1 π r = 1}.
Denote byπ (m+1) = n i=1γ i n which is a feasible point of the simplex. We propose to update π as
where t (m) ∈ (0, 1]. In practice, t (m) is chosen to be the largest value in the grid {δ k ; k = 0, 1, 2, . . .} (0 < δ < 1) such that (23) is not increased. In our examples, δ = 0.1 worked well. b) Coordinate descent improvement with respect to φ and ρ:
A simple calculation shows that the M-Step decouples for each component into k distinct optimization problems of the form
with
Problem (24) has the same form as (8); in particular, it is convex in (ρ r , φ r,1 , . . . , φ r,p ). Instead of fully optimizing (24), we only minimize with respect to each of the coordinates, holding the other coordinates at their current value. Closed-form coordinate updates can easily be computed for each component r (r = 1, . . . , k) using Proposition 1:
where S j is defined as
r,s X j ,X s and j = 1, . . . , p.
Because we only improve Q pen (θ|θ (m) ) instead of a full minimization, see M-Step a) and b), this is a generalized EM (GEM) algorithm. We call it the block coordinate descent generalized EM algorithm (BCD-GEM); the word block refers to the fact that we are updating all components of π at once. Its numerical properties are discussed in Section 6.4. Remark 1. For the convex penalty function with γ = 0, a minimization with respect to π in M-Step a) is achieved with π (m+1) = n i=1γ i n , i.e., using t (m) = 1. Then, our M-Step corresponds to exact coordinate-wise minimization of Q pen (θ|θ (m) ).

Active set algorithm

There is a simple way to speed-up the algorithm described above. When updating the coordinates φ r,j in the M-Step b), we restrict ourselves during every 10 EM-iterations to the current active set (the non-zero coordinates) and visit the remaining coordinates every 11th EM-iteration to update the active set. In very high-dimensional and sparse settings, this leads to a remarkable decrease in computational times. A similar active set strategy is also used in Friedman et al. (2007) and Meier et al. (2008). We illustrate in Section 7.3 the gain of speed when staying during every 10 EM-iterations within the active set.

Initialization

The algorithm of Section 6.1 requires the specification of starting values θ (0) . We found empirically that the following initialization works well. For each observation i, i = 1, . . . , n, draw randomly a class κ ∈ {1, . . . , k}. Assign for observation i and the corresponding component κ the weightγ i,κ = 0.9 and weightsγ i,r = 0.1 for all other components. Finally, normalizeγ i,r , r = 1, . . . , k, to achieve that summing over the indices k yields the value one, to get the normalized valuesγ i,r . Note that this can be viewed as an initialization of the E-Step. In the M-Step which follows afterwards, we update all coordinates from the initial values φ (0) r,j = 0, ρ (0) r = 2, π (0) r = 1/k, r = 1, . . . , k, j = 1, . . . , p.

Numerical convergence of the BCD-GEM algorithm

We address here the convergence properties of the BCD-GEM algorithm described in Section 6.1. A detailed account of the convergence properties of the EM algorithm in a general setting has been given by Wu (1983). Under regularity conditions including differentiability and continuity, convergence to stationary points is proved for the EM algorithm. For the GEM algorithm, similar statements are true under conditions which are often hard to verify.
As a GEM algorithm, our BCD-GEM algorithm has the descent property which means that the criterion function is reduced in each iteration:
Since −n −1 ℓ
pen,λ (θ) is bounded from below (Proposition 2), the following result holds.
pen,λ (θ (m) ) decreases monotonically to some valuel > −∞.
In Remark 1, we noted that, for the convex penalty function with γ = 0, the M-Step of the algorithm corresponds to exact coordinate-wise minimization of Q pen (θ|θ (m) ). In this case, convergence to a stationary point can be shown.
Theorem 6. Consider the BCD-GEM algorithm for the criterion function in (10) with γ = 0. Then, every cluster pointθ ∈ Θ of the sequence {θ (m) ; m = 0, 1, 2, . . .}, generated by the BCD-GEM algorithm, is a stationary point of the criterion function in (10).
A precise definition of a stationary point in a non-differentiable setup and a proof of the Theorem are given in Appendix C. The proof uses the crucial facts that Q pen (θ|θ ′ ) is a convex function in θ and that it is strictly convex in each coordinate of θ.
7 Simulations, real data example and computational timings

Simulations

We consider four different simulation setups. Simulation scenario 1 compares the performance of the unpenalized MLE with our estimators from Section 3.2 (FMRLasso) and Section 3.3 (FMRAdapt) in a situation where the total number of noise covariates grows successively. For computing the unpenalized MLE, we used the R-package FlexMix (Leisch, 2004;Leisch, 2007, 2008); Simulation 2 explores sparsity; Simulation 3 compares cross-validation and BIC; and Simulation 4 compares the different penalty functions with the parameters γ = 0, 1/2, 1. For every setting, the results are based on 100 independent simulation runs.
All simulations are based on Gaussian FMR models as in (1); the coefficients π r , β r , σ r and the sample size n are specified below. The covariate X is generated from a multivariate normal distribution with mean 0 and covariance structure as specified below.
Unless otherwise specified, the penalty with γ = 1 is used in all simulations. As explored empirically in Simulation 4, in case of balanced problems (approximately equal π r ), the FMRLasso performs similarly for all three penalties. In unbalanced situations, the best results are typically achieved with γ = 1. In addition, unless otherwise specified, the true number of components k is assumed to be known.
For all models, training-, validation-and test data are generated of equal size n. The estimators are computed on the training data, with the tuning parameter (e.g., λ) selected by minimizing twice the negative log-likelihood (log-likelihood loss) on the validation data. As performance measure, the predictive log-likelihood loss (twice the negative log-likelihood) of the selected model is computed on the test data.
Regarding variable selection, we count a covariable X (j) as selected ifβ r,j = 0 for at least one r ∈ {1, . . . , k}. To assess the performance of FMRLasso on recovering the sparsity structure, we report the number of truly selected covariates (True Positives) and falsely selected covariates (False Positives).
Obviously, the performances depend on the signal-to-noise ratio (SNR) which we define for an FMR model as SNR = Var(Y ) Var(Y |β r = 0; r = 1, . . . , k) = k r=1 π r (β T r Cov(X)β r + σ 2 r ) k r=1 π r σ 2 r , where the last equality follows since E[X] = 0.

Simulation 1

We consider five different FMR models: M1, M2, M3, M4 and M5. The parameters (π r , β r , σ r ), the sample size n of the training-, validation-and test-data, the correlation structure of covariates corr(X (l) , X (m) ) and the signal-to-noise ratio (SNR) are specified in Table 1. Models M1, M2, M3 and M5 have two components and five active covariates, whereas model M4 has three components and six active covariates. M1, M2 and M3 differ only in their variances σ 2 1 , σ 2 2 and hence have different signal-to-noise ratios. Model M5 has a non-diagonal covariance structure. Furthermore, in model M5, the variances σ 2 1 , σ 2 2 are tuned to achieve the same signal-to-noise ratio as in model M1.
We compare the performances of the maximum likelihood estimator (MLE), the FMRLasso and the FMRAdapt in a situation where the number of noise covariates grows successively. For the models M1, M2, M3, M5 with two components, we start with p tot = 5 (no noise covariates) and go up to p tot = 125 (120 noise covariates). For the three component model M4, we start with p tot = 6 (no noise covariates) and go up to p tot = 155 (149 noise covariates).
The boxplots in Figures 1 -5 of the predictive log-likelihood loss, denoted by Error, the True Positives (TP ) and the False Positives (FP ) over 100 simulation runs summarize the results for the different models. We read off from these boxplots that the MLE performs very badly when we add noise covariates. On the other hand, our penalized estimators remain stable. For example, for M1 the MLE with p tot = 20 performs worse than the FMRLasso with p tot = 125, or for M4 the MLE with p tot = 10 performs worse than the FMRLasso with p tot = 75. Impressive is also the huge gain of the FMRAdapt method over FMRLasso in terms of log-likelihood loss and false positives. (3,3,3,3,3) (3,3,3,3,3) (3,3,3,3,3) (3,3,0,0,0,0) (3,3,3,3,3) β 2 (-1,-1,-1,-1,-1) (-1,-1,-1,-1,-1) (-1,-1,-1,-1,-1) (0,0,-2,-2,0,0) (-1,-1,-1,-1,-1) β 3 ---(0,0,0,0,-3,2) σ 0.5, 0.5 1, 1 1.5, 1.5 0.5, 0.5, 0.5 0.95, 0.95 π 0.5, 0.5 0.5, 0.5 0.5, 0.5 1/3, 1/3, 1/3 0.5, 0.5 corr(X (l) , X (m) ) δ l,m δ l,m δ l,m δ l,m 0.8 |l−m| SNR 101 26 12.1 53 101 Table 1: Models for simulation 1. δ l,m denotes Kronecker's delta.

Simulation 2

In this section, we explore the sparsity properties of the FMRLasso. The model specifications are given in Table 2. Consider the ratios p act : n : p tot . The total number of covariates p tot grows faster than the number of observations n and the number of active covariates p act : when p tot is doubled, p act is raised by one and n is raised by 50 from model to model. In particular, we obtain a series of models which gets "sparser" as n grows (larger ratio n/p act ). In order to compare the performance of the FMRLasso, we report the True Positive Rate (TPR) and the False Positive Rate (FPR) defined as: TPR = #truly selected covariates #active covariates , FPR = #falsely selected covariates #inactive covariates .
These numbers are reported in Figure 6. We see that the False Positive Rate approaches zero for sparser models, indicating that the FMRLasso recovers the true model better in sparser settings regardless of the large number of noise covariates.

Simulation 3

So far, we regarded the number k of components as given, while we have chosen an optimal λ opt by minimizing the negative log-likelihood loss on validation data. In this section, we compare the performance of 10-fold cross-validation and the BIC criterion presented in Section 3.4 for selecting the tuning parameters k and λ. We use model M1 of Section 7.1.1 with p tot = 25, 50, 75. For each of these models, we tune the FMRLasso estimator according to the following strategies: (-1, -1, -1, 0, 0, . . . ) σ 0.5, 0.5 π 0.5, 0.5 Table 2: Series of models for simulation 2 which gets "sparser" as n grows: when p tot is doubled, p act is raised by one and n is raised by 50 from model to model. (1) Assume the number of components is given (k = 2). Choose the optimal tuning parameter λ opt using 10-fold cross-validation.
(2) Assume the number of components is given (k = 2). Choose λ opt by minimizing the BIC criterion (15).
(3) Choose the number of components k ∈ {1, 2, 3} and λ opt by minimizing the BIC criterion (15).
The results of this simulation are presented in Figure 7, where boxplots of the log-likelihood loss (Error ) are shown. All three strategies perform equally well. With p tot = 25 the BIC criterion in strategy (3) always chooses k = 2. For the model with p tot = 50, strategy (3) chooses k = 2 in 98 simulation runs and k = 3 in two runs. Finally, with p tot = 75, the third strategy chooses k = 2 in 92 runs and k = 3 eight times.

Simulation 4

In the preceding simulations, we always used the value γ = 1 in the penalty term of the FMRLasso estimator (10). In this section, we compare the FMRLasso for different values γ = 0, 1/2, 1. First, we compute the FMRLasso for γ = 0, 1/2, 1 on model M1 of Section 7.1.1 with p tot = 50. Then we do the same calculations for an "unbalanced" version of this model with π 1 = 0.3 and π 2 = 0.7.
In Figure 8, the boxplots of the log-likelihood loss (Error ), the False Positives (FP ) and the True Positives (TP ) over 100 simulation runs are shown. We see that the FMRLasso performs similarly for γ = 0, 1/2, 1. Nevertheless, the value γ = 1 is slightly preferable in the "unbalanced" setup.
(1) The lower row of panels shows the same boxplots for an "unbalanced" version of model M1 with π 1 = 0.3 and π 2 = 0.7.

Real data example

We now apply the FMRLasso to a dataset of riboflavin (vitamin B 2 ) production by Bacillus Subtilis. The real-valued response variable is the logarithm of the riboflavin production rate. The data has been kindly provided by DSM (Switzerland). There are p = 4088 covariates (genes) measuring the logarithm of the expression level of 4088 genes and measurements of n = 146 genetically engineered mutants of Bacillus Subtilis. The population seems to be rather heterogeneous as there are different strains of Bacillus Subtilis which are cultured under different fermentation conditions. We do not know the different homogeneity subgroups. For this reason, a FMR model with more than one component might be more appropriate than a simple linear regression model.
We compute the FMRLasso estimator for k = 1, . . . , 5 components. To keep the computational effort reasonable, we use only the 100 covariates (genes) exhibiting the highest empirical variances. We choose the optimal tuning parameter λ opt by 10-fold cross-validation (using the log-likelihood loss). As a result, we get five different estimators which we compare according to their cross-validated log-likelihood loss (CV Error ). These numbers are plotted in Figure 9. The estimator with three components performs clearly best, resulting in a 17% improvement in prediction over a (non-mixture) linear model, and it selects 51 genes. In Figure 10, the coefficients of the 20 most important genes, ordered according to 3 r=1 |β r,j |, are shown. From the important variables, only gene 83 shows the opposite sign of the estimated regression coefficients among the three different mixture components. However, it happens that some covariates (genes) exhibit a strong effect in one or two mixture components but none in the remaining other components. Finally, for comparison, the one-component (non-mixture) model selects 26 genes, of which 24 are also selected in the three-component model.  Figure 10: Riboflavin production data. Coefficients of the 20 most important genes, ordered according to 3 r=1 |β r,j |, for the prediction optimal model with three components.

Computational timings

In this section, we report on the run times of the BCD-GEM algorithm on two highdimensional examples. In particular, we focus on the substantial gain of speed achieved by using the active set version of the algorithm described in Section 6.2. All computations were carried out with the statistical computing language and environment R. Timings depend on the stopping criterion used in the algorithm. We stop the algorithm if the relative function improvement and the relative change of the parameter vector are small enough, i.e.,
We consider a high-dimensional version of the two component model M1 from Section 7.1.1 with n = 200, p tot = 1000 and the riboflavin dataset from Section 7.2 with three components, n = 146 and p tot = 100. We use the BCD-GEM algorithm with and without active set strategy to fit the FMRLasso on a small grid of eight values for λ. The corresponding BIC, CPU times (in seconds) and number of EM-iterations are reported in Tables 3 and  4. The values for the BCD-GEM without active set strategy are written in brackets. For model M1 and an appropriate λ with minimal BIC score, the active set algorithm converges in 5.96 seconds whereas the standard BCD-GEM needs 53.15 seconds. There is also a considerable gain of speed for the real data: 0.89 seconds versus 3.57 seconds for λ with optimal BIC. Note that in

Discussion

We have presented an ℓ 1 -penalized estimator for a finite mixture of high-dimensional Gaussian regressions where the number of covariates may greatly exceed sample size. Such a model and the corresponding Lasso-type estimator are useful to blindly account for often encountered inhomogeneity of high-dimensional data. On a high-dimensional real data example, we demonstrate a 17% gain in prediction accuracy over a (non-mixture) linear model.
The computation and mathematical analysis in such a high-dimensional mixture model is challenging due to the non-convex behavior of the negative log-likelihood. Moreover, with high-dimensional estimation defined via optimization of a non-convex objective function, there is a major gap between the actual computation and the procedure analyzed in theory. We do not provide an answer to this issue in this thesis. Regarding the computation in FMR models, a simple reparameterization is very beneficial and the ℓ 1penalty term makes the optimization problem numerically much better behaved. We develop an efficient generalized EM algorithm and we prove its numerical convergence to a stationary point. Regarding the statistical properties, besides standard low-dimensional asymptotics, we present a non-asymptotic oracle inequality for the Lasso-type estimator in a high-dimensional setting with general, non-convex but smooth loss functions. The mathematical arguments are different than what is typically used for convex losses.

Appendices

A Proofs for Section 4 A.1 Proof of Theorem 1
We assume the regularity assumptions (A)-(C) of Fan and Li (2001). The theorem follows from Theorem 1 of Fan and Li (2001). ⊔ ⊓

A.2 Proof of Theorem 2

In order to keep the notation simple, we give the proof for a two class mixture with k = 2. All arguments in the proof can also be used for a general mixture with more than two components. Remember that −n −1 ℓ adapt (θ) is given by
where ℓ(θ) is the log-likelihood function. The weights w r,j are given by w r,j = 1 |φ ini r,j | , r = 1, 2, and j = 1, . . . , p.
Assertion 1. Letθ be a root-n consistent local minimizer of −n −1 ℓ adapt (θ) (construction as in Fan and Li (2001)).
For all (r, j) ∈ S, we easily see from consistency ofθ that P[(r, j) ∈Ŝ] → 1. It then remains to show that for all (r, j) ∈ S c , P[(r, j) ∈Ŝ c ] → 1. Assume the contrary, i.e., w.l.o.g there is an s ∈ {1, . . . , p} with φ 1,s = 0 such thatφ 1,s = 0 with non-vanishing probability.
By Taylor's theorem, applied to the function n −1 ∂ℓ(θ) ∂φ 1,s , there exists a (random) vectorθ on the line segment between θ 0 andθ such that 1 n ∂ℓ adapt ∂φ 1,s θ = 1 n ∂ℓ ∂φ 1,s θ 0 (1)
Now, using the regularity assumptions and the central limit theorem, term (1) is of order O P ( 1 √ n ). Similarly, term (2) is of order O P (1) by the law of large numbers. Term (3) is of order O P (1) by the law of large numbers and the regularity condition on the third derivatives (condition (C) of Fan and Li (2001)). Therefore, we have 1 n ∂ℓ adapt ∂φ 1,s θ = O P (
Asθ is root-n consistent we get
From the assumption on the initial estimator, we have
Therefore, the second term in the brackets of (26) dominates the first and the probability of the event sgn 1 n ∂ℓ adapt ∂φ 1,s θ = −sgn(φ 1,s ) = 0 tends to 1. But this contradicts the assumption thatθ is a local minimizer (i.e., 1 n ∂ℓ adapt ∂φ 1,s θ = 0).

Assertion 2.

Write θ = (θ S , θ S c ). From part 1), it follows that with probability tending to oneθ S is a root-n local minimizer of −n −1 ℓ adapt (θ S , 0). By using a Taylor expansion we find, (1) is of order −I S (θ 0 ) + o P (1) (law of large numbers); term (2) is of order o P (1) (consistency); and term (3), with some abuse of notation an (|S| + 3)-vector of (|S| + 3) × (|S| + 3) matrices, is of order O P (1) (law of large numbers and regularity condition on the third derivatives). Therefore, we have
Notice that 1 √ n ℓ ′ | θ 0,S d N (0, I S (θ 0 )) by the central limit theorem. Furthermore, √ nλ = o(1) as λ = o(n −1/2 ). Therefore, √ n(θ S − θ 0,S ) d N (0, I S (θ 0 ) −1 ) follows from Equation (27)
Hence E(ψ|ψ 0 (x)) ≥ ψ − ψ 0 (x) 2 2 Λ 2 min /2 − d 3/2 C 3 ψ − ψ 0 (x) 3 2 /6. Now, apply the auxiliary lemma below, with K 2 0 = dK 2 , Λ 2 = Λ 2 min /2, and C = d 3/2 C 3 /6. ⊔ ⊓ Auxiliary Lemma. Let h : [−K 0 , K 0 ] → [0, ∞) have the following properties:

Proof (Auxiliary Lemma)

If ε 0 > K 0 , we have h(z) ≥ Λ 2 z 2 /2 for all |z| ≤ K 0 .
If ε 0 ≤ K 0 and |z| ≤ ε 0 , we also have h(z) ≥ (Λ 2 − ε 0 C)z 2 ≥ Λ 2 z 2 /2.

B.2 Proof of Lemma 2

In order to prove Lemma 2, we first state and proof a suitable entropy bound:
We introduce the norm
For a collection H of functions on X × Y, we let H(·, H, · Pn ) be the entropy of H equipped with the metric induced by the norm · Pn (for a definition of the entropy of a metric space see van de Geer (2000)).
Define for ǫ > 0,
Entropy Lemma For a constant C 0 depending on k and m, we have for all u > 0 and M n > 0,
Proof (Entropy Lemma) We have
It follows that
Let N (·, Γ, τ ) denote the covering number of a metric space (Γ, τ ) with metric (induced by the norm) τ , and H(·, Γ, τ ) = log N (·, Γ, τ ) be its entropy (for a definition of the covering number of a metric space see van de Geer (2000)). If Γ is a ball with radius ǫ in Euclidean space R N , one easily verifies that
Thus H(u, {η ∈ R m : ||η − η 0 || 2 ≤ ǫ}, · 2 ) ≤ m log 5ǫ u , ∀u > 0. Moreover, applying a bound as in Lemma 2.6.11 of van der Vaart and Wellner (1996) gives
We can therefore conclude that H 3 √ dM n u, (L ϑ − L ϑ 0 )l{G 1 ≤ M n } : ϑ ∈Θ(ǫ) , · Pn ≤ ǫ 2 u 2 + m + 1 log 5ǫ u + log(1 + kp) .
Let's now turn to the main proof of Lemma 2.
In what follows, {c t } are constants depending on Λ max , k, m and K. The truncated version of the empirical process is
Let ǫ > 0 be arbitrary. We invoke Lemma 3.2 in van de Geer (2000), combined with a symmetrization lemma (e.g., a conditional version of Lemma 3.3 in van de Geer (2000)).
We apply these lemmas to the class (L ϑ − L ϑ 0 )l{G 1 ≤ M n } : ϑ ∈Θ(ǫ) .
In the notation used in Lemma 3.2 of van de Geer (2000), we take δ = c 4 T ǫM n log n log(p ∨ n)/n, and R = c 5 (ǫ ∧ 1)M n . This then gives
Here, we use the bound (for 0 < a ≤ 1), 1 a 1 u log 1 u du ≤ log 3/2 1 a .
We then invoke the peeling device: split the setΘ into sets
where j ∈ Z, and 2 −j+1 ≥ λ 0 . There are no more than c 9 log n indices j ≤ 0 with 2 −j+1 ≥ λ 0 . Hence, we get
with P x probability at least
Finally, to remove the truncation, we use |(L ϑ (x, y) − L ϑ 0 (x, y))l{G 1 (y) > M n }| ≤ dKG 1 (y)l{G 1 (y) > M n }.

Hence

(V trunc n (ϑ) − V trunc n (ϑ 0 )) − (V n (ϑ) − V n (ϑ 0 )) ( φ − φ * 1 + η − η * 2 ) ∨ λ 0 ≤ dK nλ 0 n i=1 G 1 (Y i )l{G 1 (Y i ) > M n } + E G 1 (Y )l{G 1 (Y ) > M n } X = x i . ⊔ ⊓

B.3 Proof of Theorem 3

On TĒ
Then we findĒ
Then we getĒ
Case 3 Suppose that φ − φ 0 1 + η − η 0 2 ≥ λ 0 , and that
Then we haveĒ
We can then apply the restricted eigenvalue condition toφ − φ 0 . This gives
So we arrive atĒ (ψ|ψ 0 ) + 2(λ − T λ 0 ) φ S c 1 ≤ 8(λ + T λ 0 ) 2 c 2 0 κ 2 s. Thus, for n independent copies Z 1 , . . . , Z n of Z, and M = 2 √ log n,
The result follows from this, as G 1 (Y ) = e K |Y | + K, and Y has a normal mixture distribution. ⊔ ⊓

B.5 Proof of Theorem 5

On T , defined in (18) with λ 0 = c 4 log 3 n log(p ∨ n)/n (c 4 as in Lemma 3; i.e., M n = c 4 log(n) in (20)), we have the basic inequalitȳ E(ψ|ψ 0 ) + λ φ 1 ≤ T λ 0 ( φ − φ 0 1 + η − η 0 2 ) ∨ λ 0 + λ φ 0 1 +Ē(ψ 0 |ψ 0 ).
Note that η − η 0 2 ≤ 2K andĒ(ψ 0 |ψ 0 ) = 0. Hence, for n sufficiently large, E(ψ|ψ 0 ) + λ φ 1 ≤ T λ 0 ( φ − φ 0 1 + 2K) + λ φ 0 1 +Ē(ψ 0 |ψ 0 ) ≤ T λ 0 ( φ 1 + φ 0 1 + 2K) + λ φ 0 1 +Ē(ψ 0 |ψ 0 ), and therefore alsō E(ψ|ψ 0 ) + (λ − T λ 0 ) φ 1 ≤ T λ 0 2K + (λ + T λ 0 ) φ 0 1 +Ē(ψ 0 |ψ 0 ).
It holds that λ ≥ 2T λ 0 (since λ = C log 3 n log(p ∨ n)/n for some C > 0 sufficiently large), λ 0 = O( log 3 n log(p ∨ n)/n) and λ = O( log 3 n log(p ∨ n)/n), and due to the assumption about φ 0 1 we obtain on the set T thatĒ(ψ|ψ 0 ) →Ē(ψ 0 |ψ 0 ) = 0 (n → ∞). Finally, the set T has large probability, as shown by Lemma 2 and using Proposition 3 and Lemma 3 for FMR models. ⊔ ⊓ C Proofs for Sections 3 and 6 C.1 Proof of Proposition 2
We restrict ourselves to a two class mixture with k = 2. Consider the function u(ξ) defined as u(ξ) = exp(ℓ (0) pen (ξ))
We will show that u(ξ) is bounded from above for ξ = (β 1 , β 2 , σ 1 , σ 2 , π 1 ) ∈ Ξ = R 2p × R 2 >0 ×[0, 1]. Then, clearly, −n −1 ℓ
pen (θ) is bounded from below for θ = (φ 1 , φ 2 , ρ 1 , ρ 2 , π 1 ) ∈ Θ = R 2p × R 2 >0 × (0, 1). The critical point for unboundedness is if we choose for an arbitrary sample point i ∈ {1, . . . , n} a β * 1 such that Y i − X T i β * 1 = 0 and let σ 1 → 0. Without the penalty term exp(−λ β * 1 1 σ 1 ) in (28) the function would tend to infinity as σ 1 → 0. But as Y i = 0 for all i ∈ {1, . . . , n}, β * 1 cannot be zero, and therefore exp(−λ β * 1 1 σ 1 ) forces u(ξ) to tend to 0 as σ 1 → 0.
Let us give a more formal proof for the boundedness of u(ξ). Choose a small 0 < ε 1 < min i {Y 2 i } and ε 2 > 0. As Y i = 0, i = 1, . . . , n, there exists a small constant m > 0 such that
holds for all i = 1, . . . , n as long as β 1 1 < m, and
holds for all i = 1, . . . , n as long as β 2 1 < m.
Furthermore, there exists a small constant δ > 0 such that
hold for all 0 < σ 1 < δ, and
hold for all 0 < σ 2 < δ.
Define the set K = {(β 1 , β 2 , σ 1 , σ 2 , π 1 ) ∈ Ξ; δ ≤ σ 1 , σ 2 }. Now u(ξ) is trivially bounded on K. From the construction of K and Equations (29)-(32), we easily see that u(ξ) is also bounded on K c , and therefore bounded on Ξ. ⊔ ⊓

C.2 Proof of Theorem 6

The density of the complete data is given by
whereas the density of the observed data is given by
i φr) 2 , θ = (ρ 1 , . . . , ρ k , φ 1,1 , φ 1,2 , . . . , φ k,p , π) ∈ Θ, Θ = R k >0 × R kp × Π ⊂ R k(p+2)−1 , with Π = {π = (π 1 , . . . , π k−1 ); π r > 0 for r = 1, . . . , k − 1 and k−1 r=1 π r < 1}, π k = 1 − k−1 r=1 π r .
Furthermore, the conditional density of the complete data given the observed data is given by k(Y, ∆|Y, θ) = f c (Y, ∆|θ)/f obs (Y |θ). Then, the penalized negative log-likelihood fulfills the equation ν pen (θ) = −n −1 ℓ
pen,λ (θ)
where Q pen (θ|θ ′ ) = −n −1 E θ ′ [log f c (Y, ∆|θ)|Y ] + λ k r=1 φ r 1 (compare Section 6.1) and H(θ|θ ′ ) = −n −1 E θ ′ [log k(Y, ∆|Y, θ)|Y ].
By Jensen's inequality, we get the following important relationship
see also Wu (1983). Q pen (θ|θ ′ ) and H(θ|θ ′ ) are continuous functions in θ and θ ′ . If we think of them as functions of θ with fixed θ ′ , we write also Q pen,θ ′ (θ) and Hθ ′ (θ). Furthermore, Q pen,θ ′ (θ) is a convex function of θ and strictly convex in each coordinate of θ. As a last preparation, we give a definition of a stationary point for non-differentiable functions (see also Tseng (2001) u(x + αd) − u(x) α ≥ 0 ∀d ∈ R k(p+2)−1 .
We are now ready to start with the proof which is inspired by Bertsekas (1995). We write θ = (θ 1 , . . . , θ D ) = (ρ 1 , . . . , ρ k , φ 1,1 , φ 1,2 , . . . , φ k,p , π),
where D = k + kp + 1 denotes the number of coordinates. Remark that the first D − 1 coordinates are univariate, whereas θ D = π is a "block coordinate" of dimension k − 1.
Proof. Let θ m = θ (m) be the sequence generated by the BCD-GEM algorithm. We need to prove that for a converging subsequence θ m j →θ ∈ Θ,θ is a stationary point of ν pen (θ). Taking directional derivatives of Equation (33) yields ν ′ pen (θ; d) = Q ′ pen,θ (θ; d) − ∇ Hθ(θ), d .
Note that ∇ Hθ(θ) = 0 as Hθ(x) is minimized for x =θ (Equation (34)). Therefore, it remains to show that Q ′ pen,θ (θ; d) ≥ 0 for all directions d. Let Using the definition of the algorithm, we have Q pen,θ m (θ m ) ≥ Q pen,θ m (z m 1 ) ≥ · · · ≥ Q pen,θ m (z m D−1 ) ≥ Q pen,θ m (θ m+1 ).
From the definition of the algorithm, we have Q pen (z m j 1 |θ m j ) ≤ Q pen (x 1 , θ m j 2 , . . . , θ m j D |θ m j ) ∀x 1 .
By continuity and taking the limit j → ∞, we obtain Q pen,θ (θ) ≤ Q pen,θ (x 1 ,θ 2 , . . . ,θ D ) ∀x 1 .
Repeating the argument we conclude thatθ is a coordinate-wise minimum. Therefore, following Tseng (2001),θ is easily seen to be a stationary point of Q pen,θ (·), in particular Q ′ pen,θ (θ; d) ≥ 0 for all directions d.

∆ i, 1

1, . . . , ∆ i,k ), i = 1, . . . , n, are i.i.d. unobserved multinomial variables showing the component-membership of the ith observation in the FMR model: ∆ i,r = 1 if observation i belongs to component r, and ∆ i,r = 0 otherwise. The expected complete (scaled) negative log-likelihood is then
The EM algorithm works by alternating between the E-and M-Step. Denote the parameter value at EM-iteration m by θ (m) (m = 0, 1, 2, . . .), where θ (0) is a vector of starting values.

Proposition 4 .

4For the BCD-GEM algorithm, −n −1 ℓ

Figure 1 :

1Simulation 1, Model M1. Top: Predictive log-likelihood loss (Error ) for MLE, FMRLasso, FMRAdapt. Bottom: False Positives (FP ) and True Positives (TP ) for FM-RLasso and FMRAdapt.

Figure 2 :Figure 3 :Figure 4 :Figure 5 :

2345Simulation 1, Model M2. Same notation as inFigure 1. Simulation 1, Model M3. Same notation as inFigure 1. Simulation 1, Model M4. Same notation as inFigure 1. Simulation 1, Model M5. Same notation as inFigure 1.

Figure 6 :

6Simulation 2 compares the performance of the FMRLasso for a series of models which gets "sparser" as the sample size grows. Top: True Positive Rate (TPR). Bottom: False Positive Rate (FPR) over 100 simulation runs.

Figure 7 :Figure 8 :

78Simulation 3 compares different strategies for choosing the tuning parameters k and λ. The boxplots show the predictive log-likelihood loss (Error ) of the FMRLasso, tuned by strategies(1),(2)and(3), for model M1 with p tot = 25, 50, 75. Simulation 4 compares the FMRLasso for different values γ = 0, 1/2, 1. The upper row of panels shows the boxplots of the log-likelihood loss (Error ), the False Positives (FP ) and the True Positives (TP ) for model M1 with p tot = 50 and π 1 = π 2 = 0.5.

Figure 9 :

9Riboflavin production data. Cross-validated negative log-likelihood loss (CV Error ) for the FMRLasso estimator when varying over different numbers of components.
Let Z be a standard normal random variable. Then by straightforward computations, for all M > 0, E[|Z|l{|Z| > M }] ≤ 2 exp[−M 2 /2], and E[|Z| 2 l{|Z| > M }] ≤ (M + 2) exp[−M 2 /2].

): Definition 1 .

1Let u be a function defined on an open setU ⊂ R k(p+2)−1 . A point x ∈ U is called stationary if u ′ (x; d) = lim α↓0

,

. . . , θ m+1 i , θ m i+1 , . . . , θ m D ).

Table 3 ,

3the BIC scores sometimes differ substantially for 
inappropriate values of λ. For such regularization parameters, the solutions are unstable 
and different local optima are attained depending on the algorithm used. However, if the

Table 3 :

3Model M1 with n = 200 and p tot = 1000. Median over 10 simulation runs of BIC, CPU times and number of EM-iterations for the BCD-GEM with and without active set strategy (the latter in brackets).λ 
3.0 
13.8 
24.6 
35.4 
46.2 
57.0 
67.8 
78.6 
BIC 
560 (628) 
536 (530) 
516 (522) 
532 (525) 
541 (540) 
561 (580) 
592 (591) 
611 (613) 
CPU [s] 
22.40 (29.98) 1.35 (3.28) 0.89 (3.57) 0.86 (3.34) 0.78 (3.87) 0.69 (2.42) 0.37 (2.56) 0.85 (4.05) 
# EM-iter. 3389 (2078) 
345 (239) 
287 (266) 
298 (247) 
296 (290) 
248 (184) 
129 (192) 
313 (302)

Table 4 :

4Riboflavin data with k = 3, n = 146 and p tot = 100. BIC, CPU times and number of EM-iterations for the BCD-GEM with and without active set strategy (the latter in brackets).
Using a Taylor expansion,. 
⊔ 
⊓ 
B Proofs for Section 5
B.1 Proof of Lemma 1
Acknowledgements N.S. acknowledges financial support from Novartis International AG, Basel, Switzerland.
Equation (33) and (34)), we have ν pen (θ 0 ) ≥ ν pen (θ 1 ) ≥ . . . ≥ ν pen (θ m ) ≥ ν pen. Additionally, from the properties of GEM. θ m+1Additionally, from the properties of GEM (Equation (33) and (34)), we have ν pen (θ 0 ) ≥ ν pen (θ 1 ) ≥ . . . ≥ ν pen (θ m ) ≥ ν pen (θ m+1 ).
Nonlinear programming. D Bertsekas, Athena ScientificBelmont, MABertsekas, D. (1995) Nonlinear programming. Belmont, MA: Athena Scientific.
Simultaneous analysis of Lasso and Dantzig selector. P Bickel, Y Ritov, A Tsybakov, Annals of Statistics. 37Bickel, P., Ritov, Y. and Tsybakov, A. (2009) Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37, 1705-1732.
Sparsity oracle inequalities for the Lasso. F Bunea, A Tsybakov, M Wegkamp, Electronic Journal of Statistics. 1Bunea, F., Tsybakov, A. and Wegkamp, M. (2007) Sparsity oracle inequalities for the Lasso. Electronic Journal of Statistics, 1, 169-194.
Stable recovery of sparse signals and an oracle inequality. T Cai, L Wang, G Xu, IEEE Transactions on Information Theory. to appearCai, T., Wang, L. and Xu, G. (2009a) Stable recovery of sparse signals and an oracle inequality. IEEE Transactions on Information Theory, to appear.
On recovery of sparse signals via ℓ 1 minimization. T Cai, G Xu, J Zhang, IEEE Transactions on Information Theory. 55Cai, T., Xu, G. and Zhang, J. (2009b) On recovery of sparse signals via ℓ 1 minimization. IEEE Transactions on Information Theory, 55, 3388-3397.
Near-ideal model selection by ℓ 1 minimization. E Candès, Y Plan, Annals of Statistics. 37Candès, E. and Plan, Y. (2009) Near-ideal model selection by ℓ 1 minimization. Annals of Statistics, 37, 2145-2177.
Decoding by linear programming. E Candès, T Tao, IEEE Transactions on Information Theory. 51Candès, E. and Tao, T. (2005) Decoding by linear programming. IEEE Transactions on Information Theory, 51, 4203-4215.
The Dantzig selector: statistical estimation when p is much larger than n (with discussion). E Candès, T Tao, Annals of Statistics. 35Candès, E. and Tao, T. (2007) The Dantzig selector: statistical estimation when p is much larger than n (with discussion). Annals of Statistics, 35, 2313-2404.
Maximum likelihood from incomplete data via the EM algorithm. A Dempster, N Laird, D Rubin, Journal of the Royal Statistical Society, Series B. 39Dempster, A., Laird, N. and Rubin, D. (1977) Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39, 1-38.
Variable selection via nonconcave penalized likelihood and its oracle properties. J Fan, R Li, Journal of the American Statistical Association. 96Fan, J. and Li, R. (2001) Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96, 1348-1360.
Pathwise coordinate optimization. J Friedman, T Hastie, H Hoefling, R Tibshirani, Annals of Applied Statistics. 1Friedman, J., Hastie, T., Hoefling, H. and Tibshirani, R. (2007) Pathwise coordinate optimization. Annals of Applied Statistics, 1, 302-332.
Regularized paths for generalized linear models via coordinate descent. J Friedman, T Hastie, R Tibshirani, Journal of Statistical Software. 33Friedman, J., Hastie, T. and Tibshirani, R. (2010) Regularized paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33, 1-22.
Penalized regression: the Bridge versus the Lasso. W J Fu, Journal of Computational and Graphical Statistics. 7Fu, W. J. (1998) Penalized regression: the Bridge versus the Lasso. Journal of Computa- tional and Graphical Statistics, 7, 397-416.
Persistence in high-dimensional predictor selection and the virtue of over-parametrization. E Greenshtein, Y Ritov, Bernoulli. 10Greenshtein, E. and Ritov, Y. (2004) Persistence in high-dimensional predictor selection and the virtue of over-parametrization. Bernoulli, 10, 971-988.
Fitting finite mixtures of generalized linear regressions in R. B Grün, F Leisch, Computational Statistics & Data Analysis. 51Grün, B. and Leisch, F. (2007) Fitting finite mixtures of generalized linear regressions in R. Computational Statistics & Data Analysis, 51, 5247-5252.
FlexMix version 2: Finite mixtures with concomitant variables and varying and constant parameters. B Grün, F Leisch, Journal of Statistical Software. 28Grün, B. and Leisch, F. (2008) FlexMix version 2: Finite mixtures with concomitant variables and varying and constant parameters. Journal of Statistical Software, 28, 1-35.
Adaptive Lasso for sparse high-dimensional regression models. J Huang, S Ma, C.-H Zhang, Statista Sinica. 18Huang, J., Ma, S. and Zhang, C.-H. (2008) Adaptive Lasso for sparse high-dimensional regression models. Statista Sinica, 18, 1603-1618.
Variable selection in finite mixture of regression models. A Khalili, J Chen, Journal of the American Statistical Association. 102Khalili, A. and Chen, J. (2007) Variable selection in finite mixture of regression models. Journal of the American Statistical Association, 102, 1025-1038.
The Dantzig selector and sparsity oracle inequalities. V Koltchinskii, Bernoulli. 15Koltchinskii, V. (2009) The Dantzig selector and sparsity oracle inequalities. Bernoulli, 15, 799-828.
Theory of Point Estimation. E Lehmann, Wadsworth and Brooks/ColePacific Grove, CALehmann, E. (1983) Theory of Point Estimation. Pacific Grove, CA: Wadsworth and Brooks/Cole.
FlexMix: A general framework for finite mixture models and latent class regression in R. F Leisch, Journal of Statistical Software. 11Leisch, F. (2004) FlexMix: A general framework for finite mixture models and latent class regression in R. Journal of Statistical Software, 11, 1-18.
Finite mixture models. G J Mclachlan, D Peel, WileyNew YorkMcLachlan, G. J. and Peel, D. (2000) Finite mixture models. Wiley, New York.
The group Lasso for logistic regression. L Meier, S Van De Geer, P Bühlmann, Journal of the Royal Statistical Society, Series B. 70Meier, L., van de Geer, S. and Bühlmann, P. (2008) The group Lasso for logistic regression. Journal of the Royal Statistical Society, Series B, 70, 53-71.
High dimensional graphs and variable selection with the Lasso. N Meinshausen, P Bühlmann, Annals of Statistics. 34Meinshausen, N. and Bühlmann, P. (2006) High dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34, 1436-1462.
Lasso-type recovery of sparse representations for highdimensional data. N Meinshausen, B Yu, Annals of Statistics. 37Meinshausen, N. and Yu, B. (2009) Lasso-type recovery of sparse representations for high- dimensional data. Annals of Statistics, 37, 246-270.
Penalized model-based clustering with application to variable selection. W Pan, X Shen, Journal of Machine Learning Research. 8Pan, W. and Shen, X. (2007) Penalized model-based clustering with application to variable selection. Journal of Machine Learning Research, 8, 1145-1164.
The Bayesian Lasso. T Park, G Casella, Journal of the American Statistical Association. 103Park, T. and Casella, G. (2008) The Bayesian Lasso. Journal of the American Statistical Association, 103, 681-686.
Regression shrinkage and selection via the Lasso. R Tibshirani, Journal of the Royal Statistical Society, Series B. 58Tibshirani, R. (1996) Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58, 267-288.
Convergence of a block coordinate descent method for nondifferentiable minimization. P Tseng, Journal of Optimization Theory and Applications. 109Tseng, P. (2001) Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109, 475-494.
A coordinate gradient descent method for nonsmooth separable minimization. P Tseng, S Yun, Mathematical Programming, Series B. 117Tseng, P. and Yun, S. (2008) A coordinate gradient descent method for nonsmooth sepa- rable minimization. Mathematical Programming, Series B, 117, 387-423.
Optimal aggregation of classifiers in statistical learning. A Tsybakov, Annals of Statistics. 32Tsybakov, A. (2004) Optimal aggregation of classifiers in statistical learning. Annals of Statistics, 32, 135-166.
High-dimensional generalized linear models and the Lasso. S Van De Geer, S Van De Geer, Annals of Statistics. 36Cambridge University PressEmpirical Processes in M-Estimationvan de Geer, S. (2000) Empirical Processes in M-Estimation. Cambridge University Press. van de Geer, S. (2008) High-dimensional generalized linear models and the Lasso. Annals of Statistics, 36, 614-645.
On the conditions used to prove oracle results for the Lasso. S Van De Geer, P Bühlmann, Electronic Journal of Statistics. 3van de Geer, S. and Bühlmann, P. (2009) On the conditions used to prove oracle results for the Lasso. Electronic Journal of Statistics, 3, 1360-1392.
Prediction and variable selection with the Adaptive Lasso. S Van De Geer, S Zhou, P Bühlmann, arXiv:1001.5176v2van de Geer, S., Zhou, S. and Bühlmann, P. (2010) Prediction and variable selection with the Adaptive Lasso. arXiv:1001.5176v2.
Asymptotic Statistics. A Van Der Vaart, A Van Der Vaart, J Wellner, Weak Convergence and Empirical Processes. Springer-Verlagvan der Vaart, A. (2007) Asymptotic Statistics. Cambridge University Press. van der Vaart, A. and Wellner, J. (1996) Weak Convergence and Empirical Processes. Springer-Verlag.
Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ 1 -constrained quadratic programming (Lasso). M Wainwright, IEEE Transactions on Information Theory. 55Wainwright, M. (2009) Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ 1 -constrained quadratic programming (Lasso). IEEE Transactions on Informa- tion Theory, 55, 2183-2202.
On the convergence properties of the EM algorithm. C Wu, Annals of Statistics. 11Wu, C. (1983) On the convergence properties of the EM algorithm. Annals of Statistics, 11, 95-103.
Nearly unbiased variable selection under minimax concave penalty. C.-H Zhang, Annals of Statistics. 38Zhang, C.-H. (2010) Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38, 894-942.
The sparsity and bias of the Lasso selection in highdimensional linear regression. C.-H Zhang, J Huang, Annals of Statistics. 36Zhang, C.-H. and Huang, J. (2008) The sparsity and bias of the Lasso selection in high- dimensional linear regression. Annals of Statistics, 36, 1567-1594.
Some sharp performance bounds for least squares regression with L1 regularization. T Zhang, Annals of Statistics. 37Zhang, T. (2009) Some sharp performance bounds for least squares regression with L1 regularization. Annals of Statistics, 37, 2109-2144.
On model selection consistency of Lasso. P Zhao, B Yu, Journal of Machine Learning Research. 7Zhao, P. and Yu, B. (2006) On model selection consistency of Lasso. Journal of Machine Learning Research, 7, 2541-2563.
The adaptive Lasso and its oracle properties. H Zou, Journal of the American Statistical Association. 101Zou, H. (2006) The adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101, 1418-1429.
