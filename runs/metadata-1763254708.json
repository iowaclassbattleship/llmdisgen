[
  {
    "corpus_id": "10009609",
    "title": "Effectiveness of biomarker-based exclusion of ventilator-acquired pneumonia to reduce antibiotic use (VAPrapid-2): study protocol for a randomised controlled trial",
    "externalids": {
      "ACL": "",
      "ArXiv": "",
      "CorpusId": "10037358",
      "DBLP": "",
      "DOI": "10.1186/s13063-016-1442-x",
      "MAG": "2473068687",
      "PubMed": "27422026",
      "PubMedCentral": "4947254"
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "X",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "X",
            "paragraphs": [
              "It is increasingly being recognised that the quantitative evaluation of biomedical research cannot depend only on the counting of citations in the serial literature. They may measure academic influence, but the funders of such research are usually more concerned to see if it has had a practical benefit, especially to patients. One of the ways in which research can influence practice is through its contribution to the evidence base supporting clinical guidelines {{12990900}}{{not_in_s2orc}}{{not_in_s2orc}}{{4992451}}{{42067051}}. These are increasingly being used across many countries in the routine clinical care of cancer patients. Most of them are published by national professional medical associations (e.g., {{6130785}}{{45033792}}{{30394593}}, but some are developed by governmental bodies (e.g., {{20079372}}.",
              "It is normal for such guidelines to have lists of references that comprise their evidence base. However, the quality of the evidence is sometimes doubtful {{3065283}}{{19332737}}{{21492156}}, and schemes have been devised to grade the quality of the clinical trials, which form a large part of the evidence base (e.g., {{22091521}}{{39213437}}{{not_in_s2orc}}{{8284450}}{{22721728}}. Even when the guidelines have been published, they are sometimes criticised as inadequate {{not_in_s2orc}}{{not_in_s2orc}}{{40677359}}, insufficient {{38413898}} or they may become outdated {{23806012}}. There is also the question of whether the guidelines will actually be followed in clinical practice {{42615096}}{{9589309}}{{25848593}}{{1814188}}. The breadth of oncology practice (both patients and treatment modalities), the rapid evolution of new treatments and the often diverse interpretation of 'evidence' by health-care professionals mean many patients are treated with hospitalspecific protocols rather than national guidelines. This situation is particularly acute in certain site-specific cancers, for example, lung {{15135204}}.",
              "A further cause of disagreement is the question of cost: a new drug may be clinically effective and better than existing drugs or a placebo, but so costly that an equivalent or greater health gain may be achievable by other means, for example, better screening to detect the disease at an early stage. This can cause considerable dissension and lead to lawsuits to make the drug available for particularly articulate patients {{39207487}}, or from companies and patients' advocacy groups, which sometimes receive their subsidies {{26204397}}. Lobbying of the UK National Institute for Health and Clinical Excellence (NICE) by pharmaceutical firms is now rife {{27084170}}, and a US politician has adopted bully-boy tactics in his efforts to subvert evidence-based medicine {{72875227}}. The cost basis of NICE's recommendations has also been criticised: the figure of d30 000 (h40 000, $60 000) per quality-adjusted life year appears not to have a scientific basis or to take account of the social costs of disease {{47554094}}.",
              "Despite all these criticisms, clinical guidelines are nevertheless gaining increasing recognition as the way forward. It does, therefore, seem worthwhile to treat them as an outcome indicator, even though a partial one, of the clinical impact of the research they cite. Several studies have analysed the evidence base of selected clinical guidelines {{154559501}}{{10747489}}{{not_in_s2orc}}. They have established that the papers cited are very clinical (when positioned on a scale from clinical observation to basic research); that the UK guidelines overcite the UK research papers; and that the cited papers are quite recent, with a temporal distribution comparable to that of the papers cited on biomedical research papers. Research from other European countries seems to be cited about as much as would be expected on the UK clinical guidelines, but that from Japan and from most developing countries is almost totally ignored.",
              "In this study, we examined three sets of the UK guidelines on a single subject, cancer, and the references on 43 different guidelines, almost all concerned with treatment rather than with prevention. The bibliographic details of the references were assembled in a file and compared with those of cancer research publications in the three peak years {{not_in_s2orc}}. The objective was to answer several policy-related questions:",
              "how do countries' relative presences among the cited references compare with their presences in cancer research? how many of the cited references are actually classifiable as cancer research? what is the research level (RL) distribution of these cited references compared with that of cancer research papers? are the cited references published in journals of high citation impact? how does the funding of the cited papers compare with that of cancer research overall?",
              "The latter two questions need to take account of the finding that the references on clinical guidelines are much more clinical than other biomedical research."
            ]
          }
        ]
      },
      {
        "header": "MATERIALS AND METHODS",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "MATERIALS AND METHODS",
            "paragraphs": []
          },
          {
            "header": "UK cancer guidelines and the analysis of their references",
            "paragraphs": [
              "There are three sets of clinical guidelines commonly used in the United Kingdom:",
              "Published by the British Medical Association in Clinical Evidence. This takes the form of a book that is revised and extended every 6 months, but is also accessible on the Web (to people in the United Kingdom); Developed by the National Institute for Health and Clinical Excellence (NICE) for the National Health Service (NHS) in England and Wales, based on Health Technology Assessments (HTAs). Most of these last are available on the Web, but not all (although it is intended by NICE that they should be). They were used in the present study, because the references on the actual guidelines were usually not visible; Developed by the Scottish Intercollegiate Guidelines Network (SIGN) for use by the NHS in Scotland. All these are freely available on the Web Only a minority of these guidelines and HTAs are applicable to cancer. The numbers are, respectively, 15, 18 and 10. Each of these 43 documents has a set of references, most of which are articles in peer-reviewed journals. A total of 3217 references were found and their details downloaded to file. Their addresses were parsed by means of a special macro so that the integer and fractional counts of each country were listed for each paper (a paper with two addresses in the United Kingdom and one in France would count unity for each on an integer count basis, but 0.67 for the United Kingdom and 0.33 for France using fractional counting). The RL of each paper was determined using the new system developed by {{7942963}}, in which each journal is assigned an RL based on the presence of 'clinical' and 'basic' words in the titles of papers it has published on a scale from 1 \u00bc clinical to 4 \u00bc basic. In addition, the RL of groups of individual cited papers could be calculated with reference to their individual titles, and the presence of 'clinical' or 'basic' words within them. The potential citation impact (PCI) of each cited paper was also determined with reference to a file of Journal Expected Citation Rates provided by Thomson Scientific (London, UK). This gave the mean number of citations for papers published in a journal in a given year and cited in the year of publication and the 4 subsequent years.",
              "Funding data for virtually all the UK papers (790 out of 796) were obtained from inspection of the acknowledgements to their funding sources in the British Library. Many of the papers had previously been looked up for the Research Outputs Database {{not_in_s2orc}} or for other projects, and only 151 needed to be sought anew. The main comparator used to normalise the results of the analysis of the cited references was a file of world oncology research papers {{27557788}}. For the years 1999 -2001, there were over 100 000 such papers, and their characteristics were used to see how the cited references compared with them, with due account being taken of the differences expected in mean RLs (the cited references being more clinical than oncology papers overall). Of the references classed as 'articles' or 'reviews', 88% were within the subfield of oncology as defined by Cancer Research UK {{27557788}}. This percentage remained sensibly constant over the period, 1994 -2004. However, the references were in much more clinical journals than world oncology papers for the year 2000, the peak year for the numbers of references, see Figure 2. This result was obtained earlier {{10747489}}{{not_in_s2orc}} but with a much simplified (and less accurate) method of categorisation of journals by RL. Of the 3217 papers, 2747 titles (86%) had either a 'clinical' or a 'basic' keyword, and the mean RL was 1.07, which is very close to the lower end of the scale (RL \u00bc 1.0), and much below the mean RL based on all the papers in the individual journals (RL \u00bc 1.43). This shows that the references were being published in journals that were relatively more basic than the papers themselves, and reinforces the message that the papers were, therefore, almost entirely clinical observation."
            ]
          }
        ]
      },
      {
        "header": "RESULTS",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "RESULTS",
            "paragraphs": []
          },
          {
            "header": "Time and research level distributions",
            "paragraphs": []
          },
          {
            "header": "Geographical analysis",
            "paragraphs": [
              "The presence of 20 leading countries in oncology research for 2000 and in the references from the clinical guidelines is shown in Table 1, where the data have been shown on a fractional count basis. Figure 3 presents the ratio between a country's presence in the guideline references and its presence in oncology research, that is, the values shown in the last column of Table 1. As would be expected, the UK oncology research is cited more than expected from its presence in world oncology by a factor of almost 3, but several other European countries' work is also relatively overcited, notably that of Denmark, Ireland and Sweden. Although Italy, which is strong in clinical trials, shows to advantage, Germany is relatively much undercited compared with its presence in cancer research in recent years. Japanese work is almost ignored, but it is likely that the Science Citation Index, where most of the references were found, does not cover Japanese clinical journals. This, however, is only a small part of the reason for the paucity of Japanese references.",
              "Within the United Kingdom, certain cities showed relatively to advantage in terms of their percentage presence within the fractional UK total of 605 papers cited by the guidelines, compared with that in the 2332 UK oncology papers published in 2000. The analysis is conveniently carried out on the basis of postcode area, the first one or two letters of the UK postcode system, for example, B \u00bc Birmingham, CB \u00bc Cambridge. Figure 4 shows a scatter plot for the 26 leading areas (out of 124), accounting for about two-thirds of both totals. The spots above the diagonal line represent areas that are more frequently cited than expected, and vice versa. Among the former, EH \u00bc Edinburgh and G \u00bc Glasgow are prominent, in part because the SIGN guidelines overcite Scottish research papers, together with SM \u00bc Sutton and Cheam (the location of the Institute of Cancer Research) and OX \u00bc Oxford.",
              "Table1 and Figure 3 show overall values, but an analysis can also be made of subsets of papers for groups of 2 or 3 years, chosen so that the four periods each have about 20% of the total cited references, see Table 2. For nearly all the countries, there are close similarities between the time trends, which suggest that the guidelines are rather consistent in the geography of their citing behaviour. Thus, Australia, Canada, Sweden, the United Kingdom  Figure 3.  Table 1. and the United States have all shown a reducing presence in oncology research, and a reducing presence in the guideline references; Germany, on the other hand, has increased its presence in both (but is still much undercited). France and Japan increased their presence in both sets of papers, but it went down slightly during the latest period."
            ]
          },
          {
            "header": "Journal citation impact scores",
            "paragraphs": [
              "The references cited tend to be published in high-impact journals. Table 3 shows that in each RL grouping, the guideline references are published in journals with a higher mean citation score (the PCI, of the papers) than world oncology papers from the year 2000.",
              "The overall mean is higher, too, at 19.9 cites in 5 years compared with 13.4. The 'superior performance' of the guideline references occurs because a large number of them are published in the highimpact general journals, The Lancet (138 of them), New England Journal of Medicine (133), British Medical Journal (78) and Journal of the American Medical Association (50)."
            ]
          },
          {
            "header": "The funding of the UK cited references",
            "paragraphs": [
              "Of the 796 UK papers, all but 6 were found and inspected to determine their funding sources. These were taken both from the addresses (as for some organisations this is an indication of funding) and from the formal acknowledgements. For the purposes of this analysis, funding sources were grouped into five main sectors:",
              "UK government, both departments and agencies; UK private nonprofit, including collecting charities, endowed foundations, hospital trustees, mixed (academic) and other nonprofit. A subset of this sector is Cancer Research UK, and its two predecessors, the Cancer Research Campaign and the Imperial Cancer Research Fund; pharmaceutical industry, both domestic and foreign (it is often difficult to distinguish as some subsidiaries have considerable autonomy in the use of research funds), and including biotech companies; nonpharma industry; no funding acknowledged.",
              "The remaining funding organisations are foreign governmental and private nonprofit sources, and international organisations,    {{not_in_s2orc}}   Impact of cancer research G Lewison and R Sullivan such as the European Commission (EC) and the World Health Organization (WHO). The funding sources vary with the RL of the papers: the more clinical papers have fewer sources and the more basic papers have more. Table 4 shows the analysis for the UK papers in oncology in {{not_in_s2orc}} Table 5 shows the results for the UK papers cited on cancer clinical guidelines. For each RL group, an estimate has been made of the funding that would have been expected had they been typical of the UK cancer research, and in the last row there are given the ratios of observed-to-expected numbers of papers (integer counts) on the assumption that the cancer clinical guideline citations are typical of oncology, but with due allowance for the different RL distributions.",
              "For example, the UK oncology papers in the first group (RL from 1.0 to 1.5) have the UK government funding on 11.1% of them, so it might be expected that there would be 0.111 \u00c2 544 \u00bc 60.4 government-funded papers among the corre-sponding group cited on cancer clinical guidelines. In fact, there were 149 such papers, showing that many more are government funded than might have been expected. When the totals for each of the six groups are added, it can be seen that the observed number of the UK government-funded papers is almost twice the predicted number. The observed total is still higher ( \u00c2 2.5) for the pharma industry-funded papers, and a little lower for Cancer Research UK papers ( \u00c2 1.8), for nonpharma industry papers ( \u00c2 1.6) and the UK private nonprofit papers ( \u00c2 1.3). Not surprisingly, there are many fewer 'unfunded' papers, the ratio of observed-to-expected numbers of papers being only just over half."
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "DISCUSSION",
      "papers_cited_discussion": [
        "9662045"
      ],
      "subsections": [
        {
          "header": "DISCUSSION",
          "paragraphs": [
            "The UK cancer clinical guidelines are sufficient in number and variety to provide a fair window on the impact of cancer research on clinical practice, not only in the United Kingdom, but in other leading countries, particularly in western Europe. We have seen that almost all the references (88%) are to papers that are within the subfield of cancer research. Because about one-third of the research supported by Cancer Research UK, in common with that of other medical research charities working in a particular disease area, is out with this subfield (most of this would comprise basic biology), it follows that little of this work can be expected to influence clinical guidelines -hardly a surprising conclusion, but nevertheless one that is worth stating.",
            "Many of the guideline references are to papers in the US and the UK general medical journals -The Journal of the American Medical Association, New England Journal Medical, British Medical Journal and The Lancet. This is one reason, but by no means the only one, for the guideline references as a whole to be in high impact, and therefore well known, journals. It appears that if researchers want their work, particularly clinical trials, to be part of the evidence base for clinical guidelines, then it is desirable for them to publish in highly cited journals. Disproportionately, many of these papers will have been funded by government or the pharmaceutical industry, with charities also playing an enhanced role compared with cancer research overall. This highlights one pitfall of national guidelines in the context of research impact assessments; many important, high quality clinical trials -either because they are early phase or negative -will not make it into guidelines. The impact of research on national clinical guidelines is just one parameter that can describe the utility of health research {{9662045}} Table 4."
          ]
        },
        {
          "header": "Impact of cancer research G Lewison and R Sullivan",
          "paragraphs": [
            "When account is taken of the clinical nature of the work cited on guidelines, the big increase in the percentage of the papers that acknowledge funding -whether from government, charities or industry -is striking (Table 5). Many (37%) of these clinical papers with RLs greater than 1.5 are reports of clinical trials, and 85% of the latter acknowledge funding compared with 71% of the others. Cancer Research UK plays the biggest role, and supports over one-third of these trials, more even than the pharmaceutical industry as a whole, or the UK government.",
            "The geographical analysis of the cited papers reveals that the UK papers have a threefold higher presence among them than in world cancer research. In part, this reflects the differences in cancer management between countries. Such overcitation also occurs on other scientific papers, so it is hardly surprising that it was found here. It might be expected that the UK guidelines, which aim to show which treatments are cost-effective, would reflect in particular the different financial basis of health-care provision in this country compared with that elsewhere, and so papers concerned with economics and costs would be even more overcited if they were from the United Kingdom. In fact, this does occur, but to a very minor extent (22% from the United Kingdom compared with 19% overall; the difference not being significant).",
            "The distribution of the cited papers within the United Kingdom differs from what might have been expected based purely on overall numbers and on the extent to which the cities carry out clinical observation rather than basic research. The simple comparison of Figure 4 needs also to take account of the mean RL of papers from each area, and, when this is done ( Figure 5), a different pattern emerges, with EH \u00bc Edinburgh, OX \u00bc Oxford and CB \u00bc Cambridge forming an axis of excellence (on this indicator) and other areas' output being less cited on guidelines. The distance of the spots from this axis gives one indicator of the performance of the different centres, an imperfect one to be sure, as there will be other confounding factors not considered here, but nevertheless a useful complement to the traditional bibliometric criterion based purely on citation counts in the scientific literature.",
            "There are in the database enough cited papers from a few other countries to enable a similar evaluation to be carried out for them. However, these data are inevitably skewed by being viewed through the prism of the UK clinical recommendations. It would be highly desirable to complement them with the results of similar exercises carried out in other countries with extensive sets of clinical guidelines, or at a European or international level. Then, provided the data were collected in exactly the same way, they could be pooled and a more international perspective on the utility of cancer research would emerge that research evaluators could employ. Such an activity could appropriately be coordinated by the European Cancer Managers' Research Forum, with all data contributors having also the right to gain access to the data provided by workers in other countries."
          ]
        },
        {
          "header": "Figure 1",
          "paragraphs": []
        },
        {
          "header": "Figure 3",
          "paragraphs": []
        },
        {
          "header": "Figure 2",
          "paragraphs": []
        },
        {
          "header": "Figure 4",
          "paragraphs": []
        },
        {
          "header": "Figure 5",
          "paragraphs": []
        },
        {
          "header": "Table 1",
          "paragraphs": []
        },
        {
          "header": "Table 2",
          "paragraphs": []
        },
        {
          "header": "Table 3 Mean",
          "paragraphs": []
        },
        {
          "header": "Table 5",
          "paragraphs": []
        }
      ]
    },
    "discussion_txt": "The UK cancer clinical guidelines are sufficient in number and variety to provide a fair window on the impact of cancer research on clinical practice, not only in the United Kingdom, but in other leading countries, particularly in western Europe. We have seen that almost all the references (88%) are to papers that are within the subfield of cancer research. Because about one-third of the research supported by Cancer Research UK, in common with that of other medical research charities working in a particular disease area, is out with this subfield (most of this would comprise basic biology), it follows that little of this work can be expected to influence clinical guidelines -hardly a surprising conclusion, but nevertheless one that is worth stating.Many of the guideline references are to papers in the US and the UK general medical journals -The Journal of the American Medical Association, New England Journal Medical, British Medical Journal and The Lancet. This is one reason, but by no means the only one, for the guideline references as a whole to be in high impact, and therefore well known, journals. It appears that if researchers want their work, particularly clinical trials, to be part of the evidence base for clinical guidelines, then it is desirable for them to publish in highly cited journals. Disproportionately, many of these papers will have been funded by government or the pharmaceutical industry, with charities also playing an enhanced role compared with cancer research overall. This highlights one pitfall of national guidelines in the context of research impact assessments; many important, high quality clinical trials -either because they are early phase or negative -will not make it into guidelines. The impact of research on national clinical guidelines is just one parameter that can describe the utility of health research {{9662045}} Table 4.When account is taken of the clinical nature of the work cited on guidelines, the big increase in the percentage of the papers that acknowledge funding -whether from government, charities or industry -is striking (Table 5). Many (37%) of these clinical papers with RLs greater than 1.5 are reports of clinical trials, and 85% of the latter acknowledge funding compared with 71% of the others. Cancer Research UK plays the biggest role, and supports over one-third of these trials, more even than the pharmaceutical industry as a whole, or the UK government.The geographical analysis of the cited papers reveals that the UK papers have a threefold higher presence among them than in world cancer research. In part, this reflects the differences in cancer management between countries. Such overcitation also occurs on other scientific papers, so it is hardly surprising that it was found here. It might be expected that the UK guidelines, which aim to show which treatments are cost-effective, would reflect in particular the different financial basis of health-care provision in this country compared with that elsewhere, and so papers concerned with economics and costs would be even more overcited if they were from the United Kingdom. In fact, this does occur, but to a very minor extent (22% from the United Kingdom compared with 19% overall; the difference not being significant).The distribution of the cited papers within the United Kingdom differs from what might have been expected based purely on overall numbers and on the extent to which the cities carry out clinical observation rather than basic research. The simple comparison of Figure 4 needs also to take account of the mean RL of papers from each area, and, when this is done ( Figure 5), a different pattern emerges, with EH \u00bc Edinburgh, OX \u00bc Oxford and CB \u00bc Cambridge forming an axis of excellence (on this indicator) and other areas' output being less cited on guidelines. The distance of the spots from this axis gives one indicator of the performance of the different centres, an imperfect one to be sure, as there will be other confounding factors not considered here, but nevertheless a useful complement to the traditional bibliometric criterion based purely on citation counts in the scientific literature.There are in the database enough cited papers from a few other countries to enable a similar evaluation to be carried out for them. However, these data are inevitably skewed by being viewed through the prism of the UK clinical recommendations. It would be highly desirable to complement them with the results of similar exercises carried out in other countries with extensive sets of clinical guidelines, or at a European or international level. Then, provided the data were collected in exactly the same way, they could be pooled and a more international perspective on the utility of cancer research would emerge that research evaluators could employ. Such an activity could appropriately be coordinated by the European Cancer Managers' Research Forum, with all data contributors having also the right to gain access to the data provided by workers in other countries.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "As a scientific research assistant, I must first commend the authors for their thorough examination of the impact of biomedical research impact assessment on clinical guidelines, particularly in the context of cancer research. Their use of multiple methods to study this topic is indeed a significant contribution to the field.",
          "",
          "The authors have identified 43 UK guidelines and associated Health Technology Assessments published up to October 2006, which they have analyzed to determine their geographical provenance and type of research. They found that the UK papers were cited nearly three times as frequently as would have been expected from their presence in world oncology research, with Edinburgh and Glasgow standing out for their unexpectedly high contributions to the guidelines' scientific base.",
          "",
          "One of the key findings of the study is the significant difference in funding sources acknowledged by the cited papers from the United Kingdom compared to the UK cancer research papers at the same research level. This highlights the importance of transparency in funding sources when evaluating the impact of research on clinical guidelines.",
          "",
          "The authors also note that their study only covers a specific time period, and it would be interesting to see how the landscape of biomedical research impact assessment has evolved in recent years. Future studies could build upon this work by expanding the time frame and examining the impact of different types of research on clinical guidelines.",
          "",
          "In conclusion, the study provides valuable insights into the impact of biomedical research impact assessment on clinical guidelines in cancer research. The findings highlight the importance of transparency in funding sources and the need for further research to understand the evolving landscape of biomedical research impact assessment."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.561042845249176,
            "R": 0.628397524356842,
            "F1": 0.5928131341934204
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.4955703616142273,
            "R": 0.566767692565918,
            "F1": 0.5287832021713257
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          ""
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "]",
          "",
          "Discussion:",
          "",
          "*   The study found that UK research papers were cited nearly three times as frequently as"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.3398705720901489,
            "R": 0.549963116645813,
            "F1": 0.42011508345603943
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.31633278727531433,
            "R": 0.4891422688961029,
            "F1": 0.38419994711875916
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "100153030",
    "title": "Revisiting the O(3) Non\u2010linear Sigma Model and Its Pohlmeyer Reduction",
    "externalids": {
      "ACL": "",
      "ArXiv": "1612.03840",
      "CorpusId": "100153030",
      "DBLP": "",
      "DOI": "10.1002/prop.201700067",
      "MAG": "2950529231",
      "PubMed": "",
      "PubMedCentral": ""
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "Introduction",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Introduction",
            "paragraphs": [
              "Meron solutions of two-dimensional non-linear sigma models {{not_in_s2orc}} are interesting singular solutions that are characterized by topological charge equal to 1/2 and logarithmically divergent action. Instanton solutions in this class of models can be understood as being composed by two merons. These have been studied especially in the context of the relation of two-dimensional sigma models to four-dimensional Yang-Mills theories {{121804459}}, (for a review on the subject see {{123579404}} and the references therein), which are also characterized by such solutions which dominate the path integral at strong coupling providing a qualitative picture for the confined phase of these theories {{10443315}}.",
              "Elliptic solutions that interpolate between instanton and meron solutions have also been discovered {{not_in_s2orc}}. These solutions can be understood as the intermediate stages of the dissociation of meron pairs, supporting the relation between meron pairs and instantons.",
              "There is an alternative approach to study non-linear sigma models in symmetric spaces, the so called Pohlmeyer reduction {{123165375}}{{123761193}}{{119534683}}{{not_in_s2orc}}{{not_in_s2orc}}{{123345148}}{{not_in_s2orc}}. The critical element of this approach is a non-local coordinate transformation that manifestly satisfies the Virasoro constraints, thus leaving only the physical degrees of freedom. In the case of the O (3) model, this method leads to the relation of the sigma model to the sine-Gordon equation {{123165375}}, while for higher dimensional symmetric spaces one results in multi-component generalizations of the latter.",
              "Whether the reduced theory can be obtained from a local action is a non-trivial question in this approach, since the degrees of freedom of the reduced theory are connected to the original ones in a non-local way. It turns out that the reduced theory can be considered as a gauged WZW model with an integrable potential in a specific gauge {{119380056}}{{18830993}}{{18368480}}, allowing a systematic method to find the Lagrangian of the reduced theory.",
              "The reduced theory has an integrable structure, allowing the construction of interesting solitonic solutions. However, the construction of the corresponding solution in the original sigma model is non-trivial due to the non-trivial relation between the original and reduced degrees of freedom. Such constructions have been of particular interest in the context of AdS/CFT correspondence in which case the non-linear sigma model describes the dynamics of strings propagating in AdS spaces {{2450039}}{{118514287}}{{2284875}}{{16004701}}{{not_in_s2orc}}.",
              "Since several aspects of the relation between the sigma model and the reduced integrable theory have not been understood yet, mapping the solutions of the reduced theories to instanton, meron and elliptic solutions or the original sigma model may provide interesting insights to the properties of Pohlmeyer reduction.",
              "The structure of this paper is as following: In section 2 we review basic facts of the O (3) non-linear sigma model. In section 3 we review the instanton, meron and elliptic solutions of the model. In section 4 we study the Pohlmeyer reduction of the Euclidean O (3) model resulting in the sinh-Gordon equation. In section 5, we study the mapping between the solutions of the sigma model in the original formulation, as reviewed in section 2, and solutions of the sinh-Gordon equation. Finally, in section 6 we discuss our results. There is also en appendix with properties of Jacobi elliptic functions that are used throughout the text."
            ]
          },
          {
            "header": "The Euclidean O(3) Non-linear Sigma Model",
            "paragraphs": []
          },
          {
            "header": "The Model",
            "paragraphs": [
              "The Euclidean O(3) non-linear sigma model describes the dynamics of a three component vector field X i (\u03c3 0 , \u03c3 1 ) with unit norm. Its action is given by",
              "Index i takes the values 1, 2, 3, while index m takes the values 0, 1 and is contracted with the Euclidean metric \u03b4 mn . We first introduce the complex coordinate z,",
              "Then, the action of the Euclidean O(3) model is written as",
              "The constraint can be easily satisfied introducing angular field variables \u0398 and \u03a6 as",
              "Then, the action is written as",
              "Finally, we can define the \"stereographic projection\" complex field variable W as,",
              "and the action is written as",
              "(2.9)",
              "Any field configuration describes a mapping from the complex plane to the sphere S 2 . Any finite action configuration must have a unique limit as the magnitude of the complex number z tends to infinity. In such cases, the complex plane can be compactified to the sphere S 2 and the finite action field configurations are mappings from S 2 to S 2 , thus, they are classified by the homotopy classes of these mappings, which are trivially the elements of the group of integers Z. One can define the topological number density as the Jacobian of the mapping",
              "(2.10)"
            ]
          },
          {
            "header": "The Equations of Motion and the Stress-energy Tensor",
            "paragraphs": [
              "It will turn out that all the formulations of the Euclidean O(3) non-linear sigma model presented in section 2.1, given by equations (2.3), (2.7) and (2.9) will be useful for different points of view of the model solutions. For this reason we will derive the equations of motion and the stress-energy tensor in all these formulations."
            ]
          },
          {
            "header": "Scalar Fields Triplet Formulation",
            "paragraphs": [
              "Starting from action (2.3), the equations of motion for the fields X m are",
              "while the equation of motion for the Lagrange multiplier \u03bb is the constraint equation",
              "Acting with \u2202\u2202 on the constraint equation we get",
              "The latter, using both equations of motion for X i and the constraint, can be written as",
              "implying that we can write down the equations of motion for the fields X i decoupled from the Lagrange multiplier as",
              "We can also calculate the stress-energy tensor and find it equal to",
              "The stress-energy tensor in the angular field variables formulation equals"
            ]
          },
          {
            "header": "Stereographic Complex Field Formulation",
            "paragraphs": [
              "The equations of motion derived from action (2.9) are",
              "The stress-energy tensor equals",
              "As expected, T zz vanishes identically as a result of conformal symmetry in all three formulations."
            ]
          },
          {
            "header": "Solutions of the O(3) Non-linear Sigma Model",
            "paragraphs": [
              "The O(3) non-linear Sigma Model is known to have several interesting solutions. We will distinguish three classes of solutions: solutions of integer topological charge, i.e. instantons, solutions of half-integer topological charge, i.e. merons and solutions that interpolate between merons and instantons. For the analysis in this section, we will use the formulation of the sigma model in terms of the complex field variable W ."
            ]
          },
          {
            "header": "Instantons",
            "paragraphs": [
              "Clearly, any holomorphic function W or antiholomorphic functionW",
              "is a solution of the equations of motion (2.24). If one wants to restrict to solutions that are characterized by finite action, i.e. solutions that become single valued at infinity, the general solution has to be written as product of factors of the form z\u2212a \u2212 z\u2212a + , for instantons and as product of factors of the formz \u2212a \u2212 z\u2212a + , for antiinstantons {{118103020}},",
              "It is easy to show that upon substitution of (3.3) and {{not_in_s2orc}} to the expression for the topological charge (2.10), we find",
              "Studying the form of the action for the solutions (3.3) and (3.4) and having found the corresponding topological charges, it is natural to interpret these solutions as Ninstanton or N -antiinstanton solutions respectively, localized at positions 1 2 a + i + a \u2212 i . Instantons and antiinstantons are the only finite action solutions of the sigma model {{121034572}}. Further details are beyond the scope of this work.",
              "Equations (2.25) and (2.26) imply that the instanton and anti-instanton solutions are characterized by vanishing stress-energy tensor."
            ]
          },
          {
            "header": "Merons",
            "paragraphs": [
              "Meron solutions are solutions characterized by vanishing topological charge except for singular points. Since the topological charge density is simply the Jacobian of the mapping defined by the fields X i , merons correspond to degenerate solutions that map the complex plane to one-dimensional submanifolds of the sphere. Demanding the above, it turns out that the general meron solution of equations (2.24) is of the form",
              "where f (z) is an arbitrary meromorphic function of z. Furthermore, the demand that the solution is single valued at infinity results in a specific selection of function f (z),",
              "which corresponds to a 2N meron configuration, with merons localized at a \u2212 i and a + i . For more details on merons, the reader is encouraged to read {{not_in_s2orc}}.",
              "Substituting (3.9) to (2.25) and (2.26), it is simple to find that the stress-energy tensor elements corresponding to the solution (3.9) are",
              "(3.12)"
            ]
          },
          {
            "header": "Elliptic Solutions",
            "paragraphs": [
              "Although, the instanton and meron solutions are both infinite families of solutions, many more solutions have been neglected due to the demand of localized topological charge. In this section, we will abandon this demand, and following {{not_in_s2orc}}, we will find solutions interpolating between the single meron solution W = z/z and the single instanton solution W = z. To do so, we substitute in equations (2.24) the ansatz",
              "It is a matter of simple algebra to find that substituting (3.13) in equations (2.9), (2.10), (2.25) and (2.26), yields",
              "and",
              "Equation {{not_in_s2orc}} implies that the unspecified function \u03c8 (t) obeys the equation",
              "which is the one-dimensional sine-Gordon equation or in other words the equation of motion of the pendulum with time having been substituted with the spacial variable t and potential equal to V = cos \u03c8 \u2212 1. {{not_in_s2orc}} Thus, this effective one-dimensional problem has a stable equilibrium that lies at \u03c8 = \u03c0 and an unstable one at \u03c8 = 0. It has to be pointed out that this one-dimensional sine-Gordon has nothing to do with the sine-Gordon that occurs after the Pohlmeyer reduction 1 . Unlike the connection between the NLSM degrees of freedom and the Pohlmeyer reduced field, which is nonlocal, as we will see in section 4, in this special case \u03c8 is locally connected to the degrees of freedom of the NLSM through {{not_in_s2orc}}. This kind of mapping of the O(3) NLSM to the sine-Gordon equation is a property of the particular model only and it greatly facilitates the study of its elliptic solutions.",
              "Simple solutions of this effective problem can give us an idea about how the general solution of this problem interpolates between the single meron and single instanton solutions. The unstable vacuum solution \u03c8 = 0 corresponds to the trivial solution W = 0. The kink solution \u03c8 = 4tan \u22121 e t = 4tan \u22121 |z| corresponds to the one-instanton solution W = z. Finally the stable vacuum solution \u03c8 = \u03c0 corresponds to the onemeron solution W = z/z. Thus, the general solution to be found should have the limits of the stable vacuum solution and the kink solution.",
              "It is a direct consequence of the equation of motion (3.19) that",
              "In the language of the pendulum the above would be simply energy conservation. We can categorize the solutions in the following classes",
              "The integral (3.21) of the equations of motion, allows the expression of the general solution in terms of the Jacobi elliptic functions (see also {{14507301}}). Several useful definitions and properties of Jacobi elliptic functions are summarized in appendix A. If one defines \u03c8 = 2\u03a8 + \u03c0, equation (3.21) is written as",
              "Assuming an initial condition of the form \u03c8 (t 0 ) = \u03c0 (or \u03a8 (t 0 ) = 0), the above expression can be integrated using the definition of the incomplete elliptic integral of the first kind (A.1)",
              "where F (x; k) is the incomplete elliptic integral of the first kind with modulus k and am (x; k) is its inverse function, namely the amplitude of Jacobi elliptic functions. It is a matter of simple algebra to find that",
              "As physically expected from the pendulum mechanical analog of the problem, the above solutions are periodic for E < 0, and quasi-periodic for E > 0, thus the indices in expressions (3.25) and {{not_in_s2orc}}. Periodic properties of Jacobi elliptic functions (A.17) allow the specification of the corresponding periods,",
              "where K (x) is the complete elliptic integral of the first kind. Property (A.11) of Jacobi elliptic functions implies that at the limit E \u2192 \u22122 one recovers the single meron solution",
              "as expected. On the other hand, property (A.14) of Jacobi elliptic functions implies that at the limit E \u2192 0 both periodic and quasi-periodic solutions tend to the instanton solution lim",
              "In figure 1, the surfaces in S 2 corresponding to the periodic elliptic solutions and the meron and instanton limits are displayed. It can be seen that indeed the elliptic solutions interpolate between the degenerate mapping of the meron configuration to the identity mapping from S 2 to S 2 that corresponds to a single instanton.",
              "We would also like to comment that the topological charge, as implied by equation (3.16), is not well defined for the elliptic solutions due to the non-existence of the limit lim t\u2192\u221e \u03c8 (t). Of course at the limits E \u2192 \u22122 and E \u2192 0, corresponding to the stable equilibrium and kink solutions respectively, the solution is neither periodic nor quasiperiodic and the limit lim t\u2192\u221e \u03c8 (t) is well defined, giving rise to a well defined topological charge number, as required for meron and instanton solutions.",
              "For solutions (3.25) and (3.26), as can be easily seen from equations (3.17) (3.18), the existence of the integral (3.21) implies that the stress-energy tensor components are equal to",
              "It is easy to check that the above expressions agree with the fact that the instanton solutions are characterized by vanishing stress-energy tensor, as well the formulas (3.11) and (3.12), giving the stress-energy tensor components for a meron configuration at the appropriate limit E \u2192 \u22122 and f (z) \u2192 z."
            ]
          },
          {
            "header": "Reduction of the Euclidean O(3) Non-linear Sigma Model",
            "paragraphs": [
              "The Pohlmeyer reduced theory is derived based on the formulation of the sigma model in terms of the scalar fields triplet X i , that take values in a symmetric target space, namely S 2 . The most known example of such reduction is for the Lorentzian version of the model under study, leading to the sine-Gordon equation {{123165375}}. Here we will follow closely the analysis of {{123165375}} in the reduction of the Euclidean version of O(3) non-linear sigma model."
            ]
          },
          {
            "header": "Definition of the Reduced Theory Field",
            "paragraphs": [
              "First, we would like to observe that by definition of the O(3) sigma model, the scalar fields obey the constraint X i X i = 1. It is a direct consequence that the vector X i is perpendicular to the vectors \u2202X i and\u2202X i .",
              "An important feature of the sigma model that is critical for Pohlmeyer reduction is the fact that the T zz component of the stress-energy tensor is a function solely of z, while the Tzz component is a function solely ofz. Indeed, using expression (2.16) for T zz , the equations of motion (2.11) and the orthogonality between X i and \u2202X i , we find",
              "The stress-energy tensor for meron solutions (3.11) and (3.12), for elliptic solutions (3.30) and (3.31), as well as the fact that the stress-energy tensor for instanton solutions vanishes, are all in accordance with this property. The above fact implies that there is an appropriate redefinition of the complex coordinate z, z = z (w) andz =z (w) that allows setting the stress-energy tensor elements to a real and positive constant 2",
              "(4.4)",
              "Notice that (4.4) implies that the fields do not acquire a given value as |w| \u2192 \u221e and thus, such boundary conditions should be abandoned when the Pohlmeyer reduced theory is considered.",
              "In the traditional Pohlmeyer reduction of the Lorentzian O(3) sigma model, the reduced field is defined as the angle between the vectors \u2202 + X i and \u2202 \u2212 X i , where x \u00b1 are light cone coordinates x \u00b1 = x 0 \u00b1x 1 . In this case, it is a natural definition for the reduced degrees of freedom to set \u2202 + X i \u2202 \u2212 X i = \u00b5 2 cos \u03d5, since \u2202 \u00b1 X i are real vectors with norm equal to \u00b5, as a consequence of the stress-energy tensor being equal to T ++ = T \u2212\u2212 = \u00b5 2 , similarly to our case. However, in the Euclidean scenario under study, the inner product of \u2202X i and\u2202X i is always positive and larger than the absolute value of \u00b5 2 . If we define the real and imaginary parts of w as x and y and remembering that we have performed an appropriate coordinate redefinition such that \u00b5 2 is real, it is true that",
              "Thus, \u2202X m\u2202 X m \u2265 \u00b5 2 and a natural definition for the reduced degrees of freedom in the case of the Euclidean sigma model is",
              "Notice that since \u2202X m\u2202 X m is both positive and larger than the absolute value of T zz , equation (4.7) is a good definition for the reduced field \u03d5 only if \u00b5 2 has been selected to be positive. Thus, one has to be careful when defining the complex variable w, so that T ww is not only constant and real, but also positive."
            ]
          },
          {
            "header": "The Basis in Enhanced Space and the Reduced Theory",
            "paragraphs": [
              "We would like to specify the dynamics of the field \u03d5. In order to do so, we need to calculate the derivative \u2202\u2202 cosh \u03d5. Using the definition (4.7) of the field \u03d5, we get",
              "The first term turns out to be the most complicated to calculate. Finding a base in the enhanced, flat, three-dimensional target space of the theory would be convenient, in order to express the second derivatives of X i in that base and calculate the desired term.",
              "As explained in section 4.1, the vector X i is perpendicular to the vectors \u2202X i and\u2202X i . Consequently, unless a degeneracy between the real and imaginary parts of \u2202X i occurs, the three vectors X i , \u2202X i and\u2202X i form a basis in the three-dimensional enhanced flat target space of the sigma model and thus, any vector can be written as a linear combination of the above.",
              "Let's take advantage of this fact and write the second derivative of X i as such a linear combination,",
              "Then, equation (4.1) and the definition of the field \u03d5 (4.7) imply that \u2202 2 X i X i = a, (4.10)",
              "The vectors X i and \u2202X i are orthogonal. It is a direct consequence that",
              "The vector \u2202X i has constant magnitude and thus,",
              "Finally, from the definition of the field variable \u03d5, we get",
              "Thus, we have all necessary information to specify the coefficients a, b and c. We find",
              "It is now straightforward to calculate the inner product \u2202 2 X i \u2202 2 X i , \u2202 2 X i\u22022 X i = a\u0101 + \u00b5 2 bc +bc + \u00b5 2 bb + cc cosh \u03d5 = \u00b5 4 + \u00b5 2 cosh \u03d5\u2202\u03d5\u2202\u03d5 .",
              "(4.20)",
              "The second and third terms of (4.8) can be easily calculated with the help of the equations of motion (2.15), expressed with the help of field \u03d5,",
              "(4.21)",
              "Differentiating the above and taking the appropriate inner product we find",
              "Finally, the last term of (4.8) can be calculated directly from the equations of motion",
              "All four terms required for the calculation of the derivative \u2202\u2202 cosh \u03d5 through equation (4.8) have been specified and we get \u00b5 2 \u2202\u2202 cosh \u03d5 = \u00b5 2 cosh \u03d5\u2202\u03d5\u2202\u03d5 \u2212 \u00b5 4 sinh 2 \u03d5, {{not_in_s2orc}} implying that \u2202\u2202\u03d5 = \u2212\u00b5 2 sinh \u03d5, We would like to point out here that in an obvious way, Pohlmeyer reduction cannot be performed if the stress-energy tensor is vanishing. Thus, it is expected that instanton solutions, which are the only finite-action solutions, do not have a counterpart within the solutions of the sinh-Gordon equation. Consequently, we expect that the counterparts of the elliptic solutions must also become problematic at the E \u2192 0 limit."
            ]
          },
          {
            "header": "Meron Solutions in the Reduced Theory",
            "paragraphs": [
              "The stress-energy tensor elements in the case of meron solutions are given by equations {{not_in_s2orc}} and {{not_in_s2orc}}. We desire performing a conformal transformation z = z (w) so that the stress-energy tensor elements are set to a positive constant. After such a conformal transformation, the new stress-energy tensor ww component is equal to",
              "So if we demand that g 2 T ww = \u00b5 2 , the appropriate conformal transformation should obey",
              "In terms of the new coordinate w, the meron solution is written as",
              "Thus, the corresponding solution of the Pohlmeyer reduced theory is",
              "This implies that all meron solutions are mapped through Pohlmeyer reduction to the ground state of the sinh-Gordon equation."
            ]
          },
          {
            "header": "The Elliptic Solutions in the Reduced Theory",
            "paragraphs": [
              "We have seen in Section 3.3, that the Euclidean O(3) NLSM accepts a family of solutions that correspond to solutions of the one-dimensional pendulum with potential V = cos \u03c8 \u2212 1, characterized by the value of the energy integral E. The stress-energy tensor for these solutions is given by equations (3.30) and (3.31)."
            ]
          },
          {
            "header": "Quasi-Periodic Elliptic Solutions",
            "paragraphs": [
              "As for merons, the reduction of the theory requires the performance of a conformal transformation in order to set the stress-energy tensor equal to a positive constant. For quasi-periodic elliptic solutions, for whom E > 0, such an appropriate conformal transformation is z = e w a . (5.6)",
              "Then, the stress-energy tensor takes the form",
              "Defining the real and imaginary parts of the complex variable w to be x and y respectively,",
              "It turns out that these solutions have a simpler expression in terms of the angular field coordinates \u0398 and \u03a6. Specifically,",
              "Since \u0398 depends only on the real part of w and \u03a6 depends only on the imaginary part of w, we acquire the following simple formulas using equations (2.21) and (2.7)",
              "Thus, the counterpart of the quasi-periodic elliptic solution (3.25) in the Pohlmeyer reduced theory is",
              "(5.14)",
              "Since the reduced field solution depends only on the real part of the complex variable w, the solutions belong to a special subset of the solutions of the Euclidean sinh-Gordon equation that depend on only one real variable, \u03d5 = \u03d5 (x) and consequently they obey",
              "These solutions have an one-dimensional mechanical analog of a \"hyperbolic pendulum\" with potential V = 4\u00b5 2 cosh \u03d5, {{not_in_s2orc}} which from energy conservation obey",
              "It is a matter of simple algebra to show that equation (5.14) implies"
            ]
          },
          {
            "header": "Periodic Elliptic Solutions",
            "paragraphs": [
              "For periodic elliptic solutions, for whom E < 0, an appropriate conformal transformation to set the stress-energy tensor to a constant is",
              "Then, the stress-energy tensor takes the form",
              "Defining the real and imaginary parts of the complex variable w to be x and y respectively,",
              "This implies that the solution in terms of the angular field coordinates is written as {{not_in_s2orc}} and similarly to the case of quasi-periodic solutions we can acquire the following simple expressions",
              "Thus, the counterpart of the periodic elliptic solution (3.26) in the reduced theory is",
              "As in the case of the quasi-periodic elliptic solution, the reduced field solution depends only on the imaginary part of the complex variable w, and, thus, the solutions belong to a special subset of the solutions of the Euclidean sinh-Gordon equation that depend only on one real variable, \u03d5 = \u03d5 (y) and therefore obey d 2 \u03d5 dy 2 = \u22124\u00b5 2 sinh \u03d5, {{not_in_s2orc}} being the same ordinary differential equation as in the case of quasi-periodic solutions.",
              "Consequently, there is also the same one-dimensional mechanical analog of a \"hyperbolic pendulum\" with potential given by equation {{not_in_s2orc}}. Energy conservation in this one-dimensional mechanical analog implies that the counterparts of periodic elliptic solutions in the Pohlmeyer reduced theory obey",
              "It is a matter of simple algebra to show that equation (5.27) implies that",
              "which is the exact same formula as in the case of quasi-periodic solutions."
            ]
          },
          {
            "header": "Comparison of the Initial and Reduced Formulations",
            "paragraphs": [
              "In the process of deriving the form of the counterparts of the NLSM solutions in the Pohlmeyer reduced theory, we performed a conformal transformation that sets the stress-energy tensor to a real and positive constant. Although equation (4.2) ensures that there is always such a transformation, it is not the case that this is the same transformation for all NLSM solutions. This becomes evident comparing the appropriate conformal transformations in the case of meron and elliptic solutions, given by equations (5.2), (5.6) and {{not_in_s2orc}}. Even in the case of the family of elliptic solutions, the same conformal transformation manages to set the stress-energy tensor components to a real and positive constant for the whole family, but a different one for each member of the family. Since this constant enters into the reduced problem as the mass scale of the sinh-Gordon equation, each solution of the elliptic family has been mapped to a solution of a different version of the sinh-Gordon equation.",
              "In order to better understand the correspondence between the elliptic solutions of the NLSM and the solutions of the sinh-Gordon equation, we would like to make a more specific conformal transformation for each solution, so that the mass scale of the sinh-Gordon equation is always the same. This can be achieved by taking advantage of the freedom to select the parameter a appearing in the conformal transformations {{not_in_s2orc}} and {{not_in_s2orc}}. Choosing",
              "the potential of the one-dimensional effective mechanical problem equals V = cosh \u03d5 (5.32) in all cases. Then, the corresponding energy constant in the reduced one-dimensional mechanical problem (the \"hyperbolic\" pendulum problem) is related with the energy constant in the initial one-dimensional mechanical problem (the pendulum problem) as",
              "Notice that E diverges as E \u2192 0. This is expected since for E \u2192 0 the elliptic solutions tend to the instanton limit, which cannot have a Pohlmeyer counterpart. In figure 2, we can see the correspondence of the energy levels between the initial and reduced problems. Interestingly, for every energy level corresponding to a periodic solution of the initial pendulum problem, there is another energy level corresponding to a quasi-periodic solution and vice versa, such that the two solutions correspond to the same energy level in the reduced \"hyperbolic pendulum\" problem. This is evident in figure 3, which depicts the relation between the energy levels in the original and reduced problems. Since the reduced problem solution is characterized only by its energy level (up to a shift in the \"time\" variable), the two solutions of the reduced problem are actually the same solution. One can easily check that equations (5.18) and (5.30), together with the appropriate setting of the parameter a (5.31), imply that two energy levels E 1 < 0 and E 2 > 0 in the original problem corresponding to the same energy level in the reduced theory E 1 = E 2 obey E 1 E 2 + 2 (E 1 + E 2 ) = 0. and thus, the two solutions would be connected with the modular transformation k \u2192 1/k. On the other hand, the two solutions in the initial formulation of the theory are not identical. The periodic solution corresponds to a surface that covers part of the whole sphere as shown in figure 1, while the quasi-periodic solution covers the whole sphere, but it is singular at the poles, as the derivatives of the fields do not vanish there. Actually, the discontinuity of the surface corresponding to the periodic solution is similar to the discontinuity to the longitudinal tangential vector of the surface corresponding to the quasi-periodic solution; one can show that",
              "One may suggest that the two distinct NLSM solutions correspond to the same solution in the reduced theory, as Pohlmeyer reduction deals with the position vector and tangential vector in a symmetric way, as two elements of the basis defined in the enhanced flat three-dimensional target space."
            ]
          }
        ]
      },
      {
        "header": "Acknowledgments",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Acknowledgments",
            "paragraphs": [
              "This work is dedicated to the memory of Prof. Ioannis Bakas. His unexpected loss deprived the scientific community of a member with an elegant way of thinking, infinite organization skills and integrity. This paper is the outcome of uncountable discussions on the connection between geometry and physics and it was built alongside our previous joint work."
            ]
          },
          {
            "header": "A Useful Formulas for Jacobi's Elliptic Functions",
            "paragraphs": [
              "The incomplete elliptic integral of the first kind is defined as"
            ]
          },
          {
            "header": "For",
            "paragraphs": []
          },
          {
            "header": "Figure 1 .",
            "paragraphs": []
          },
          {
            "header": "Figure 2 .",
            "paragraphs": []
          },
          {
            "header": "Figure 3 .",
            "paragraphs": []
          },
          {
            "header": "F",
            "paragraphs": []
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "12750838"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": [
            "We found the corresponding counterparts of meron and elliptic solutions of Euclidean O(3) non-linear sigma model in the Pohlmeyer reduced version of the same theory, namely the sinh-Gordon equation. Summarizing, we may conclude the following: Instanton solutions, which are the only finite action solutions of the O(3) model, do not have a counterpart, as they are characterized by vanishing stress-energy tensor. Therefore, not every solution of the non-linear sigma model has a counterpart in the Pohlmeyer reduced theory. Especially solutions with a well defined topological number cannot be mapped to the Pohlmeyer reduced theory, as they have to be characterized by a unique limit at complex infinity, which is inconsistent with the form of the imposed Virasoro constraint in Pohlmeyer reduction.",
            "All meron solutions correspond to the ground state of the sinh-Gordon equation. It seems that this is a more general property that singular configurations such as a meron have. In a similar manner, strings propagating into R t \u00d7 S 2 are described by a non-linear sigma model that is reducible to the sine-Gordon equation. Classical string solutions that are singular in a similar sense (the world-sheet tends to an onedimensional manifold, such as a point particle solution, or the \"hoop\" string solution) have the property of being mapped to either the stable or the unstable vacuum of the sine-Gordon equation {{12750838}}.",
            "Elliptic solutions, which can be viewed as solutions of an one-dimensional mechanical pendulum problem in the initial formulation of the theory, have also an onedimensional mechanical analogue with a hyperbolic cosine potential in the Pohlmeyer reduced description. For every periodic solution in the pendulum problem, there is a quasi-periodic solution and vice versa that has the same counterpart in the hyperbolic cosine problem. The above pair of solutions correspond to physically distinct solutions in the initial formulation of the sigma model. Furthermore, different, physically inequivalent solutions of the sigma model may correspond to the same solution in the Pohlmeyer reduced theory. This may be attributed to the fact that Pohlmeyer reduction deals with the fields and their derivatives in a symmetric way, as elements of a vector basis in an enhanced higher dimensional space, in our case R 3 .",
            "It would be interesting to generalize these results studying the correspondence between solutions of non-linear sigma models with different target space geometries or in higher dimensional target spaces."
          ]
        }
      ]
    },
    "discussion_txt": "We found the corresponding counterparts of meron and elliptic solutions of Euclidean O(3) non-linear sigma model in the Pohlmeyer reduced version of the same theory, namely the sinh-Gordon equation. Summarizing, we may conclude the following: Instanton solutions, which are the only finite action solutions of the O(3) model, do not have a counterpart, as they are characterized by vanishing stress-energy tensor. Therefore, not every solution of the non-linear sigma model has a counterpart in the Pohlmeyer reduced theory. Especially solutions with a well defined topological number cannot be mapped to the Pohlmeyer reduced theory, as they have to be characterized by a unique limit at complex infinity, which is inconsistent with the form of the imposed Virasoro constraint in Pohlmeyer reduction.All meron solutions correspond to the ground state of the sinh-Gordon equation. It seems that this is a more general property that singular configurations such as a meron have. In a similar manner, strings propagating into R t \u00d7 S 2 are described by a non-linear sigma model that is reducible to the sine-Gordon equation. Classical string solutions that are singular in a similar sense (the world-sheet tends to an onedimensional manifold, such as a point particle solution, or the \"hoop\" string solution) have the property of being mapped to either the stable or the unstable vacuum of the sine-Gordon equation {{12750838}}.Elliptic solutions, which can be viewed as solutions of an one-dimensional mechanical pendulum problem in the initial formulation of the theory, have also an onedimensional mechanical analogue with a hyperbolic cosine potential in the Pohlmeyer reduced description. For every periodic solution in the pendulum problem, there is a quasi-periodic solution and vice versa that has the same counterpart in the hyperbolic cosine problem. The above pair of solutions correspond to physically distinct solutions in the initial formulation of the sigma model. Furthermore, different, physically inequivalent solutions of the sigma model may correspond to the same solution in the Pohlmeyer reduced theory. This may be attributed to the fact that Pohlmeyer reduction deals with the fields and their derivatives in a symmetric way, as elements of a vector basis in an enhanced higher dimensional space, in our case R 3 .It would be interesting to generalize these results studying the correspondence between solutions of non-linear sigma models with different target space geometries or in higher dimensional target spaces.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "Title: Mapping between Solutions of the Euclidean O(3) Non-Linear Sigma Model and the Sinh-Gordon Equation",
          "",
          "Abstract:",
          "",
          "In this paper, we investigate the relationship between known solutions of the Euclidean O(3) non-linear sigma model and their counterparts in the Pohlmeyer reduced theory, specifically the sinh-Gordon equation. Our results show that instantons do not have a direct counterpart in the reduced theory, while merons correspond to the ground state and elliptic solutions are characterized by a two-to-one correspondence between solutions in the two descriptions.",
          "",
          "Introduction:",
          "",
          "The Euclidean O(3) non-linear sigma model is a fundamental model in theoretical physics, which describes the dynamics of a scalar field in a curved space-time. The model has been extensively studied, and various solutions have been found, including instantons, merons, and elliptic solutions. In recent years, there has been growing interest in understanding the relationship between these solutions and the Pohlmeyer reduced theory, which is a simplified version of the model obtained by integrating out the scalar field.",
          "",
          "In this paper, we aim to provide a systematic study of the mapping between known solutions of the Euclidean O(3) non-linear sigma model and their counterparts in the Pohlmeyer reduced theory. Specifically, we focus on the sinh-Gordon equation, which is a integrable system that arises as a reduced theory of the non-linear sigma model.",
          "",
          "Results:",
          "",
          "Our results can be summarized as follows:",
          "",
          "1. Instantons do not have a direct counterpart in the Pohlmeyer reduced theory. This is because the instantons are topological defects that arise from the non-trivial topology of the space-time, and they cannot be reduced to a set of local equations in the Pohlmeyer reduced theory.",
          "2. Merons correspond to the ground state of the system in both the non-linear sigma model and the Pohlmeyer reduced theory. This is because the merons are the solutions that preserve the symmetry of the system, and they are therefore the most stable solutions in both descriptions.",
          "3. Elliptic solutions are characterized by a two-to-one correspondence between solutions in the two descriptions. Specifically, every elliptic solution in the non-linear sigma model corresponds to two solutions in the Pohlmeyer reduced theory, which are related by a non-trivial scaling transformation.",
          "",
          "Discussion:",
          "",
          "Our results provide a systematic understanding of the mapping between known solutions of the Euclidean O(3) non-linear sigma model and their counterparts in the Pohlmeyer reduced theory. The absence of a direct counterpart for instantons in the reduced theory highlights the importance of non-trivial topology in the model, which cannot be captured by the simplified reduced theory. On the other hand, the correspondence between merons and the ground state in both descriptions suggests that the reduced theory provides a useful framework for understanding the behavior of the system near the ground state. Finally, the two-to-one correspondence between elliptic solutions in the two descriptions provides a new perspective on the structure of the solution space of the non-linear sigma model.",
          "",
          "Conclusion:",
          "",
          "In conclusion, our study provides a systematic investigation of the mapping between known solutions of the Euclidean O(3) non-linear sigma model and their counterparts in the Pohlmeyer reduced theory. Our results highlight the importance of non-trivial topology in the model and provide new insights into the behavior of the system near the ground state. The two-to-one correspondence between elliptic solutions in the two descriptions provides a new perspective on the structure of the solution space of the non-linear sigma model, which can be further explored in future studies."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.6601484417915344,
            "R": 0.7190260887145996,
            "F1": 0.6883305311203003
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.5716783404350281,
            "R": 0.5756226181983948,
            "F1": 0.573643684387207
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          "The sine-Gordon equation, which describes the dynamics of a massless scalar field in 2"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.379201740026474,
            "R": 0.6098263263702393,
            "F1": 0.46762514114379883
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.37043505907058716,
            "R": 0.5955415964126587,
            "F1": 0.4567594528198242
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "Instantons do not have a counterpart in the reduced theory, while merons have a counterpart."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.3950977325439453,
            "R": 0.7041651010513306,
            "F1": 0.5061829090118408
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.36191582679748535,
            "R": 0.6203585863113403,
            "F1": 0.45713821053504944
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10020161",
    "title": "Effectiveness of biomarker-based exclusion of ventilator-acquired pneumonia to reduce antibiotic use (VAPrapid-2): study protocol for a randomised controlled trial",
    "externalids": {
      "ACL": "",
      "ArXiv": "",
      "CorpusId": "10037358",
      "DBLP": "",
      "DOI": "10.1186/s13063-016-1442-x",
      "MAG": "2473068687",
      "PubMed": "27422026",
      "PubMedCentral": "4947254"
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "Introduction",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Introduction",
            "paragraphs": [
              "Alignments in parallel corpora provide a straightforward basis for the extraction of paraphrases by means of re-translating pivots and then ranking the obtained set of candidates. For example, if the German verb aufsteigen is aligned with the English pivot verbs rise and climb up, and the two English verbs are in turn aligned with the German verbs aufsteigen, ansteigen and hochklettern, then ansteigen and hochklettern represent two paraphrase candidates for the German verb aufsteigen. {{15728911}} were the first to apply this method to gather paraphrases for individual words and multi-word expressions, using translation probabilities as criteria for ranking the obtained paraphrase candidates.",
              "This standard re-translation approach however suffers from a major re-translation sense problem, because the paraphrase candidates cannot distinguish between the various senses of the target word or phrase. Consequently, (i) the different senses of the original word or phrase are merged,  when the back translations of all pivot words are collected within one set of paraphrase candidates; and (ii) the ranking step does not guarantee that all senses of a target are covered by the top-ranked candidates, as more frequent senses amass higher translation probabilities and are favoured.",
              "Recently, {{11942888}} proposed two approaches to distinguish between paraphrase senses (i.e., aiming to solve problem (i) above). In this paper, we address both facets (i) and (ii) of the re-translation sense problem, while focusing on an emprically challenging class of multi-word expressions, i.e., German particle verbs (PVs). German PVs can appear morphologically joint or separated (such as steigt . . . auf ), and are often highly ambiguous. For example, the 138 PVs we use in this paper have an average number of 5.3 senses according to the Duden 1 dictionary. Table 1 illustrates the re-translation sense problem for German PVs. It lists the 10 top-ranked paraphrases for the target verb ausrichten obtained with the standard method. Four synonyms in the 10 top-ranked candidates were judged valid according to the Duden, covering three out of five senses listed in the Duden. Synonyms for a fourth sense \"to tell\" (sagen,\u00fcbermitteln, weitergeben) existed in the candidate list, but were ranked low.",
              "Our approach to incorporate word senses into the standard paraphrase extraction applies a graph-based clustering to the set of paraphrase candidates, based on a method described in {{13272287}}{{718563}}. It divides the set of candidates into clusters by reducing edges in an originally fully-connected graph to those exceeding a dynamic similarity threshold. The resulting clusters are taken as paraphrase senses, and different parameters from the graphical clustering (such as connectedness in clusters; cluster centroid positions; etc.) are supposed to enhance the paraphrase ranking step. With this setting, we aim to achieve higher precision in the top-ranked candidates, and to cover a wider range of senses as the original re-translation method. {{15728911}} introduced the idea of extracting paraphrases with the retranslation method. Their work controls for word senses regarding specific test sentences, but not on the type level. Subsequent approaches improved the basic re-translation method, including {{2755801}} who restrict paraphrases by syntactic type; and Wittmann et al. (2014) who add distributional similarity between paraphrase candidate and target word as a ranking feature. Approaches that applied extracted paraphrases relying on the re-translation method include the evaluation of SMT {{16241846}} and query expansion in Q-A systems {{2713391}}."
            ]
          }
        ]
      },
      {
        "header": "Related Work",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Related Work",
            "paragraphs": [
              "Most recently, {{11942888}} proposed two clustering algorithms to address one of the sense problems: They discriminate between target word senses, exploiting hierarchical graph factorization clustering and spectral clustering. The approaches cluster all words in the Paraphrase Database {{6067240}} and focus on English nouns in their evaluation.",
              "A different line of research on synonym extraction has exploited distributional models, by relying on the contextual similarity of two words or phrases, e.g. {{not_in_s2orc}}, van der Plas and Tiedemann (2006), {{7747235}}, {{1588782}}. Typically, these methods do not incorporate word sense discrimination."
            ]
          },
          {
            "header": "Synonym Extraction Pipeline",
            "paragraphs": [
              "This section lays out the process of extracting, clustering and ranking synonym candidates."
            ]
          },
          {
            "header": "Synonym Candidate Extraction",
            "paragraphs": [
              "Following the basic approach for synonym extraction outlined by {{15728911}}, we gather all translations (i.e., pivots) of an input particle verb, and then re-translate the pivots. The back translations constitute the set of synonym candidates for the target particle verb.",
              "In order to rank the candidates according to how likely they represent synonyms, each candidate is assigned a probability. The synonym probability p(e 2 |e 1 ) e2 =e1 for a synonym candidate verb e 2 given a target particle verb e 1 is calculated as the product of two translation probabilities: the pivot probability p(f i |e 1 ), i.e. the probability of the English pivot f i being a translation of the particle verb e 1 , and the return probability p(e 2 |f i ), i.e. the probability that the synonym candidate e 2 is a translation of the English pivot f i . The final synonym score for e 2 is the sum over all pivots f 1..n that re-translate into the candidate:",
              "The translation probabilities are based on relative frequencies of the counts in a parallel corpus, cf. section 4.1.",
              "Filtering We apply filtering heuristics at the pivot probability step and the return probability step: obviously useless pivots containing only stop-words (e.g. articles) or punctuation are discarded. In the back-translation step, synonym candidates that did not include a verb are removed. Furthermore, we removed pivots (pivot probability step) and synonym candidates (return probability step) consisting only of light verbs, due to their lack of semantic content and tendency to be part of multi-word expressions. If left unfiltered, light verbs often become super-nodes in the graphs later on (see section 3.2) due to their high distributional similarity with a large number of other synonym candidates. This makes it difficult to partition the graphs into meaningful clusters with the algorithm used here.",
              "Distributional Similarity We add distributional information as an additional feature for the ranking of synonym candidates, because weighting the score from equation (1) by simple multiplication with the distributional similarity between the candidate and the target (as obtained from large corpus data, cf. section 4.1), has been found to improve the ranking {{1998986}}.",
              "Properties of the clusters: C(#(cand)) number of synonym candidates in a cluster C(av-sim(cand,c)) average distributional similarity between synonym candidates in a cluster and the cluster centroid C(av(#(e))) average number of edges in the clusters of the cluster analyses C(#(e)) total number of edges in a cluster C(av-sim(cand,v)) average distributional similariy between synonym candidates in a cluster and the target PV C(av-sim(cand,gc)) average distributional similariy between all synonym candidates and the global centroid C(sim(c,v)) distributional similarity between a cluster centroid and the target PV C(con) connectedness of a cluster Properties of the synonym candidates: S(tr) translation probability of a synonym candidate S(#(e)) number of edges of a synonym candidate S(cl%(#(e))) proportion of cluster edges for a synonym candidate S(sim(cand,v)) distributional similarity between a synonym candidate and the target PV S(sim(cand,c)) distributional similarity between a synonym candidate and the cluster centroid S(sim(cand,gc)) distributional similarity between a synonym candidate and the global centroid Table 2: Properties of synonym candidates and clusters."
            ]
          },
          {
            "header": "Graph-Based Clustering of Candidates",
            "paragraphs": [
              "The clustering algorithm suggested by {{718563}} is adopted for clustering all extracted synonym candidates for a specific particle verb target. In a first step, a fully connected undirected graph of all synonym candidates is created as a starting point, with nodes corresponding to synonym candidates and edges connecting two candidates; edge weights are set according to their distributional similarity. In a second step, a similarity threshold is calculated, in order to delete edges with weights below the threshold. The threshold is initialized with the mean value between all edge weights in the fully connected graph. Subsequently, the threshold is updated iteratively:",
              "1. The synonym candidate pairs are partitioned into two groups: P 1 contains pairs with similarities below the current threshold, and P 2 contains pairs with similarities above the current threshold and sharing at least one pivot."
            ]
          },
          {
            "header": "A new threshold is set: T =",
            "paragraphs": [
              "A P 1 +A P 2 2 , where A P i is the mean over all similarities in P i .",
              "After convergence, the resulting graph consists of disconnected clusters of synonym candidates. Singleton clusters are ignored. The sub-graphs represent the cluster analysis to be used in the ranking of synonyms for the target particle verb."
            ]
          },
          {
            "header": "Iterative Application of Clustering Algorithm",
            "paragraphs": [
              "Because the resulting clusterings of the synonym candidates typically contain one very large (and many small) clusters, we extend the original algorithm and iteratively re-apply the clustering: After one pass of the clustering algorithm as described above (T 1 ), the resulting set of connected synonym candidates becomes the input to another iteration of the algorithm (T 2...n ). Each iteration of the algorithm results in a smaller and more strongly partitioned sub-graph of the initially fully connected graph because the similarity threshold for edges becomes successively higher."
            ]
          },
          {
            "header": "Synonym Candidate Ranking",
            "paragraphs": [
              "Assuming that clusters represent senses, we hypothesize that combining properties of individual synonym candidates with properties of the graphbased clusters of synonym candidates results in a ranking of the synonym candidates that overcomes both facets of the re-translation sense problem: Including synonym candidates from various clusters should ensure more senses of the target particle verbs in the top-ranked list; and identifying salient clusters should improve the ranking. Table 2 lists the properties of the individual synonym candidates S and the properties of the graph-based cluster analyses C that we consider potentially useful. For the experiments in section 4, we use all combinations of S and C properties."
            ]
          },
          {
            "header": "Experiments, Results and Discussion",
            "paragraphs": []
          },
          {
            "header": "Data and Evaluation",
            "paragraphs": [
              "For the extraction of synonym candidates, we use the German-English version of Europarl (1.5M parallel sentences) with GIZA++ word alignments for the extraction of synonym candidates. In the alignments, the German data is lemmatized and reordered in order to treat split occurrences of particle and verb as a single word {{441443}}{{1540169}} (2) S(tr) \u00b7 S(sim(cand,v)) \u00b7 C(av-sim(cand,gc)) 38.26 27.90 2.04 46.89 5 clustering + ranking (3) S(tr) \u00b7 S(sim(cand,v)) 38.19 27.90 2.04 46.89 6 clustering + ranking (4) S(tr) \u00b7 S(sim(cand,v)) \u00b7 C(sim(cand,v)) 38.12 27.90 2.04 46.89 7 clustering + ranking (5) S(tr) \u00b7 S(sim(cand,v)) \u00b7 C(con) 37.97 27.83 2.03 46.65 Table 3: Evaluation of basic approaches and best five rankings: precision & no./proportion of senses.",
              "The distributional similarity sim is determined by cosine similarities between vectors relying on co-occurrences in a window of 20 words. We use the German web corpus DECOW14AX {{7987482}}{{51844671}} containing 12 billion tokens, with the 10,000 most common nouns as vector dimensions. The feature values are calculated as Local Mutual Information (LMI), cf. {{not_in_s2orc}}.",
              "Our dataset contains the same 138 German particle verbs from Europarl as in previous work {{1998986}}, all PVs with a frequency f \u2265 15 and at least 30 synonyms listed in the Duden dictionary. For the evaluation, we also rely on the Duden, which provides synonyms for the target particle verbs and groups the synonyms by word sense. We consider four evaluation measures, and compare the ranking formulas by macro-averaging each of the evaluation measures over all 138 particle verbs:",
              "\u2022 Precision among the 10/20 top-ranked synonym candidates.",
              "\u2022 Number and proportion of senses represented among the 10 top-ranked synonyms."
            ]
          }
        ]
      },
      {
        "header": "Results",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Results",
            "paragraphs": [
              "The basic system (line 1 in table 3) only relies on the translation probabilities (S(tr)). It is extended by incorporating the distributional similarity between the target particle verb and the synonym candidates (line 2). Our five best rankings with one iteration of graphical clustering (T 1 ) are shown in lines 3-7. All of these include the translation probability and the distributional similarity between candidate and particle verb; only one makes use of cluster information. Thus, the simple distributional extension is so powerful that additional cluster information cannot improve the system any further. The most relevant cluster measure is the number of edges of the cluster C(#(e)), an indication of cluster size and connectedness.",
              "While the best three clustering systems 2 outperform the extended basic system (line 2) in terms of top-10/top-20 precision, none of the improvements is significant. {{not_in_s2orc}} Also, the number and proportion of senses remain the same as in the basic approach with distributional extension. Further iterations of the clustering step (T 2...n ) up to n = 8 lead to increasingly worse precision scores and sense detection, cf. figure 1 for T 1...5 ."
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "11942888"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": [
            "Overall, the distributional similarity between the target word and the synonym candidates represents the strongest extension of the basic retranslation approach, and the cluster graphs do not provide further useful information. A breakdown of the cluster analyses revealed that the cluster sizes are very unevenly distributed. Typically, there is one very large cluster and several considerably smaller clusters, as shown by the first part of  Table 4: Distribution of candidates, synonyms and senses in the largest cluster vs. all other clusters in the iterations T 1 -T 5 .",
            "proportion of candidates in the remaining clusters.",
            "In addition, we found that most correct synonyms are also in the largest cluster (middle part of table 4). Accordingly, the cluster analyses do not represent partitions of the target verb senses, but most senses are in the largest cluster (bottom part of table 4).",
            "Consequently, while the synonym features are useful for ranking the set of candidates, clusterlevel features are ineffective as they are derived from effectively meaningless cluster analyses. 4 While re-applying the clustering step gradually overcomes the uneven cluster distribution (iterations T 2 -T 5 in table 4), the sizes of the graphs decrease dramatically. For example (not depicted in table 4), on average there are only 169 candidates left in T 5 compared to 1,792 in T 1 , with an average of 2.8 correct synonyms instead of 22.5, and an average of 1.7 senses instead of 4.5.",
            "We assume that partitioning the candidate set according to senses in combination with the cluster-level measures is a valid approach to deal with the word sense problem, but based on our analysis we conclude that either (i) the context vectors are not suitable to differentiate between senses, or that (ii) the clustering algorithm is inapt for this scenario. A possible solution might be to apply the algorithms suggested in {{11942888}}. Finally, no weighting was applied to any of the properties listed in table 2. This could be improved by using a held-out data development set, and a greater number of particle verbs (we only use 138) would probably be needed as well."
          ]
        },
        {
          "header": "Summary",
          "paragraphs": [
            "We hypothesized that graph-based clustering properties in addition to synonym candidate properties should improve the precision of synonym identification and ranking, and extend the diversity of synonym senses. Unfortunately, our extensions failed, and analyses of cluster properties revealed that future work should improve the vector representations and compare other clustering algorithms. One should keep in mind, however, that we focused on a specifically challenging class of multi-word expressions: highly ambiguous German particle verbs."
          ]
        },
        {
          "header": "Figure 1 :",
          "paragraphs": []
        },
        {
          "header": "Table 1 :",
          "paragraphs": []
        },
        {
          "header": "table 4 ,",
          "paragraphs": []
        }
      ]
    },
    "discussion_txt": "Overall, the distributional similarity between the target word and the synonym candidates represents the strongest extension of the basic retranslation approach, and the cluster graphs do not provide further useful information. A breakdown of the cluster analyses revealed that the cluster sizes are very unevenly distributed. Typically, there is one very large cluster and several considerably smaller clusters, as shown by the first part of  Table 4: Distribution of candidates, synonyms and senses in the largest cluster vs. all other clusters in the iterations T 1 -T 5 .proportion of candidates in the remaining clusters.In addition, we found that most correct synonyms are also in the largest cluster (middle part of table 4). Accordingly, the cluster analyses do not represent partitions of the target verb senses, but most senses are in the largest cluster (bottom part of table 4).Consequently, while the synonym features are useful for ranking the set of candidates, clusterlevel features are ineffective as they are derived from effectively meaningless cluster analyses. 4 While re-applying the clustering step gradually overcomes the uneven cluster distribution (iterations T 2 -T 5 in table 4), the sizes of the graphs decrease dramatically. For example (not depicted in table 4), on average there are only 169 candidates left in T 5 compared to 1,792 in T 1 , with an average of 2.8 correct synonyms instead of 22.5, and an average of 1.7 senses instead of 4.5.We assume that partitioning the candidate set according to senses in combination with the cluster-level measures is a valid approach to deal with the word sense problem, but based on our analysis we conclude that either (i) the context vectors are not suitable to differentiate between senses, or that (ii) the clustering algorithm is inapt for this scenario. A possible solution might be to apply the algorithms suggested in {{11942888}}. Finally, no weighting was applied to any of the properties listed in table 2. This could be improved by using a held-out data development set, and a greater number of particle verbs (we only use 138) would probably be needed as well.We hypothesized that graph-based clustering properties in addition to synonym candidate properties should improve the precision of synonym identification and ranking, and extend the diversity of synonym senses. Unfortunately, our extensions failed, and analyses of cluster properties revealed that future work should improve the vector representations and compare other clustering algorithms. One should keep in mind, however, that we focused on a specifically challenging class of multi-word expressions: highly ambiguous German particle verbs.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "Thank you for providing the abstract. Based on the information provided, I will conduct a scientific discussion on the paper's findings and methodology.",
          "",
          "Firstly, the paper aims to address the challenge of automatically inducing synonym paraphrases for German particle verbs, which are a difficult class of words to handle due to their complex grammatical and semantic properties. The authors propose an approach that combines a graph-based clustering method for word sense discrimination with an existing paraphrase extraction system. The primary goals of this approach are to improve the precision of synonym identification and ranking, and to expand the diversity of synonym senses.",
          "",
          "The authors' approach is based on the observation that word senses in German can be represented as nodes in a graph, where the edges represent the relationships between senses. By clustering these nodes based on their semantic similarity, the authors can identify and disambiguate different senses of a word. This approach is similar to that proposed by Cocos and Callison-Burch (2016), who used a similar graph-based method for word sense discrimination.",
          "",
          "The authors evaluate their approach using a standard paraphrase extraction system and compare it to an extended baseline that incorporates a simple distributional similarity measure. The results show that the proposed approach significantly improves over the standard system, but does not outperform the extended baseline. This suggests that while the graph-based clustering method is effective in identifying and disambiguating word senses, it may not be sufficient to overcome the challenges of synonym paraphrase induction for German particle verbs.",
          "",
          "The paper's findings have implications for the development of paraphrase extraction systems for German language. The use of graph-based clustering methods for word sense discrimination can be a valuable tool for improving the precision of synonym identification and ranking. However, more research is needed to develop a comprehensive and effective approach for synonym paraphrase induction for German particle verbs.",
          "",
          "In conclusion, the paper presents a novel approach to synonym paraphrase induction for German particle verbs, which combines a graph-based clustering method with an existing paraphrase extraction system. The results show that the proposed approach significantly improves over the standard system, but does not outperform an extended baseline incorporating a simple distributional similarity measure. The findings have implications for the development of paraphrase extraction systems for German language, and highlight the need for further research in this area."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.5643178224563599,
            "R": 0.5976482629776001,
            "F1": 0.5805050134658813
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.5012438297271729,
            "R": 0.550325870513916,
            "F1": 0.524639368057251
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          "The abstract suggests that the paper addresses the challenge of automatically identifying synonym paraphrases for German particle verbs"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.3628638982772827,
            "R": 0.5409380197525024,
            "F1": 0.4343581795692444
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.34112587571144104,
            "R": 0.596710205078125,
            "F1": 0.4340914189815521
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "Here is the scientific discussion:",
          "",
          "The proposed method incorporates a graph-based clustering approach for word sense discrimination"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.35988152027130127,
            "R": 0.6008248925209045,
            "F1": 0.4501391351222992
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.3339342772960663,
            "R": 0.5362627506256104,
            "F1": 0.41157689690589905
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10037358",
    "title": "Graph-based Clustering of Synonym Senses for German Particle Verbs",
    "externalids": {
      "ACL": "W16-1805",
      "ArXiv": "",
      "CorpusId": "10020161",
      "DBLP": "conf/mwe/WittmannMW16",
      "DOI": "10.18653/v1/W16-1805",
      "MAG": "2513477335",
      "PubMed": "",
      "PubMedCentral": ""
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "S T U D Y P R O T O C O L Open Access",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "S T U D Y P R O T O C O L Open Access",
            "paragraphs": [
              "Effectiveness of biomarker-based exclusion of ventilator-acquired pneumonia to reduce antibiotic use (VAPrapid-2): study protocol for a randomised controlled trial Background Patients admitted to the intensive care unit (ICU) constitute a group vulnerable to healthcare-associated infection (HCAI), and consistent with that, antibiotic consumption in the ICU is considerably higher than in other hospital environments {{16152062}}. The growing global challenge of antimicrobial-resistance requires improved antibiotic stewardship. This judgment is, however, challenging in critically ill patients in whom clinical signs of infection are non-specific and where the consequences of missing a treatable infection may be significant.",
              "Ventilator-acquired pneumonia (VAP) is a common ICU HCAI and highlights the challenges of antibiotic stewardship. Few clinical features have a specificity of greater than 60 % {{not_in_s2orc}}{{8340831}}{{8178883}}, and infection is confirmed in approximately 40 % of patients with suspected VAP {{17391743}}. The majority of patients with suspected VAP have antibiotics started at the point of clinical suspicion and de-escalation or discontinuation of antibiotics is possible after 2-3 days, when culture information becomes available. During this period, a significant proportion of patients will receive antibiotics that may not be indicated, and furthermore, if antibiotics are not discontinued in light of negative cultures, a full course of unnecessary antibiotics may be administered.",
              "Addressing limitations in diagnostic methods for infections has the potential to improve antibiotic management by expediting the diagnostic process. Biomarkers of infection can act as rapid surrogate markers. In a recent multi-centre observational study {{17391743}}, we validated the findings of a single-centrederivation study {{5156253}}{{2617662}}, showing that bronchoalveolar lavage (BAL) fluid biomarkers could form a reliable test to exclude VAP. Low concentrations of BAL fluid interleukin 1-beta (IL-1\u03b2) consistently have a strong negative predictive value (NPV) in both the derivation and validation cohorts. Furthermore, in the validation cohort, a combination of IL-1\u03b2 and IL-8 could be used as a biomarker test to rule out VAP with high confidence, with a NPV of 1 and a post-test probability of 0 % (95 % confidence interval 0-9.2 %). These biomarkers are measured by cytometric bead array (CBA), which is a multi-plex, flow cytometric application that can be performed in approximately 6 hours, offering the potential for a rapid biomarker-based test in the ICU.",
              "The aim of this randomised trial is to determine if, in adult patients with suspected VAP, the use of the additional rule-out biomarker test will improve antibiotic management and reduce antimicrobial use in comparison to decision making based on microbiology results alone."
            ]
          },
          {
            "header": "Methods/Design",
            "paragraphs": [
              "This protocol outlines a multi-centre, prospective, controlled trial in which patients with suspected VAP are randomised 1:1 to a rapid biomarker-rule-out test in addition to standard care, compared to standard care alone (clinical judgment plus standard microbiological culture). The primary outcome measure is antibiotic-free days (AFD) in the 7 days following BAL. This clinical trial adheres to the Consolidated Standards of Reporting Trials statement (Fig. 1) {{5387743}} and principles of Good Clinical Practice."
            ]
          },
          {
            "header": "Study population",
            "paragraphs": [
              "Participants are being recruited from the ICUs of 17 National Health Service (NHS) Trusts in the UK, for a total of 22 ICUs. These ICUs cover a broad case mix of medical, surgical and trauma patients, who are representative of current practice across the NHS.",
              "Patients with suspected VAP who are at least 18 years of age, intubated and mechanically ventilated for 48 hours or more are considered eligible for inclusion in the trial. VAP is suspected based on the presence of a new or worsening chest X-ray or computed tomography (CT) changes consistent with pneumonia in the context of at least two of the following: temperature < 35\u00b0C or > 38\u00b0C; a white cell count of < 4 \u00d7 10 9 / L or > 11 \u00d7 10 9 / L; or purulent tracheal secretions {{14907563}}. Patients must also be considered suitable for early discontinuation of antibiotics by the clinical team (i.e. have no extra-pulmonary source of infection that mandates the use of continued antibiotics).",
              "Patients are excluded on the basis of previously published criteria {{39264424}} that predict poor tolerance of the bronchoscopy and BAL that have been previously applied in our studies {{17391743}}{{2617662}}. These are: PaO 2 < 8kPa on a FiO 2 > 0.7; positive end-expiratory pressure of > 15 cm H 2 O; peak airway pressure > 35 cm H 2 O; heart rate > 140 beats per minute; mean arterial pressure < 65 mmHg; bleeding diathesis including platelet count < 20 \u00d7 10 9 /L or international normalised ratio (INR) > 3; poorly controlled intracranial pressure (>20 mmHg); or ICU consultant deems the procedure not to be safe. Patients may only be randomised to the study once (i.e. they are excluded if they have had previous BAL as part of the study). Patients are also excluded if assent/ consent is declined.",
              "Co-enrolment is allowed with observational studies. Co-enrolment with interventional studies is allowed following consideration of any scientific or statistical interactions in accordance with current UK recommendations {{72495254}}."
            ]
          },
          {
            "header": "Biomarker assay and laboratory set up",
            "paragraphs": [
              "The biomarker assay is performed in six NHS or university laboratories that act as testing 'hubs'. Participating ICUs had to be within an expected travel time of 1.5 hours of a hub. BAL fluid samples are transported to the laboratory on ice immediately after sampling and measurement of BAL fluid IL-1\u03b2 and IL-8 by cytometric bead array is performed with minimal delay on arrival. IL-1\u03b2 and IL-8 are combined by logistic regression by the equation, established in the validation study {{17391743}}:",
              "VAP is excluded if the regression output falls below a defined level (-1.7616). All equipment, reagents and ongoing maintenance are supplied by the study's industry partner, Becton Dickinson Biosciences (Franklin Lakes, NJ, USA). Each hub has been issued an Accuri C6 flow cytometer. This is a benchtop flow cytometer designed for its ease of use. The biomarker assay and all Accuri standard operating procedures (SOPs) have been designed such that they can be carried out by healthcare service laboratory technicians with limited flow cytometry experience. Centralised training in the biomarker assay and Accuri C6 was provided by investigators and scientists from Newcastle University and Becton Dickinson.",
              "Accuri quality control tests are performed once a week by each hub, and these data are monitored by the flow cytometry core facility in Newcastle University. Technical support is provided by the investigators at Newcastle University and by Becton Dickinson."
            ]
          },
          {
            "header": "Intervention",
            "paragraphs": [
              "All patients enrolled in the trial have suspected VAP and undergo the same clinical procedures of bronchoscopy and BAL. The BAL is performed according to a previously described SOP {{17391743}}.",
              "Patients are randomised to have BAL samples analysed by either the biomarker test in addition to semiquantitative culture (the intervention arm) or semiquantitative cultures alone (the control arm). The intervention arm is referred to as the 'biomarker-guided recommendation on antibiotics' group and the control arm the 'routine use of antibiotics' group. All semiquantitative cultures are performed in a NHS or Public Health England laboratory and handled by a SOP in accordance with the UK Standards for Microbiological Investigation {{not_in_s2orc}}. VAP is confirmed by the widely used threshold of growth of a potential pathogen at > 10 4 colony-forming units per ml (CFU/ml) {{1369998}}.",
              "Biomarker results are reported to the clinical team by the technician using a standard script after approximately 6 hours. It is anticipated that all patients would have antibiotics started at the point of suspicion of VAP. In the event of a biomarker result that falls below the cut-off value, the clinical team is advised that VAP is excluded with high confidence and that early discontinuation of antibiotics is advised. If the biomarker value is above the threshold the clinical team is advised that VAP cannot be excluded and that standard care should continue."
            ]
          },
          {
            "header": "Risk to participants",
            "paragraphs": [
              "Patients enrolled in this trial are by definition critically unwell, and minimising the risk to these patients is of paramount importance. BAL is an established and widely used technique for sampling the alveolar regions in ICU patients {{1369998}}{{11640076}}. Not only do eligibility criteria exclude patients who would poorly tolerate a BAL, but patients can be excluded based on the clinicians' judgment of the risk profile.",
              "The second consideration of risk is around the early discontinuation of antibiotics. The risk exists that antibiotics may be incorrectly discontinued in the face of undetected infection (i.e. in the setting of a false negative biomarker test). The validation study demonstrated that the threshold for excluding VAP using BAL IL-1\u03b2 and IL-8 had an NPV of 1 which gives confidence in the test's performance, but with a 95 % confidence interval of 0.92-1.0 false negatives remaining possible. Decisions around antibiotic prescribing are not dictated by the trial protocol, and these ultimately remain at the discretion of the treating clinician. This allows the clinician to restart antibiotics if it is felt they were discontinued inappropriately. This minimises the risk to patients and also makes the trial more pragmatic, testing its use in 'real life' clinical practice."
            ]
          },
          {
            "header": "Primary outcome and sample size",
            "paragraphs": [
              "The primary outcome measure is the frequency distribution of AFD in the 7 days following BAL. This interval was used as this is the average reported duration of antibiotic therapy for suspected VAP in UK practice {{17287844}}. For the purposes of study design, AFD can be considered to be an integer value with patients in one of eight categories (0-7 AFD). Fewer days of antibiotic treatment will be detected as an increase in the proportion of patients with higher numbers of AFD and fewer patients having zero AFD. Antimicrobials delivered for prophylaxis will be excluded from analysis.",
              "The sample size is based on the frequency distribution of AFD in the 7 days after BAL in our previous validation study. This baseline distribution of AFD showed a skew towards patients with suspected VAP having few or no AFD (with this effect manifest both in those with ultimately confirmed VAP and those in whom BAL microbiology did not confirm VAP). We modelled changes in the distribution of AFD from baseline to a distribution with more patients having higher numbers of AFD. Table 1 gives examples of the different frequency distributions and their effect sizes. We judged a distribution that equates to effect sizes above 0.07 would represent a clinically important difference. By way of illustration, this would be equivalent to a change from a baseline median of 0 AFD (interquartile range 0-2.5 AFD) to a median of 1.5 AFD (interquartile range 0-3.5 AFD) under biomarker-guided treatment. An effect size of 0.0797 requires 90 patients per trial arm and allowing for a 15 % drop out, the total sample size is 210. This outcome measure and effect size was presented for national stakeholder peer review and judged to be appropriate (UK Critical Care Research Forum, July 2013). Figure 2 illustrates the changes in the AFD distribution between baseline and model 3 from Table 1."
            ]
          },
          {
            "header": "Secondary outcome measures",
            "paragraphs": [
              "Secondary outcome measures will include antibiotic days and AFD, expressed as continuous variables, at 7, 14 and 28 days; ventilator-free days at 28 days; 28-day mortality and ICU mortality; sequential organ failure assessment (SOFA) score at days 3, 7 and 14; duration of level 2 (high dependency unit) care, level 3 (intensive care unit) care and hospital stay; Clostridium difficile and MRSA infections up to hospital discharge, death or 56 days; and antibiotic-resistant pathogen cultures up to hospital discharge, death or 56 days. A healthcare-resource-utilisation analysis will be calculated from the duration of level 2 care, level 3 care and hospital stay up to discharge, death or 56 days.",
              "Since this is a trial of a complex intervention, a process evaluation will be carried out in parallel with this trial. This will evaluate the process of conducting the trial and will aim to determine reasons for potential discrepancies in the expected and observed trial outcome, as well as providing information on the potential implementation of the intervention after trial completion. Additional files outline the detailed study protocol for the process evaluation (see Additional files 1 and 2)."
            ]
          }
        ]
      },
      {
        "header": "Data collection",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Data collection",
            "paragraphs": [
              "Data are collected on the day of enrolment (day 0); day 3; day 7; day 14; day 28; and date of discharge, death or at 56 days. Clinical data collected include the age, gender, date of admission to the hospital and ICU, reason for admission to the hospital and ICU, functional comorbidities index, acute physiology and chronic health Table 1 Models of different frequency distribution of AFD. Standard care distribution is based on data obtained from our validation cohort. The different models demonstrate increasing shifts in the frequency distribution towards more AFD in the sample. These distributions are illustrative, and different proportions in each category could give the same effect size. AFD antibiotic-free days  . 2 Graphical representation of the shift in distribution in antibiotic-free days (AFD) between baseline and model 3 evaluation (APACHE) 2 score at admission, SOFA score, time from MV to suspected VAP, and pathogens cultured from BAL. Safety measures will be recorded relative to the bronchoscopy and BAL. These will include SaO 2 , heart rate, blood pressure and PaO 2 :FiO 2 . Biological data will include concentrations of IL-1\u03b2 and IL-8 in the BAL fluid for patients who are randomised to the intervention arm."
            ]
          },
          {
            "header": "Recruitment process and consent",
            "paragraphs": [
              "All patients on the participating ICUs are screened on weekdays for eligibility. Potentially eligible patients are discussed with the ICU consultant to determine the appropriateness of early discontinuation of antibiotics and whether any other safety concerns are present that would exclude the patient.",
              "Consent and assent procedures are in keeping with the legal framework of England, Northern Ireland (Mental Capacity Act, 2005) or Scotland (Adults with Incapacity (Scotland) Act, 2000) for consent/assent of adults without capacity. In England and Northern Ireland informed assent is obtained, where possible, following discussion with the patient's next of kin (personal consultee). Where a personal consultee is unavailable, assent is provided by a nominated consultee, usually the ICU consultant, providing they are not also a member of the research team. In circumstances where the next of kin are unable to attend the ICU promptly, eliciting their opinion is possible by telephone to inform nominated assent.",
              "In Scotland the patient's relative or welfare attorney provides the informed consent. If the patient's relative or welfare attorney is unable to attend the ICU, consent may be provided in a telephone conversation, providing a second member of staff witnesses the discussion.",
              "Patients who recover capacity will be approached to provide retrospective informed consent. The decision as to whether that patient has regained capacity will reside with the treating team. The patient will be given sufficient time to consider the trial information before providing their consent to continued trial involvement."
            ]
          },
          {
            "header": "Randomisation and blinding",
            "paragraphs": [
              "Once consent/assent is obtained, the clinical team informs the laboratory technician who will perform the biomarker test, and the technician then initiates randomisation through the Newcastle Clinical Trials Unit (NCTU). Randomisation is performed using a webbased randomisation service. Patients are randomised to the intervention arm or control arm in a 1:1 ratio by permuted blocks of variable length and stratified by site. The randomisation service generates either an instruction that the patient is randomised to 'biomarker-guided recommendation on antibiotics: analyse sample on arrival' or to 'routine use of antibiotics: do not analyse sample on arrival'. This message is emailed to the technician.",
              "The clinical team is initially blinded to the trial arm since all trial procedures are performed in all participants. Unblinding occurs when the biomarker result is called back to the clinical team after approximately 6 hours. To ensure consistency in unblinding, after this period, the clinical team are also informed if the patient was randomised to the control arm. The technician contacts the clinical team with the results according to local arrangements, which include contacting the on-call ICU consultant, ICU resident or the local principal investigator."
            ]
          },
          {
            "header": "Statistical analysis",
            "paragraphs": [
              "Baseline clinical data will be compared between trial arms for balance using graphical and summary statistics appropriate for the data type of each variable. No formal tests of equality will be carried out. The primary outcome measure will be analysed by a chi-squared test on a 2 \u00d7 8 table of study arm versus AFD categories. Secondary outcome measures will be analysed by fitting appropriate generalised linear models with intervention and centre as covariates. Link functions will be determined by the type of outcome variable. A sub-group analysis for patients with trauma or head injury will be included as will a sub-group analysis based on clinician assessment of likelihood of VAP. Data analysis will be performed on an intention-to-treat basis, although other exploratory analyses including per-protocol analysis will be considered. A per-protocol analysis will be performed excluding patients who were randomised to the biomarker-guided recommendation on antibiotics arm, but who had a technical issue with the assay and therefore defaulted to standard care.",
              "A within-trial cost analysis will be undertaken to assess the hospital resource use from the point of randomisation until hospital discharge or death, whichever occurs first, for a maximum of 56 days. Patient-level hospital resource use will be estimated from length of ICU stay and length of hospital stay. Multiple regression analyses will be performed to examine patient factors, which are potentially associated with costs. The robustness of the results will be evaluated using sensitivity analyses."
            ]
          },
          {
            "header": "Monitoring and adverse event reporting",
            "paragraphs": [
              "A Data Monitoring and Ethics Committee will have oversight of the trial. This is an independent body that has oversight of safety data. They will make recommendations to the sponsor as to whether the trial should progress, be modified or terminated.",
              "NCTU will monitor adherence to the trial protocol and completeness of the data collection. Adverse events (AE) and serious adverse events (SAE) that occur within 2 hours of BAL are reported to NCTU. Site investigators are able to report an AE/SAE outside of this period if they feel an AE/SAE is related to the study."
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "17391743"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": [
            "Limitations in diagnostic techniques to correctly rule-in or rule-out infections results in many patients receiving unnecessary antibiotics. Successfully validating the use of BAL fluid IL-1\u03b2 and IL-8 to exclude VAP has been a significant step forward for developing diagnostics in this area {{17391743}}. A rapid rule-out of VAP should allow for discontinuation of antibiotics on the day of suspicion of VAP and therefore improve antibiotic management. This randomised controlled trial aims to determine the clinical utility of a rapid biomarker-based test by measuring the antibiotic use, which is expressed as AFD, in the 7 days that follow BAL.",
            "Few biomarkers are used in the ICU to guide antibiotic management, and none specifically for VAP. Due to the novelty of the biomarker and the conduct of this trial in a complex clinical environment, regular education and reinforcement of the trial protocol is necessary with participating sites. Furthermore, the trial methodology accommodates the complexity of the intervention by including elements such as the process evaluation.",
            "This trial represents a potentially important step forward for novel diagnostics in the ICU. If effective in its primary outcome it could result in an important improvement in antimicrobial stewardship in this patient group. It also represents a challenging and complex trial of novel diagnostics in a difficult clinical space. The process of conducting this trial will provide valuable information to inform future trials."
          ]
        },
        {
          "header": "Trial status",
          "paragraphs": [
            "The trial is currently active in all participating sites. The first patient was recruited in December 2013, and the trial is due to complete in 2016. Abbreviations AE, adverse event; AFD, antibiotic-free days; APACHE, acute physiology and chronic health evaluation; BAL, bronchoalveolar lavage; CBA, cytometric bead array; CFU, colony-forming units; CT, computed tomography; HCAI, healthcareassociated infection; ICU, intensive care unit; IL-1\u03b2, interleukin-1 beta; IL-8, interleukin-8; MRSA, methicillin-resistant Staphylococcus aureus; NCTU, Newcastle Clinical Trials Unit; NHS, National Health Service; NPV, negative predictive value; SAE, serious adverse event; SOFA, sequential organ failure assessment; SOP, standard operating procedure; VAP, ventilator-acquired pneumonia"
          ]
        },
        {
          "header": "Additional files",
          "paragraphs": []
        },
        {
          "header": "Fig. 1",
          "paragraphs": []
        },
        {
          "header": "Additional file 1 :",
          "paragraphs": []
        }
      ]
    },
    "discussion_txt": "Limitations in diagnostic techniques to correctly rule-in or rule-out infections results in many patients receiving unnecessary antibiotics. Successfully validating the use of BAL fluid IL-1\u03b2 and IL-8 to exclude VAP has been a significant step forward for developing diagnostics in this area {{17391743}}. A rapid rule-out of VAP should allow for discontinuation of antibiotics on the day of suspicion of VAP and therefore improve antibiotic management. This randomised controlled trial aims to determine the clinical utility of a rapid biomarker-based test by measuring the antibiotic use, which is expressed as AFD, in the 7 days that follow BAL.Few biomarkers are used in the ICU to guide antibiotic management, and none specifically for VAP. Due to the novelty of the biomarker and the conduct of this trial in a complex clinical environment, regular education and reinforcement of the trial protocol is necessary with participating sites. Furthermore, the trial methodology accommodates the complexity of the intervention by including elements such as the process evaluation.This trial represents a potentially important step forward for novel diagnostics in the ICU. If effective in its primary outcome it could result in an important improvement in antimicrobial stewardship in this patient group. It also represents a challenging and complex trial of novel diagnostics in a difficult clinical space. The process of conducting this trial will provide valuable information to inform future trials.The trial is currently active in all participating sites. The first patient was recruited in December 2013, and the trial is due to complete in 2016. Abbreviations AE, adverse event; AFD, antibiotic-free days; APACHE, acute physiology and chronic health evaluation; BAL, bronchoalveolar lavage; CBA, cytometric bead array; CFU, colony-forming units; CT, computed tomography; HCAI, healthcareassociated infection; ICU, intensive care unit; IL-1\u03b2, interleukin-1 beta; IL-8, interleukin-8; MRSA, methicillin-resistant Staphylococcus aureus; NCTU, Newcastle Clinical Trials Unit; NHS, National Health Service; NPV, negative predictive value; SAE, serious adverse event; SOFA, sequential organ failure assessment; SOP, standard operating procedure; VAP, ventilator-acquired pneumonia",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "The abstract presents a prospective, multi-center, randomized, controlled trial to evaluate the use of biomarker-based diagnostics for the rapid exclusion of ventilator-acquired pneumonia (VAP). The study aims to determine whether a rapid biomarker-guided recommendation on antibiotics results in fewer antibiotics and improved antimicrobial management compared to routine use of antibiotics in patients with clinically suspected VAP.",
          "",
          "The study design is as follows: patients with clinically suspected VAP undergo bronchoalveolar lavage (BAL) and VAP is confirmed by growth of a potential pathogen at >104 colony-forming units per milliliter (CFU/ml). Patients are then randomized 1:1 to either a 'biomarker-guided recommendation on antibiotics' or 'routine use of antibiotics' group. In the biomarker-guided group, BAL fluid is tested for interleukin-1 beta (IL-1\u03b2) and interleukin-8 (IL-8) in addition to routine microbiology testing. The clinical teams are blinded to the intervention until 6 hours after randomization, when the biomarker results are reported to the clinician. The primary outcome is a change in the frequency distribution of antibiotic-free days (AFD) in the 7 days following BAL.",
          "",
          "The study has several secondary outcome measures, including antibiotic use at 14 and 28 days, ventilator-free days, 28-day mortality and ICU mortality, sequential organ failure assessment (SOFA) at days 3, 7, and 14, duration of stay in critical care and the hospital, antibiotic-associated infections, and antibiotic-resistant pathogen cultures up to hospital discharge, death or 56 days. A healthcare-resource-utilization analysis will also be calculated from the duration of critical care and hospital stay.",
          "",
          "The study will enroll 210 patients and will have 80% power to detect a clinically significant shift in the distribution of AFD towards more patients having fewer antibiotics. The results of this trial will provide valuable insights into the use of biomarker-based diagnostics for the rapid exclusion of VAP and the potential to improve antibiotic management in this patient population.",
          "",
          "In conclusion, the study design is well-justified and the outcome measures are relevant to the study question. The use of biomarker-based diagnostics for the rapid exclusion of VAP has the potential to improve antibiotic management and reduce the emergence of antibiotic resistance in this patient population. The results of this trial will be important for clinicians and researchers working in the field of critical care medicine."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.6289622187614441,
            "R": 0.6602491140365601,
            "F1": 0.64422607421875
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.587127685546875,
            "R": 0.6028414964675903,
            "F1": 0.5948808193206787
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          ""
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "**Scientific Discussion**",
          "",
          "The proposed trial is designed to investigate the impact of biomarker-based diagnostics"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.38704246282577515,
            "R": 0.6074835062026978,
            "F1": 0.4728321135044098
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.35919874906539917,
            "R": 0.5819487571716309,
            "F1": 0.4442135989665985
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10040435",
    "title": "Attitudes toward home-based malaria testing in rural and urban Sierra Leone",
    "externalids": {
      "ACL": "",
      "ArXiv": "",
      "CorpusId": "10040435",
      "DBLP": "",
      "DOI": "10.1186/s12936-015-0582-x",
      "MAG": "2145843270",
      "PubMed": "25880198",
      "PubMedCentral": "4334841"
    },
    "year": "2015",
    "level": "section",
    "sections": [
      {
        "header": "Background",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Background",
            "paragraphs": [
              "Presumptive treatment of suspected cases of malaria based on symptoms is common in many endemic areas. In Bo, Sierra Leone, more than half of adults who think they have malaria make presumptive self-diagnoses and then treat at home, usually with herbs or with pharmaceuticals purchased without prescriptions in local markets {{1437023}}. The 2013 Malaria Indicator Survey in Sierra Leone found that 29% of urban children and 41% of rural children were not taken to a healthcare facility when they had fevers {{not_in_s2orc}}. These statistics indicate a high rate of malaria treatment without testing. Treating suspected malaria without first confirming the presence of Plasmodium parasites may lead to a variety of negative individual and community health outcomes due to the misuse of anti-parasitic medications, money wasted on ineffective or unnecessary treatments, and delayed therapy for the actual cause of the fever when the illness is not due to malaria {{20700141}}. The World Health Organization and global health partnerships recommend the use of diagnostic testing to avoid unnecessary treatment {{not_in_s2orc}}. Initiatives such as 'test-treat-track' promote testing all persons suspected to have malaria, treating with antimalarial drugs only those who have a positive malaria test result, and tracking progress toward malaria control through health surveillance systems {{14536605}}.",
              "Two main types of malaria tests are currently available, microscopy and rapid diagnostic tests (RDTs). Microscopic diagnosis is based on visualizing Plasmodium parasites in stained and magnified blood smears. Although microscopy is still considered the gold standard for malaria testing because it shows the actual presence or absence of the parasites and allows the parasite load to be quantified {{33198901}}, microscopes are usually only available at clinical laboratories. Microscopy is also dependent on the skill of the laboratory technician, and the sensitivity and specificity of the test may vary considerably depending on the person performing the examination {{33198901}}{{10844924}}.",
              "RDTs do not visualize the parasite but instead detect antigens present in a small sample of blood applied to a test strip {{33198901}}{{10844924}}. Within about five to 20 minutes of applying the blood of a person with malaria to a test strip, one or more lines will appear in the window of the test kit indicating if parasitic antigens have been detected {{33198901}}{{10844924}}. One line usually indicates a negative test result, and two (or more) lines indicate the presence of the antigen {{33198901}}. The results may also indicate an invalid result, which would require the test to be re-administered with a new kit. RDTs are a type of point-of-care test (POCT) or point-of-need test {{30808505}}. While RDTs are most often used at clinical facilities, they can also be used in home and community settings by community health workers (CHWs) and community health volunteers (CHVs). POCTs are useful when they are cost-effective and accurate tools for providing rapid diagnostic results {{30808505}}. Studies in a variety of settings have demonstrated that malaria RDTs have adequately high sensitivity and specificity {{33198901}} and can be cost-effective when compared to microscopy {{17043489}}{{18750996}}{{9123380}}{{3054915}}.",
              "Although diagnosis prior to treatment is strongly encouraged by clinical guidelines, there are a variety of factors that limit access to and uptake of malaria testing. Cultural and community beliefs about blood and perceptions of healthcare providers may discourage testing {{6263749}}, lower-income households may not be able to afford testing or may not prioritize testing when budgeting for healthcare expenses {{20700141}}{{1219402}}, and rural residents may delay seeking care from the formal medical system because of the costs and time associated with travelling to a healthcare facility {{20889255}}{{35364155}}. The lack of physical or geographical access to testing is one that can be at least partly addressed by programmes offering community case management (CCM) of malaria. CCM may also overcome some cultural and economic barriers {{not_in_s2orc}}{{6084901}}.",
              "CCM programmes empower CHWs and CHVs to offer basic, low-cost healthcare services to households that might not otherwise have routine access to medical assistance. CHWs and CHVs participating in CCM programmes for malaria perform two key functions: using RDTs to confirm suspected malaria cases and then dispensing anti-malarial drugs to those who test positive for malaria with an RDT {{74007}}. The RDT kits used by CHWs and CHVs generally include disposable gloves, alcohol swabs, a lancet for drawing a small amount of blood, the test kit itself, any buffer solutions necessary for the test, and a small container for safely disposing of the lancet and other hazardous materials {{not_in_s2orc}}. CHWs and CHVs also maintain stocks of commonly needed medications. The ability to use RDTs outside of a laboratory-based setting may increase the number of febrile persons willing to take a malaria test.",
              "The purpose of this study was to use a cross-sectional population-based sampling approach to examine malaria testing practices, attitudes towards malaria testing, and preferences for testing location in rural and urban Bo, Sierra Leone."
            ]
          }
        ]
      },
      {
        "header": "Methods",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Methods",
            "paragraphs": []
          },
          {
            "header": "Study area",
            "paragraphs": [
              "This study was conducted in the city of Bo, Sierra Leone's second largest urban area, and in surrounding rural areas within the Bo District. Sierra Leone is located on the coast of West Africa, and Bo District is located in the nation's Southern Province. Falciparum malaria is endemic in Sierra Leone, and the country has one of the highest prevalence rates of malaria in the world {{not_in_s2orc}}. Malaria diagnosis (usually through microscopy rather than RDTs) and artemisinin-based combination therapy (ACT) are available free to users of all ages from public providers {{not_in_s2orc}}. There are few healthcare facilities in rural areas, but the city of Bo is home to a diversity of public and private outpatient clinics and several hospitals offering both inpatient and outpatient services {{not_in_s2orc}}{{1027998}}."
            ]
          },
          {
            "header": "Study population",
            "paragraphs": [
              "In 2009, Mercy Hospital Research Laboratory (MHRL) used a participatory geographic information system (GIS) process to map the entire city of Bo, including the locations of all residential, public and commercial structures {{14590349}}. The GIS has been regularly updated since then. This database enabled MHRL to use a geographybased sampling method to recruit households from Bo city for this study. A two-stage cluster (neighbourhoodbased) sampling method was used to identify urban participants. First, seven of 68 sections (neighbourhoods) in the city of Bo were randomly sampled, and then 30 households from each of those seven sections were randomly sampled for participation. In the rural areas, 23 rural communities (villages) were randomly sampled for participation from a list of the 1,266 villages in Bo District. Interviewers visited these communities to recruit participants. When fewer than 30 households were located within the community, interviewers visited residences at the periphery of the villages until they had contacted 30 households. In total, 824 of the 900 (91.6%) contacted households participated in the study, including 667 of 690 (96.7%) rural households and 157 of 210 (74.8%) urban households.",
              "In each of the 824 participating households, one adult representative (age 18 or older) was asked to participate in a study of attitudes and practices related to malaria testing. The interviewers preferentially recruited household members who said they were febrile at the time of data collection. In both the urban and rural areas, the interviewers alternated between asking to interview a female household member and requesting an interview with a male household member. If an adult of the preferred gender was not available, a household member of the opposite gender was interviewed instead. Responses to interviewer questions were recorded in an OpenData-Kit questionnaire installed on smartphones. All interviews were completed between 14 December, 2013 and 31 January, 2014."
            ]
          },
          {
            "header": "Questionnaire",
            "paragraphs": [
              "Each participant was asked whether he or she was currently febrile or without fever. Based on this response, the next questions related to either the current fever or to the most recent febrile illness the participant had experienced. The first set of questions asked whether a diagnostic test had been performed to determine the cause of the fever. Participants who did not complete a diagnostic test during the current or most recent febrile illness were asked a series of yes/no questions about their reasons for not seeking a microbiologicallyconfirmed diagnosis. The next set of questions asked whether the participant consulted a doctor or other healthcare professional about the fever and, if so, asked about the clinician's recommendations for testing. Participants were asked about their attitudes about homebased self-testing, CHV-assisted home-based testing and laboratory-based malaria testing. Data were also collected about demographics, socio-economic status and self-rated quality of life."
            ]
          },
          {
            "header": "Ethical considerations",
            "paragraphs": [
              "The study was approved by the institutional review boards of Njala University, George Mason University, the Liverpool School of Tropical Medicine, the US Naval Research Laboratory, and the Office of the Sierra Leone Ethics and Scientific Review Committee (SLESRC). Participation was voluntary, and no incentives or compensation for participation were provided. All participants provided a thumbprint as documentation of their informed consent."
            ]
          }
        ]
      },
      {
        "header": "Analysis",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Analysis",
            "paragraphs": [
              "The primary variable of interest for this analysis was a preference for home-based testing, whether conducted by self/family or by a CHV, over laboratory-based testing for suspected cases of malaria. Participants who answered both the \"self/family vs. laboratory\" and \"CHV vs. laboratory\" questions with a preference for homebased testing were considered to have a strong preference for home-based testing, and participants who answered both of these questions with a preference for laboratory-based testing were considered to have a strong preference for laboratory-based testing. Participants indicating no preference, those who would not take the test at either venue, and those who preferred home-based testing for one of the paired-comparison questions and laboratory-based testing for the other were not considered to have a strong preference for home-based or laboratory-based testing and were excluded from the analyses that focused on only participants with consistent preference for one of the two testing settings. Participant characteristics and testing preferences were compared using two-sided Pearson's Chi-squared tests. Because there were significant differences in the characteristics of rural and urban participants, most analyses were conducted separately by location. The statistical analysis was conducted using SPSS version 22 with a significance level of \u03b1 = 0.05."
            ]
          }
        ]
      },
      {
        "header": "Results",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Results",
            "paragraphs": [
              "The age and gender distributions of rural and urban participants were similar ( Table 1). The mean age of the participants was 40.9 years (standard deviation: 18.3), with half of the participants male and half female, as per the study design. However, there were significant socioeconomic differences by location, with rural residents reporting lower levels of education (p < 0.001), lower rates of literacy (p < 0.001) and less ownership of mobile phones (p < 0.001). Rural residents were also more likely than urban residents to rate their quality of life and their health status as poor (p < 0.01). Because of these significant differences by location, all analyses were conducted separately for rural and urban participants.",
              "Participant answers to the paired-comparison questions about preferred malaria testing location expressed a significant preference for home-based testing over laboratory-based testing in rural areas (Table 2). Among the 660 rural residents who responded to these questions, 68.8% (n = 454) preferred home-based testing by self/family or by a CHV over laboratory-based testing; {{not_in_s2orc}}.8% (131) preferred laboratory-based testing over home-based testing; 4.2% (28) rated laboratory-based  testing as better than CHV-assisted testing but worse than self/family testing; 2.6% (17) rated laboratory-based testing as better than self/family testing but worse than CHV-assisted testing; and 4.5% (30) indicated no preference or said they would decline all of these testing options. Among the 154 urban residents who responded, 38.3% (59) preferred home-based testing over laboratory-based testing and 43.5% (67) preferred laboratory-based testing over home-based testing. In both rural and urban areas, CHV-assisted home-based testing was preferred over self/ family-conducted home-based testing. In rural areas, 68.1% (450) preferred CHV-assisted testing compared to only 28.1% (186) who preferred self/family-conducted testing. In urban areas, 76.8% (119) preferred testing by a CHV and 21.3% (33) preferred testing by self or a family member.",
              "In total, 47.8% (n = 308/644) of the rural participants and 75.3% (116/154) of the urban participants said they usually consult with a healthcare provider about suspected malaria (Table 3). Among rural and urban participants, 37.1% (247/666) and 55.8% (87/156), respectively, reported having had a diagnostic test to determine the cause of the most recent febrile illness, 35.6% (231/649) and 62.7% (94/150) reported usually taking a diagnostic test to confirm suspected malaria, and 89.5% (578/646) and 88.2% (125/153) said that they usually bought medications or herbs to treat malaria without first consulting with a healthcare provider.",
              "Rates of preference for home-based testing and for laboratory-based testing were similar in rural and urban areas by gender (p = 0.488 and 0.713, respectively), age (p = 0.670 and 0.605), and years of education (p = 0.094 and 0.039, with urban residents with more education more likely to prefer laboratory-based testing). There was no difference in testing location preference by fever status at the time of the survey (p = 0.929). Among rural residents, a preference for home-based testing over laboratory-testing was especially high among those who were not comfortable reading a newspaper (p = 0.006), those not owning a mobile phone (p = 0.031), those who reported usually self-treating suspected malaria rather than consulting with a healthcare provider (p < 0.001), and those who believe that adult fevers are always caused by malaria (p < 0.001). Among urban residents, a preference for home-based testing over laboratorytesting was especially high among those who were able to read English (p = 0.003) and comfortable reading a newspaper (p = 0.001), those who did not usually take a diagnostic test to confirm suspected malaria (p < 0.001), and those who reported being very confident about their ability to perform a malaria self-test after training (p < 0.001). Those who indicated a preference for home-based testing were significantly more likely than those preferring laboratory-based testing to say that they would be willing to pay extra for the option of a homebased test, and this was found in both rural (p < 0.001) and urban (p = 0.005) areas.",
              "In total, 31.7% (198/625) of rural participants and 27.3% (42/154) of urban participants said they were very confident they could perform a malaria test on themselves or a family member without assistance. Among those indicating a preference for self/family-conducted home-based testing over CHV-assisted testing, 74.2% (138/186) of rural residents and 33.3% (11/33) of urban residents were very confident in their ability to conduct a test on themselves or a family member. Reported selfconfidence was lower among participants indicating a preference for a home-based tested administered by a CHV rather than self/family, with only 14.0% (60/428) of rural residents and 26.1% (31/119) or urban residents with this preference saying they were very confident in their self-testing ability. In total, 50.1% (314/627) of the rural participants were very confident they could perform a test if they had been trained on how to use a test kit, including 88.7% (165/186) with a preference for self/family-conducted home-based testing and 34.0% (146/430) with a preference for testing administered by a CHV. Among the urban participants, 61.7% (95/154) were very confident in their testing abilities if trained, including 75.8% (25/33) with a preference for self/familyconducted home-based testing and 58.8% (70/119) with a preference for testing administered by a CHV.",
              "Among the 488 participants who did not seek a diagnostic test for their most recent febrile illness, 330 (67.6%) preferred a home-based test and 95 (19.5%) preferred a laboratory-based test ( Table 4). The most common reasons for not seeking testing were similar for both testing preference and location groups: not wanting to spend money on the test (68.0%), waiting to see if the fever resolved within a few days (45.0%), not wanting to travel to a clinical facility for a test (41.0%) and assuming that a formal diagnosis was not needed because adults could presumptively diagnose the cause of the fever (37.4%). Those with a preference for home-based testing were more likely than those with a preference for laboratory-based testing to say that they did not need a test because they already knew the cause of the fever (p = 0.025). Urban non-testers were significantly more likely than rural non-testers to assume that they already knew the cause of the fever (p = <0.001) and to say that they did not want to wait for test results (p = 0.010). A dislike of blood draws was not a common reason not to seek testing (15.6%)."
            ]
          }
        ]
      },
      {
        "header": "Conclusions",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Conclusions",
            "paragraphs": [
              "Expanded access to CCM might improve access to diagnosis and appropriate, timely and cost-efficient treatment. Existing CHV networks in Sierra Leone could provide a foundation for intensifying CCM. The success of home-based testing in these communities would likely be dependent on (1) a high level of respect for and trust in new and existing CHVs by the communities they support; (2) the availability of a strong network of CHVs who are confident in their own RDT skills and are able to conduct tests themselves as well as teach others to conduct tests; (3) sufficient public health resources to support 'train-the-trainer' sessions to teach CHVs how to coach their neighbours through their first self/family malaria tests; and, (4) a competitive price point for home test kits when compared to the out-of-pocket costs of laboratory-based testing.",
              "The possible importance of home-based self-testing has been highlighted by the recent Ebola outbreak. In the months after this survey was conducted, hundreds of cases of Ebola occurred in Bo district {{26074854}}{{not_in_s2orc}}. Many clinics and hospitals in Bo closed their doors rather than risk exposing their staff to the Ebola virus, and others were forced to temporarily close because of quarantines. Laboratory-based testing for febrile illnesses became more difficult to conduct and more costly due to the need to assume that all biological specimens could be from a patient with Ebola, even when the most likely diagnosis was something more common and much less fatal. Quarantines meant that some CHVs in rural areas could not travel from their home villages to neighbouring ones or stopped volunteering because they feared contracting Ebola. Thus, access to routine medical care, including diagnosis and treatment of common infections such as malaria, decreased significantly, especially in rural areas that already had limited access to health services. Home-based testing that allows trained individuals with suspected malaria to test themselves and family members at home, thus limiting the exposure of others to their blood, could improve the home-based care that has become necessary for many households as a result of the strain Ebola put on the entire healthcare system of Sierra Leone."
            ]
          },
          {
            "header": "Table 1",
            "paragraphs": []
          },
          {
            "header": "Table 2",
            "paragraphs": []
          },
          {
            "header": "Table 4",
            "paragraphs": []
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "10253380",
        "1841654",
        "18750996",
        "30384887",
        "6084901",
        "7359381",
        "74007"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": [
            "The participants in this study, especially those living in rural areas, expressed a high level of interest in homebased testing for malaria. A recent systematic review of Table 3 Characteristics of participants who indicated a consistent preference for home-based testing (by self/family or CHV) or laboratory-based testing 27 studies of CCM of malaria found consistent evidence that trained CHWs could successfully conduct RDTs and use the test results to recommend appropriate treatment {{74007}}. Studies in which community members are part of the selection process for CHVs are particularly successful in increasing access to testing {{18750996}}{{6084901}}. The primary concerns about CCM of malaria are some CHWs recommending anti-malarial medications following negative malaria tests and many febrile individuals not following through on recommended clinical consultations after negative test results {{74007}}{{10253380}}. Few studies have examined the use of home-based RDTs conducted by family members trained and supervised by CHVs rather than by the CHVs themselves, but this study suggested an openness to self/family testing in addition to CHV testing. Self/family testing with RDTs prepurchased by the household might increase malariatesting access in small farming villages that are located farther than easy walking distance from a community with a trained CHV. Home-based malaria testing, often with the assistance of a CHV, has been successfully implemented in other sub-Saharan African communities. In the rural Iganga district of Uganda, community members expressed a high level of trust in CHVs who were trained to conduct malaria RDTs and to recommend appropriate malaria treatment {{7359381}}. In villages in the South Kordofan State of Sudan, the use of CHV-conducted RDTs significantly increased access to malaria tests and malaria treatment, especially during the rainy season when travel to a clinic was often impossible {{6084901}}. In two provinces in Zambia, CHVs had a very high rate of correct recommendations for malaria treatment or referral to a healthcare facility after conducting in-home RDTs {{18750996}}. Given the substantial interest in home-based malaria testing in rural Bo district in Sierra Leone, it seems likely that CHVs in these communities could successfully test and treat malaria in homes, and perhaps even train willing and confident family members to conduct RDTs in their own households, making appropriate referrals for hospital care when necessary.",
            "The most commonly reported barriers to the uptake of home-based testing for malaria include concerns about infection transmission during blood draws {{7359381}}{{30384887}} and concerns about reliable access to RDTs for homebased testing use {{18750996}}{{10253380}}{{1841654}}. Any programme for homebased testing would need to include clear instructions about how to safely dispose of lancets and test kits. CHVs could be encouraged to collect used kits they had distributed and to ensure that they were processed as medical waste. Consistent access to RDTs would need to be supported through community partnerships with local hospitals and government health officials.",
            "This study in Sierra Leone used a cross-sectional approach with self-reported answers about febrile status and testing preferences. Participants were not asked to try conducting a malaria test on themselves or a family member, so all answers about willingness to conduct a test and confidence about the ability to conduct a test are hypothetical. This willingness-to-test survey will need to be followed by field tests of RDTs by CHVs and CHV-trained family members. Should these tests prove to be acceptable to the community, safe and valid, the programme could be expanded to include RDTs for other locally-important infections."
          ]
        }
      ]
    },
    "discussion_txt": "The participants in this study, especially those living in rural areas, expressed a high level of interest in homebased testing for malaria. A recent systematic review of Table 3 Characteristics of participants who indicated a consistent preference for home-based testing (by self/family or CHV) or laboratory-based testing 27 studies of CCM of malaria found consistent evidence that trained CHWs could successfully conduct RDTs and use the test results to recommend appropriate treatment {{74007}}. Studies in which community members are part of the selection process for CHVs are particularly successful in increasing access to testing {{18750996}}{{6084901}}. The primary concerns about CCM of malaria are some CHWs recommending anti-malarial medications following negative malaria tests and many febrile individuals not following through on recommended clinical consultations after negative test results {{74007}}{{10253380}}. Few studies have examined the use of home-based RDTs conducted by family members trained and supervised by CHVs rather than by the CHVs themselves, but this study suggested an openness to self/family testing in addition to CHV testing. Self/family testing with RDTs prepurchased by the household might increase malariatesting access in small farming villages that are located farther than easy walking distance from a community with a trained CHV. Home-based malaria testing, often with the assistance of a CHV, has been successfully implemented in other sub-Saharan African communities. In the rural Iganga district of Uganda, community members expressed a high level of trust in CHVs who were trained to conduct malaria RDTs and to recommend appropriate malaria treatment {{7359381}}. In villages in the South Kordofan State of Sudan, the use of CHV-conducted RDTs significantly increased access to malaria tests and malaria treatment, especially during the rainy season when travel to a clinic was often impossible {{6084901}}. In two provinces in Zambia, CHVs had a very high rate of correct recommendations for malaria treatment or referral to a healthcare facility after conducting in-home RDTs {{18750996}}. Given the substantial interest in home-based malaria testing in rural Bo district in Sierra Leone, it seems likely that CHVs in these communities could successfully test and treat malaria in homes, and perhaps even train willing and confident family members to conduct RDTs in their own households, making appropriate referrals for hospital care when necessary.The most commonly reported barriers to the uptake of home-based testing for malaria include concerns about infection transmission during blood draws {{7359381}}{{30384887}} and concerns about reliable access to RDTs for homebased testing use {{18750996}}{{10253380}}{{1841654}}. Any programme for homebased testing would need to include clear instructions about how to safely dispose of lancets and test kits. CHVs could be encouraged to collect used kits they had distributed and to ensure that they were processed as medical waste. Consistent access to RDTs would need to be supported through community partnerships with local hospitals and government health officials.This study in Sierra Leone used a cross-sectional approach with self-reported answers about febrile status and testing preferences. Participants were not asked to try conducting a malaria test on themselves or a family member, so all answers about willingness to conduct a test and confidence about the ability to conduct a test are hypothetical. This willingness-to-test survey will need to be followed by field tests of RDTs by CHVs and CHV-trained family members. Should these tests prove to be acceptable to the community, safe and valid, the programme could be expanded to include RDTs for other locally-important infections.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "Thank you for providing the abstract. Based on the findings of this study, it appears that there is a preference among rural and urban residents in Bo, Sierra Leone for home-based malaria testing, rather than traveling to a clinical facility for laboratory-based testing. Specifically, among rural residents, 69% preferred a self/family- or community health volunteer (CHV)-conducted home-based malaria test, while among urban residents, 38% preferred this type of test. Additionally, if offered a home-based test, 28% of rural residents and 68% of urban residents would prefer a CHV-assisted test. These findings suggest that there is a strong interest in and willingness to use home-based malaria testing in this population.",
          "",
          "The study also found that the most common reasons for not seeking malaria testing were the cost of testing, waiting to see if the fever resolved on its own, and not wanting to travel to a clinical facility for a test. These findings highlight the need to address these barriers in order to increase the uptake of malaria testing in this population.",
          "",
          "Notably, the study found that 36% of rural and 63% of urban residents reported usually taking a diagnostic test to confirm suspected malaria. This suggests that there is a need for increased awareness and education about the importance of malaria testing, particularly in rural areas where access to healthcare facilities may be more limited.",
          "",
          "The study also assessed the confidence of participants in performing a malaria test on themselves or a family member without assistance, as well as their willingness to pay for a home-based test. Findings showed that 50% of rural and 62% of urban participants were very confident they could perform a malaria test after training, and 56% of rural and 33% of urban participants said they would pay more for a home-based test than a laboratory-based test. These findings suggest that training and education on malaria testing may be effective in increasing confidence and willingness to use home-based testing.",
          "",
          "Overall, the study suggests that expanding community case management of malaria to include home testing by CHVs and family members may increase the proportion of individuals with febrile illnesses who confirm a positive diagnosis prior to initiating treatment. This approach has the potential to improve access to malaria testing, particularly in rural areas where healthcare facilities may be more limited. However, it is important to address the barriers to malaria testing identified in the study, such as cost and lack of awareness, in order to maximize the impact of this approach."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.6008744835853577,
            "R": 0.6523211002349854,
            "F1": 0.6255418062210083
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.5622923374176025,
            "R": 0.6037204265594482,
            "F1": 0.5822704434394836
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          ""
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "The study's findings suggest that rural residents are more likely to prefer home-based malaria testing than urban"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.36990657448768616,
            "R": 0.6113429665565491,
            "F1": 0.4609220623970032
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.3580303192138672,
            "R": 0.5711299777030945,
            "F1": 0.440143346786499
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10042502",
    "title": "Graph-based Clustering of Synonym Senses for German Particle Verbs",
    "externalids": {
      "ACL": "W16-1805",
      "ArXiv": "",
      "CorpusId": "10020161",
      "DBLP": "conf/mwe/WittmannMW16",
      "DOI": "10.18653/v1/W16-1805",
      "MAG": "2513477335",
      "PubMed": "",
      "PubMedCentral": ""
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "Introduction",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Introduction",
            "paragraphs": [
              "We seek to find interactive proofs between quantum provers and classical verifiers, both limited to polynomial-time calculations. That is to say, we would like to have a procedure where a classical computer (the \"verifier\"), limited to a polynomial number of operations, can query a quantum computer (the \"prover\"), also limited to a polynomial number of operations, and tap into its resources in order to perform some computation. Additionally, if the verifier unhappily interacts with a malicious quantum computer it should be able to detect this and abort the calculation, even if the prover has unlimited computational resources. To make the challenge less trivial, there should exist interactive proofs for problems that are harder than the verifier could solve by itself and ideally there should exist interactive proofs for any problem that the prover can solve by itself.",
              "This problem is interesting for a variety of reasons. First, as a complexity theoretic question it has obvious value in further developing the theory of how powerful quantum computers are. From a practical computing point of view, it would be nice to know whether it would be possible to have cheap classical computers interact with large (and presumably more expensive) quantum \"servers,\" paying for services as required. Of course the users would like to know that they get their money's worth, and interactive computations can confirm this. As well, from an experimental point of view, interactive proofs can be used to verify the operation of some experimental apparatus. This is of particular importance for quantum experiments since it may well be that, for large experiments, it is impossible (in practical terms) to classically compute what the predictions of the quantum model are, leading to questions about the falsifiability of the quantum formalism {{not_in_s2orc}}.",
              "Very little is known about this problem as stated. Clearly the set of languages recognizable by a poly-time classical verifier and poly-time quantum prover lies somewhere between P and BQP since on one hand the verifier can ignore the prover, and on the other hand the verifier and honest prover together form a poly-time quantum machine. As well, there do exist interactive proofs for all of BQP since BQP \u2286 PSPACE and PSPACE = IP {{not_in_s2orc}}{{32614901}}, but the known constructions require the prover to solve PSPACE-complete problems. Constructions for particular problems are known ( {{15800770}} for example) and of course anything in NP has a trivial interactive proof, but beyond this little is known.",
              "Current techniques [ABG + 07, BLM + 09, MMMO06, MY04, MY98, MYS12,",
              "McK10b, MS12, PAB + 09, PAM + 10, {{not_in_s2orc}} for probing the behaviour of adversarial quantum systems all rely on entanglement and hence in order to make use of them we must introduce more provers. Reichardt et al. {{not_in_s2orc}} considered the case of two provers. Here we will consider the case of a polynomial number of provers, but each limited to a single operation, and show that we can recognize all of BQP with this model. Our construction uses two major components. One is self-testing and the other is measurement-based quantum computation. Self-testing allows us to confirm that the provers hold onto a graph state and perform certain measurements on this state when instructed to do so. Measurement-based quantum computation allows us to use these verified resources to perform the desired calculation."
            ]
          }
        ]
      },
      {
        "header": "Previous work",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Previous work",
            "paragraphs": [
              "Self-testing was introduced by Mayers and Yao {{16531904}}{{14069874}}. Their goal was to establish that a pair of devices share a maximally entangled pair of qubits, and that the devices implement some specific measurements, all while making a minimum of assumptions on the devices. Specifically they make no assumptions about the dimension of the Hilbert space associated with the devices. Meanwhile, van Dam et al. {{358133}} considered testing gates in the context of known Hilbert space dimension. Magniez et al. {{not_in_s2orc}} combined the two approaches, allowing testing of entire quantum circuits. Further refinements, including simpler proof techniques and extension to complex measurements appear in {{not_in_s2orc}} and {{44939389}}. Self-testing of graph states, critical for our application, appears in {{34099972}}. Miller and Shi {{not_in_s2orc}} also give a general construction for self-testing states based on any XOR game.",
              "These works all require additional assumptions. In particular, they assume that devices can be used repeatedly in an independent and identical manner in order to gather necessary statistics. As well, {{not_in_s2orc}} assumes that certain states are in a product form. McKague and Magniez (in preparation) remove these assumptions for quantum circuits using techniques similar to those used here.",
              "Stemming from a different heritage, Broadbent et al. {{650251}} considered a semi-quantum verifier who only prepares single qubit states, and a fully quantum prover. They give a construction for an interactive proof for any language in BQP. Additionally, they describe (without rigorous proof) an ex-tension using two quantum provers and a classical verifier. Their construction uses measurement-based quantum computation. Aharonov et al. {{not_in_s2orc}} also describe a semi-quantum protocol using a constant sized quantum verifier and a polynomial-time quantum prover.",
              "In the context of quantum cryptography, Ac\u00edn et al.",
              "[ABG + 07] introduced device independent quantum key distribution. This model is very similar to that used here. However, rather than computation the goal is to expand a private shared key. From a physics perspective, {{not_in_s2orc}} [BLM + 09] and McKague et al. {{118535156}} consider self-testing type entanglement tests from the perspective of Bell inequalities.",
              "Most recently, Reichardt et al. {{not_in_s2orc}} proved a very general result allowing two non-communicating quantum provers along with a classical verifier to recognize all of BQP. The core of their result is a self-test, using only two provers, for multiple EPR pairs and measurements. Using this tool they show how to test individual gates and perform measurements via teleportation. Finally, they combine the results to give an interactive proof for entire quantum circuits.",
              "Measurement-based quantum computation, also known as one-way quantum computation or graph state computation, was introduced by Raussendorf and Briegel {{8839192}}{{6197709}}. In this model of computation we begin with a graph state and perform measurements on each vertex, with the sequence of vertices and the measurement bases used determined by the calculation we wish to make. The outcome of the calculation is then derived from the measurement outcomes. One important aspect of the measurements is that they are adaptive -the measurement basis for a particular vertex can depend on the outcomes of measurements on previous vertices. This allows us to perform any calculation in BQP. The particular variety of graph-state computation that we use is due to Mhalla and Perdrix {{not_in_s2orc}}. The advantage of this model is that it requires measurements in the X-Z plane only."
            ]
          },
          {
            "header": "Contributions",
            "paragraphs": [
              "We make several important contributions. First, we modify the proof for the graph state self-test from {{34099972}}, allowing a tighter error analysis. as in {{34099972}}. This exponential improvement in the error scaling in n makes it possible to self-test with a polynomial number of trials to achieve a constant error. We also analyse the error in the case of adaptive measurements, which are required for measurement-based quantum computing. Additionally we extend the graph state test to X-Z plane measurements in order to achieve universal computation. Finally we show how to use the self-test in order to test the provers for honesty in the interactive proof scenario. Combining this test for honesty with measurement-based quantum computation we achieve the following theorem:",
              "Theorem 1. For every language L \u2208 BQP and input x there exists a poly(|x|)time verifier V which interacts with a poly(|x|) number of non-communicating quantum provers such that",
              "\u2022 If x \u2208 L then there exists 1 a set of honest quantum provers, each of which performs a single operation, for which V accepts with probability at least c = 2/3.",
              "\u2022 If x / \u2208 L then, for any set of provers, V accepts with probability no more than s = 1/3.",
              "Along the way we we also prove several results which may be of independent interest. In particular our error analysis for triangular cluster states can be applied to general graph states and stabilizer states enabling self-testing of these states with robust error bounds. As well, our error bounds for adaptive measurements are quite general, applying to general quantum circuits which incorporate the untrusted measurements performed by the provers.",
              "Compared with the result of Reichardt et al. {{not_in_s2orc}} our contribution is to provide a different construction with different underlying computational model, that of measurement-based quantum computation. While they use a constant number of provers, each of which runs in polynomial time, we use a polynomial number of provers, each of which runs in constant time (indeed, each prover only performs a single measurement). The advantage of our technique is that, since only measurements are used, there is no need for any process tomography. As well, the provers are very easy to implement, requiring only the ability to measure in four different bases (once an appropriate graph state is prepared). Finally, there is a very nice conceptual advantage, which is that the measurement-based calculation that is performed is exactly what would be done with trusted devices, whereas the Reichardt et al. construction requires qubits to be teleported between the two provers at each gate."
            ]
          },
          {
            "header": "Overview of construction",
            "paragraphs": [
              "We can divide our interactive proof into two distinct units: the calculation and the test for honesty. The calculation is exactly the same measurementbased quantum computation that would be performed for trusted devices. The test for honesty is derived from self-testing.",
              "We give some technical details of measurement-based quantum computation in section 2.1. The procedure can be summarized as:",
              "1. Prepare a universal graph state 2. Perform measurements to obtain a computation-specific graph state 3. Measure vertices in sequence, adapting bases according to outcomes from previous measurements"
            ]
          },
          {
            "header": "Calculate final outcome",
            "paragraphs": [
              "In order to perform the computation we need the provers to share a graph state and be able to measure vertices. The verifier performs all the classical computation, including deriving the measurement patterns, the required graph state, and the final outcome.",
              "Our main contributions lie in constructing a test for honesty. Here we must define some test such that if the provers were to cheat on the calculation then they will fail the test. Our test for honesty is based on the graph state self-test, originally presented in {{34099972}}. It allows the verifier to establish that the provers have access to high quality copies of the desired graph state and X and Z Pauli measurements. We give details for this test, including our improved proof in section 3.1.",
              "In addition, for the measurement-based quantum computation we also need measurements covering the entire X-Z plane. This is a simple extension of the graph-state test, which we present in section 3.2.",
              "The graph-state test, with extensions, define a set of subtests, each of which the provers must pass. To administer the entire test, the verifier just chooses one of these subtests at random. If the provers actually hold the required graph state and perform the measurements faithfully then they will pass the test with high probability, and if their behaviour deviates too much from the honest provers then they will pass with a lower probability. The gap is 1/poly(n) for a constant error bound and is calculated in section 3.4.",
              "With all of this in place we obtain a simple statement: if the provers deviate from the honest behaviour by more than \u03b4 (see section 2.4 for a definition), then they will pass the test with probability at most c test \u2212\u01eb, where \u01eb is a function of \u03b4 and c test is the probability of honest provers passing the test. Hence if the provers attempt to cheat we will catch them. The details are given in section 3.4.",
              "Having shown how to test whether the provers are honest, and how to perform the desired calculation, we must put these two components together to form the interactive proof. The structure is quite simple: randomly decide whether to check for honesty or perform the calculation and then proceed. The critical observation is that, for an individual prover, the queries received look the same whether the verifier is testing or calculating. More specifically, every query that appears as part of a calculation also appears as part of the test for honesty. Hence the test for honesty catches provers who wish to cheat on the calculation.",
              "The final technical piece of the puzzle is to determine with what probability to test for honesty. We give the derivation in section 4."
            ]
          },
          {
            "header": "Technical introduction",
            "paragraphs": [
              "In this section we present some notation and definitions used in the construction and proof. Further technical results are collected in appendix A for convenience."
            ]
          },
          {
            "header": "Measurement-based Quantum Computation",
            "paragraphs": [
              "Here we give a general overview of measurement-based quantum computation. Our goal is to provide sufficient background for readers to understand the major features of measurement-based quantum computation. For more detail we refer the reader to {{8839192}}{{6197709}}.",
              "Let us start with a basic teleportation circuit as in figure 1. Rather than performing entanglement swapping with an EPR pair held in memory, as in the usual case, we entangle the input and output qubits directly using a CTRL-X gate. The classical result of the measurement in the X basis is used to control a Z gate, which applies a necessary correction. Direct calculation shows that the input state appears in the output register after the circuit is applied. In the second circuit in figure 1, we convert the CTRL-X gate to a CTRL-Z gate and two Hadamard gates. In the third circuit in figure 1, the left Hadamard simply changes the initial state from |0 to |+ . We move the right Hadamard past the Z correction, which then becomes an X correction gate. Figure 1: Three equivalent basic teleportation circuits. In the second circuit the CTRL-X gate is replaced with a CTRL-Z gate sandwiched between two Hadamard gates. In the third circuit the left Hadamard gate changes |0 to |+ and the right Hadamard gate moves past the Z correction, changing it to an X. Now suppose that we apply a unitary U to the qubit as in figure 2. For this construction we suppose that U(\u03b8) = exp( i\u03b8Z 2 ) so that it commutes with the CTRL-Z as in the second circuit of figure 2. Now we can see U as a modification of the measurement basis as in the final circuit. Since we originally measured in the X basis the new measurement basis will be in the X-Y plane of the Bloch sphere: U \u2020 XU = R(\u03b8) = cos \u03b8 X + sin \u03b8 Y . In the second circuit the fact that U(\u03b8) = exp i \u03b8Z 2 is diagonal means that it commutes with the CTRL-Z gate. In the third circuit the U(\u03b8) gate has modified the measurement basis to R(\u03b8) = cos \u03b8 X + sin \u03b8 Y .",
              "Next we consider how multiple teleportations work together. First we consider the case of two cascaded teleportations as in figure 3. Using measurement angles \u03b8 1 and \u03b8 2 , the overall unitary applied by the circuit is HU(\u03b8 2 )HU(\u03b8 1 ). In the second circuit of figure 3 we have moved the second CTRL-Z gate, used to entangled the second and third qubits together, to the left past the X correction on the second qubit. This induces a Z correction on the third qubit, controlled along with the X correction. Finally, in the third circuit we incorporate the X correction into the measurement angle on the second qubit. Indeed, since XR(\u03b8)X = R(\u2212\u03b8), the angle \u03b8 2 becomes \u2212\u03b8 2 whenever an X correction is needed.",
              "We have seen how to convert X corrections into changes in the measurement angle. Z corrections are even easier to apply. Since ZR(\u03b8)Z = \u2212R(\u03b8), a Z correction corresponds to simply inverting the output of a measurement. Figure 3: Two cascaded teleportations. The first circuit teleports the first qubit to the third, applying HU(\u03b8 2 )HU(\u03b8 1 ). In the second circuit we have moved the CTRL-Z to the left past the X correction, inducing a Z correction on the third qubit, but allowing all the CTRL-Z gates to be applied before any measurements are made. Finally, since XR(\u03b8)X = R(\u2212\u03b8) the X correction can be omitted in favour of a change of measurement basis. Figure 4 shows how X and Z corrections together modify the behaviour of the measurement. So far our construction has the following features: we can apply a sequence of unitaries HU(\u03b8 n ) . . . HU(\u03b8 1 ) to a qubit by repeatedly teleporting the qubit and varying the measurement angle used in the teleportation. The necessary corrections from the teleportation can be incorporated into subsequent measurement angles and outcomes, and all the entangling CTRL-Z Figure 4: Incorporating X and Z corrections into measurements. We have X and Z corrections according to some previous measurement results x, z \u2208 {\u00b11}. The X correction is incorporated into the measurement as a change in the angle. The Z correction is incorporated by flipping the outcome of the measurement.",
              "gates can be pushed to the start of the procedure. Hence we can perform a single qubit circuit by first building a large entangled state using |+ states and CTRL-Z gates, and then measuring the qubits in sequence, adapting measurement angles as we go. Note that the gates HU(\u03b8) form a universal set.",
              "In order to perform general circuits we need one more piece of the puzzle, which is two-qubit gates. In this case we obtain universality by including CTRL-Z gates. These can be applied at any time during the circuit and appear as additional CTRL-Z gates on target qubits when we translate into the teleportation scheme. These can be treated similarly to the CTRL-Z gates which are used to entangle input and output qubits for teleportation. In particular, we can push the CTRL-Z gates back to the beginning of the circuit, past X and Z corrections. This induces extra corrections which must be taken into account on subsequent measurements. Now we have the complete picture. A calculation begins by preparing many |+ states and entangling them with CTRL-Z gates. Then they are measured one at a time, and measurements are adjusted to incorporate X and Z corrections as required.",
              "The initial state, prepared by applying CTRL-Z gates to qubits in the |+ state, is called a graph state and will play an important role in our results here. Our construction will use a slightly different model of measurement-based quantum computation. Although the usual and most easily understood method utilises measurements in the X-Y plane, we will instead use a different model, due to Mahalla and Perdrix {{not_in_s2orc}}, which requires only X-Z plane measurements. In particular they prove the following theorem:",
              "Theorem 2 (Mahalla and Perdrix {{not_in_s2orc}}). Triangular cluster states are universal resources for measurement-based computation based on X-Z plane measurements.",
              "Triangular cluster states are graph states where the underlying graph is a triangular lattice. As we shall see, these particular graph states are particularly easy to self-test since every vertex is in a triangle. The proof of their theorem consists of two parts: showing that triangular cluster states can be converted into other graph states using measurements alone, and showing that X-Z measurements suffice for universal computation. The details of the proof are not important for our results here. What is important is that their construction introduces a small overhead, so that a given quantum circuit gets translated into a graph state with size polynomial in the size of the original circuit."
            ]
          },
          {
            "header": "Operators, isometries, bit strings",
            "paragraphs": [
              "We will frequently deal with a tensor product of operators over several subsystems. To make this easier we use the following notation: Definition 1. Given some collection of operators {M j : j = 1 . . . n} with M j operating on the j-th subsystem, and a vector x \u2208 {0, 1} n define",
              "(1)",
              "This notation is quite frequently used with Pauli operators, but here we do not assume that the M j operators are all the same. Instead, we merely suppose that there is some common label \"M\", which may refer to different operators on different subsystems.",
              "Another set of objects that we will deal with frequently is isometries.",
              "Definition 2. An isometry is a linear operator \u03a6 : X \u2192 Y that preserves inner products.",
              "Isometries are a natural generalization of unitaries where the image space of \u2295 is not necessarily the same as X , and may in general have a larger dimension. As a concrete and pertinent example, adding an ancilla prepared in a particular state and applying a unitary are both isometries, as is their composition. Isometries are naturally extended to the dual space by \u03a6( \u03c8|) = \u03a6(|\u03c8 ) \u2020 and to operators by \u03a6(|x y|) = \u03a6(|x )\u03a6( y|), plus linearity.",
              "As we shall see, we will need to address the state spaces of provers individually, se we will need the concept of a local isometry.",
              "Definition 3. A local isometry on n subsystems is an isometry of the form",
              "where \u03a6 j operates on the j-th subsystem only.",
              "Here a tensor product of isometries is evaluated in a way analogous to how a tensor product of unitaries is applied: decompose the state into a sum of product states and apply the operator to the appropriate vector in the tensor product. That is to say,",
              "(3)",
              "From this it is easy to derive the following properties of local isometries.",
              "Lemma 1. Let \u03a6 = \u03a6 1 \u2297 \u03a6 2 be a local isometry, |\u03c8 1,2 be a bipartite state, and M 1 be a local operator on the first subsystem. Then",
              "We make extensive use of bit strings. For an n-bit string t the j-th bit is t j . Inner products of bit strings are given by",
              "We will, at times, consider the inner product as an integer, and at other times as a bit (i.e. over Z or Z 2 ). Where the difference is important we will specify. For example, t \u00b7 t taken over Z gives the number of ones in t but when taken over Z 2 it is the parity of the number of ones. Finally, we define the bit string 1 v to have a one only in the v position and zeros elsewhere, i.e. (1 v ) j = \u03b4 vj ."
            ]
          },
          {
            "header": "Graph states",
            "paragraphs": [
              "We assume that the reader is familiar with the basics of graph theory. A good resource is {{not_in_s2orc}}. We now fix some notation for our convenience. Let",
              "In other words, the induced subgraph is the maximal subgraph of G on vertices in S.",
              "The graph state |G is an n-qubit state, with qubits labelled by vertices, which is stabilized by the operators",
              "That is, S v has X on vertex v and Z on each of its neighbours and",
              "Equivalently,",
              "To explain, let us write",
              "Now since A u,v = A u,v = 1 whenever (u, v) \u2208 E, we are counting edges. The summation and A are symmetric, so we are double counting and we always get an even number (hence the 1 2 appearing in the exponent above). Let T x = {v|x v = 1}, then we are summing over all the vertices in T x , double counting the edges in the induced subgraph.",
              "For completeness we show that the above two definitions are equivalent by showing that |G is stabilized by S v :",
              "where we have re-indexed the summation by x \u2192 x \u2295 1 v . The \u00b1 in the exponent of the \u22121 represents the fact that we only care about the parity of the exponent, so we can add or subtract as we please.",
              "There are two cases. First, if v \u2208 T x then T x\u22951v does not contain v. The subgraph on T x is obtained from the induced subgraph on T x\u22951v by adding v and all the associated edges, x \u00b7 A1 v of them, and the total number of edges in the induced subgraph on T x is",
              "In the other case v / \u2208 T x , so we obtain T x by removing v and all associated edges from T x\u22951v , so",
              "Hence",
              "We have shown that the operators S v stabilize |G . It is also easy to see that the S v operators are independent: any one cannot be obtained by multiplying together others. They also commute with each other. We then have n independent, commuting n-qubit Pauli operators which stabilize a 1-dimensional space {{not_in_s2orc}}.",
              "Operationally, graph states are constructed by beginning with the qubits in the state |+ \u2297n and applying CTRL-Z gates on vertices u, v whenever (u, v) \u2208 E.",
              "The above reasoning will be important later on. In particular, we can apply (13) and (14) repeatedly over all v such that y v = 1 to prove the following lemma."
            ]
          },
          {
            "header": "Definition for \"closeness\"",
            "paragraphs": [
              "We will need to establish that the state held by the provers is \"close to\" a given graph state and that the measurements they perform are \"close to\" the ideal X-Z plane observables. However, there are many transformations that the provers can apply to both states and measurements which are invisible to the verifier. In particular, the provers may add an ancilla or apply a local change of basis (simultaneously to both the state and measurements). In fact, we will see that for the states and observables we use these are the only undetectable transformations that they can apply 2 . We can account for such transformations by allowing an arbitrary isometry which undoes these transformations and presents us with the required graph state plus some arbitrary ancilla state. We also allow for some noise by comparing states in the usual vector norm.",
              "Definition 4. {{not_in_s2orc}} We say that a multi-partite state |\u03c8 \u2032 and observables {M \u2032 } are \u01eb-equivalent to |\u03c8 and {M} if there exists a local isometry \u03a6 and a state |junk such that for every M",
              "Here we are thinking of \"M\" as both the ideal operation on |\u03c8 and as a label for the operation M \u2032 .",
              "Evidently this definition guarantees that the two systems behave like each other since isometries preserve inner products, and hence outcome probabilities. As we shall see, it is also a necessary condition for states and measurements to behave close to the ideal graph states and X-Z plane measurements. Hence any other definition we could choose is at most a different characterization of the errors and in the exact case is equivalent. The error bound used here has an operational meaning since we can quickly bound the error in outcome distributions from it.",
              "There is one shortcoming of this definition, which is that it is impossible to test states or operators which contain any imaginary component in the ideal case (this restriction does not apply to the states and operators held by the provers, only to the ideal that we compare them to.) The simple reason is that the provers may apply a complex conjugation to everything without changing the distribution of their responses to the verifier. This transformation is not an isometry, and hence it is impossible to conclude that any system satisfies the above definition based on classical interaction alone. It is, however, possible to extend the definition to account for this case {{44939389}}. We do not need to use this extended definition here since all our ideal operators and states are real."
            ]
          },
          {
            "header": "Modelling the provers",
            "paragraphs": [
              "An important argument in our work is that we can model the provers, even in the dishonest case, by a pure joint state held by the provers, and a collection of observables for each prover, one per possible query to that prover.",
              "First, it should be clear that it is not a restriction to consider pure states. Any mixed state can be purified and the purification given to any one of the provers. This only increases the power of the provers by giving them additional information held in the purification.",
              "Next, since our provers will only receive one query and respond with one message, we can model their actions by a measurement. Any pre-processing done before the measurement can be incorporated into the choice of measurement along with any post-processing. Further, since we are not making any assumptions on the dimension of the state held by the provers, their measurements can be taken to be projective and, since the provers will always respond with \u00b11 the projectors can be combined into an observable without any loss of information or generality.",
              "Finally, we must consider how the provers will behave knowing that some of the time they will be tested and some of the time they will be asked to perform the calculation. As well, in certain cases the provers will know for certain that they are being tested, although they will never be able to conclude that they are certainly taking part in the calculation. The provers know in advance the list of possible query strings (there are only four) and whatever their strategy, they use some physical processes to decide on their output. We then roll this process into the measurement observable, so that each possible query string corresponds to a single observable which represents the entire strategy of the prover."
            ]
          },
          {
            "header": "Test for honesty",
            "paragraphs": [
              "In order to develop a test for honesty we go through several steps. The first step is to develop a test for graph states. This is the foundation on which we build the test for honesty. After showing how we can verify that the provers hold onto a particular graph state we then show how to test measurements in the X-Z plane. Adaptive measurements built on measurements in the X-Z plane are the next step. Finally, we put all of the tests together into a single test and show how the probability of passing this test relates to the amount of error in an adaptive measurement performed on the same state and using the same measurements."
            ]
          },
          {
            "header": "Self-test for triangular cluster states",
            "paragraphs": [
              "In this section we develop a self-test for triangular cluster states. The techniques used are similar to those in {{34099972}}. However, we make some modifications which allow for a tighter error analysis and clearer notation. Although we give the construction for triangular cluster states only, the same techniques can be extended to work with any stabilizer state, as in {{34099972}}.",
              "Theorem 3. Let G be a triangular grid graph on n vertices with adjacency matrix A and let \u01eb > 0. Further, suppose that for an n-partite state |\u03c8 \u2032 with local measurements",
              "and for each triangle",
              "then there exists a local isometry \u03a6 and state |junk such that",
              "We may interpret Theorem 3 as follows: for each triangular cluster state there exists a set of non-local correlations that uniquely identifies that graph state and X and Z measurements, up to local unitaries and additional ancillas.",
              "The proof can be divided into several sections. The final goal is to construct an isometry \u03a6 and prove that it takes the state close to the desired graph state. The construction for the isometry is given in terms of the X \u2032 and Z \u2032 operators on each vertex. To bound the error we need to know how these operators behave and in particular that they approximately anti-commute. This is done in Lemma 3 and corollary 1. For the final proof we will also need to get rid of some X \u2032 operators. In the ideal case we can use the stabilizers:",
              "In Lemma 4 we show that this, and a generalization to many X's, is also approximately true for the primed operators. With these estimations in place we the proceed with the proof of Theorem 3."
            ]
          },
          {
            "header": "Preliminary technical estimations",
            "paragraphs": [
              "Our graph G is a triangular lattice, so every vertex lies in a triangle. For self-testing this gives a nice advantage, since it is particularly easy to show that X \u2032 and Z \u2032 anti-commute for vertices in a triangle.",
              "Proof. First, let T = {u, v, w} be a triangle containing v. The first part of Lemma 9, together with the conditions of Theorem 3, tell us",
              "for x \u2208 {u, v, w}, and from triangle \u03c4",
              "Applying the second part of Lemma 9 three times to combine these, we find",
              "The Z \u2032 s operating on vertices outside T all cancel since they appear in S \u2032 x and in Z \u2032A1u for some x \u2208 {u, v, w} and there are no X \u2032 operators outside the triangle. We are left with",
              "Examining how the operators commute with each other, we can pair up and cancel the X \u2032 x and Z \u2032 x for x \u2208 {u, w}, resulting in",
              "Rearranging by multiplying by Z \u2032 v X \u2032 v , we obtain our result. Note that it is sufficient to consider a set of triangles that covers the set of vertices and hence Theorem 3 holds for all graphs in which each vertex is contained in a triangle. In fact, as in {{34099972}}, it is sufficient to consider one triangle or just one edge in a connected graph, but this will give a less robust result. Lemma 2 in {{34099972}} shows that if X \u2032 v and Z \u2032 v approximately anti-commute, then so do X \u2032 u and Z \u2032 u for some neighbour u of v. Using this one can induct along paths to all vertices in a connected component. For our purposes this is unnecessary since all vertices lie in at least one triangle.",
              "The above lemma can be generalized to products of operators, as in the following corollary.",
              "Corollary 1. Let s, t \u2208 {0, 1} n . Under the conditions of Theorem 3,",
              "where s \u00b7 t is taken over Z.",
              "This can be seen by repeatedly applying Lemma 3, once for every v such that s v = 1 = t v , and using the triangle inequality. If s x = 1 but t x = 0, or vice versa, for some x \u2208 V , then the single operator on vertex x commutes with all other operators. Now we consider the physical \"stabilizer generators\" S \u2032 v . The conditions of Theorem 3 establish that they really are (close to) stabilizers of |\u03c8 \u2032 . Next we consider products of these generators and show that they too almost stabilize |\u03c8 \u2032 .",
              "Lemma 4. Let t \u2208 {0, 1} n . Under the conditions of Theorem 3,",
              "where At is evaluated over Z 2 and t \u00b7 At and t \u00b7 t are evaluated over Z.",
              "Proof. First, by Lemma 9 we find",
              "The right term can be expanded as v\u2208V tv=1",
              "We fix an ordering < on V , and evaluate the product according to that ordering. Thus if t v = t u = 1 and u < v then S \u2032 u appears in the product to the left of S \u2032 v . Now suppose that A uv = 1. Then Z \u2032 u in S \u2032 v appears to the right of the only occurrence of X \u2032 u in S \u2032 u . We may commute Z \u2032 u to the right past all remaining operators on the u system, so that Z \u2032 u appears to the right of all X \u2032 operators. The opposite is true if v > u, in which case we may commute Z \u2032 u to the left, and it appears to the left of all X \u2032 operators. Thus we may write the above as",
              "Let A L be the lower triangular part of A (with 0s elsewhere) and A U the upper triangular part. Then we may rewrite the above as",
              "Using corollary 1 we obtain",
              "Noting that A U t + A L t = At and t \u00b7 A L t = 1 2 (t \u00b7 At) since A is symmetric, we apply the triangle inequality along with (31) to find",
              "which is readily transformed into the desired result by multiplying by X \u2032t ."
            ]
          },
          {
            "header": "Proof of Theorem 3",
            "paragraphs": [
              "We are now in a position to prove Theorem 3. This is done by giving a construction for \u03a6 and using the above lemmas to prove that it has the necessary properties. Figure 5: Circuit for \u03a6 v Proof. We will use \u03a6 v as defined in figure 5. The circuit is modified from that used in {{34099972}}{{118535156}} and earlier works, differing in the state of the ancilla. Whereas we use an entangled pair of qubits |\u03c6 + , previous works used |0 . We also add an initial CTRL-X gate which was not needed when the initial state was |0 . When X \u2032 v and Z \u2032 v are the Pauli X and Z gates the circuit is clearly a SWAP gate. The idea is to swap the hidden qubit in the input wire with an explicit qubit. As we shall see, the use of a maximally entangled pair of qubits in the ancilla wires allows for a tighter robustness analysis than is possible with the earlier version of the circuit.",
              "The structure of the proof is a sequence of chained inequalities between states |\u03c8 j and |\u03c8 j+1 for j = 1 . . . 4 defined below. We then use the triangle inequality to find the total distance. We define \u03a6 = v\u2208V \u03a6 v , and |\u03c8 1 = \u03a6 (Z \u2032p X \u2032q |\u03c8 \u2032 ) to be the state after the isometry \u03a6 is applied. Using Lemma 12 this becomes",
              "where s, t, u \u2208 {0, 1} n . The first step is to move Z \u2032p to the left using corollary 1, obtaining",
              "Next we move the combined Z \u2032 s back to the right using corollary 1 again:",
              "Moving Z \u2032 to the left past the combined X \u2032 s using corollary 1 one last time, we define |\u03c8 4 by",
              "Now make two changes of variable, t \u2192 t \u2295 p and u \u2192 u \u2295 q, arriving at 1 \u221a 2 3n s,t,u (\u22121) t\u00b7s (\u22121) p\u00b7u Z \u2032t X \u2032u\u2295s |\u03c8 \u2032 |s |u \u2295 q .",
              "Next we replace the X \u2032 s with Z \u2032 s using Lemma 4",
              "s,t,u (\u22121) t\u00b7s (\u22121) p\u00b7u (\u22121) 1 2 (u\u2295s)\u00b7A(u\u2295s) Z \u2032t\u2295A(u\u2295s) |\u03c8 \u2032 |s |u \u2295 q .",
              "after which the state factorizes:",
              "In the case where \u01eb = 0 the above essentially gives the entire proof. The remainder of the proof estimates the error at each step.",
              "First we note that all the states above are normalized. In all cases this is easy to prove. We show the calculation for |\u03c8 2 , with the others proceeding similarly. We find",
              "The su|s \u2032 u \u2032 factor implies that u \u2032 = u and s \u2032 = s for all non-zero terms so",
              "The X \u2032u operators square to the identity, and subsequently so do the Z \u2032p s.",
              "We then make a change of variable t \u2032 \u2192 t \u2032 \u2295 t and break (\u22121) t\u00b7(s\u2295u) into (\u22121) t\u00b7s (\u22121) t\u00b7u . The summand no longer depends on t \u2032 so we can omit it from the summation, multiplying by 2 n instead. We also bring the summation over u inside, forming an inner sum.",
              "1 2 2n",
              "Lemma 10 says that the inner sum is 0 except when t = 0, so we can drop the Z \u2032t s and the summation over t, and t\u00b7s = 0. Then the X \u2032s\u2295q s then square to the identity. We are left with 1 2 n s \u03c8 \u2032 |\u03c8 \u2032 = 1. Now we estimate the distances between successive states starting with |\u03c8 1 and |\u03c8 2 . From the definition of the 2-norm,",
              "Next we determine \u03c8 1 |\u03c8 2 :",
              "From the su|s \u2032 u \u2032 term, we see than s = s \u2032 and u = u \u2032 in all non-zero terms. This allows us to cancel X \u2032u X \u2032u \u2032 and remove the u \u2032 and s \u2032 variables. We also pull the sum over u in as an inner sum 1 2 3n",
              "(52) Next we use Lemma 10 to see that the inner sum is zero except when t\u2295t \u2032 = 0. The terms (\u22121) (t\u2295t \u2032 )\u00b7s) and Z \u2032t\u2295t \u2032 then become 1 and the identity, leaving the summand independent of t and t \u2032 . We remove them from the sum, multiplying by 2 n instead. Finally, we make the change of variable s \u2192 s \u2295 q to get 1 2 n s (\u22121) p\u00b7s \u03c8 \u2032 |Z \u2032p X \u2032s Z \u2032p X \u2032s |\u03c8 \u2032 .",
              "Next set \u01eb p,s = \u03c8 \u2032 |Z \u2032p X \u2032s Z \u2032p X \u2032s |\u03c8 \u2032 \u2212 (\u22121) p\u00b7s \u03c8 \u2032 |Z \u2032p X \u2032s X \u2032s Z \u2032p |\u03c8 \u2032 , with the second term becoming just (\u22121) p\u00b7s , so that the above becomes",
              "Corollary 1 and the third part of Lemma 9 give |\u01eb p,s | \u2264 4(p\u00b7s) \u221a 2\u01eb. Lemma 11 tells us how to deal with the sum over s, and we write",
              "and plugging this back into the definition of the 2-norm gives",
              "Using similar estimates we find",
              "where the final sum is over s and t rather than s with the fixed string p. We appeal to the second part of Lemma 11 as the last step.",
              "The remaining distance to calculate is |||\u03c8 4 \u2212 |\u03c8 5 ||. Again we proceed by way of the definition of ||\u00b7|| and the inner product.",
              "For non-zero terms s = s \u2032 and u = u \u2032 . Re-indexing by s \u2192 s \u2295 u we find that the above is equal to",
              "where we have pulled all the terms dependent on u into the inner sum. Lemma 10 says that this inner sum is zero except where t \u2295 t \u2032 = 0 when it is 2 n . Substituting these in, the above becomes",
              "Now let us bound the inner product:",
              "To obtain the second line above we have used the triangle inequality. The third line comes from applying Lemma 4 and Lemma 9. We then use Lemma 11 to obtain the last line. Finally we find",
              "Adding all the bounds using the triangle inequality we obtain"
            ]
          },
          {
            "header": "Error bounds for non-Pauli measurements",
            "paragraphs": [
              "In order to achieve universal computation we need to have measurements other than just X and Z. It suffices to have X-Z plane measurements. Let us define",
              "We use the symbol R \u2032 u (\u03b8) to denote the \u00b11 eigenvalue observable that the prover uses when queried with the angle \u03b8. We do not make any prior assumption on how R \u2032 u (\u03b8) is related to X \u2032 u or Z \u2032 u . Instead we will derive said relationship via the graph-state test and further measurements.",
              "Lemma 5. Under the conditions of Theorem 3, if we have measurements R \u2032 v (\u03b8) and an edge (u, v) such that",
              "then with \u03a6 and |junk set to those in Theorem 3,",
              "where \u03b4 is the bound in Theorem 3",
              "Proof. From Theorem 3 we obtain \u03a6 and |junk so that",
              "for M \u2032 \u2208 {Z \u2032A1v , X \u2032 u Z \u2032A1u\u22951v } in particular. From the stabilizer generators S u and S v we find X u Z A1u\u22951v |\u03c8 = Z v |\u03c8 and Z A1v |\u03c8 = X v |\u03c8 , hence linearity of \u03a6 and the triangle inequality give",
              "Using cos \u03b8 X v + sin \u03b8 Z v = R v (\u03b8) and cos \u03b8 + sin \u03b8 \u2264 2 this becomes",
              "Now since || \u03c8 \u2032 |R \u2032 v (\u03b8)|| \u221e = 1, and \u03a6 preserves inner products, we have",
              "Using the triangle inequality and (69) we find",
              "We now apply Lemma 9 to obtain the desired bound.",
              "The lemma says that if we can estimate the expected value for a certain operator we can bound the error on R \u2032 u (\u03b8). Later in section 3.4 we will show how we can estimate said expected value."
            ]
          },
          {
            "header": "Error bounds for measurement patterns",
            "paragraphs": [
              "Our bounds in the previous section show that we can bound the error when applying a measurement of the form M 1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 M n , which gives a single bit of output. However, for graph state computation we need something much more substantial since we will need to measure the subsystems in a sequence, with each basis chosen as a function of the previous outcomes. In fact we will prove something even stronger than this.",
              "We will consider a stronger situation where instead of trusted classical computation and classical interaction, we have some trusted quantum computation and quantum interaction with the provers. The provers allow the basis to be chosen quantumly and they similarly return the result coherently. We can model this by specifying that, when queried with a quantum register, prover j applies",
              "where M \u2032 j,k corresponds to the observable that prover j uses when queried with input k \u2208 {0 . . . m j }. The prover then passes the control register back to the verifier and the result of the query is stored as a \u00b11 phase. We will require that the prover's actions are all of this form, although they are free to choose the M \u2032 j,k as they like. As well, the ideal operator V j has this form, using observables M j,k .",
              "Assuming 4 M \u2032 j,0 = I we can retrieve the outcome for measurement M \u2032 j,k by preparing the state 1 \u221a 2 (|0 + |k ) and observing the relative phase change in the prover's response. Hence this model includes the original classical behaviour as a particular case.",
              "A general circuit for the verifier-prover interaction in this stronger model is given in figure 6. The verifier first applies some unitary U 0 to prepare its initial state, and then performs the first query to prover 1, V \u2032 1 . The verifier then applies some unitary U 1 to its internal state and performs the second query to prover 2, V \u2032 2 , and so on. The combined operation is U n V \u2032 n . . . U 1 V \u2032 1 U 0 . We require that each V \u2032 j is applied at most once and for convenience we suppose that they are numbered in the order in which they are applied. In this circuit we have always used the same the control wire, which is a q-dit with dimension equal to the maximum m j + 1. This is not a limitation since we can always use the same control wire by incorporating swaps into the U's if necessary.",
              ". . . Figure 6: Semi-trusted circuit incorporating untrusted measurements by the provers,",
              "That is, W \u2032 j represents running the circuit until the point after U j has been applied. Similarly, let W j be the ideal circuit where we substitute in the ideal V j (constructed from the ideal M j,k ).",
              "Lemma 6. Let |\u03c8 , |\u03c8 \u2032 , |junk , and \u03a6 = \u03a6 1 \u2297 . . . \u03a6 n be given along with M \u2032 j,k and M j,k (k = 0 . . . m and M \u2032 j,0 = I) such that",
              "Further, let some |\u03c6 and U j be given where |\u03c6 contains a register of dimension at least m + 1 and U j acts only on the |\u03c6 registers, and let, V j , V \u2032 j , W j and W \u2032 j be defined as above. Then",
              "The intuition is that each V \u2032 j is the sum of operators |k k| \u2297 M \u2032 j,k , each of which is close to the corresponding ideal operator. We can then use the triangle inequality to say that V \u2032 j as a whole is close to its ideal counterpart. Inducting over the depth of the circuit gives the desired result.",
              "Proof. The proof proceeds by induction. For the case n = 0 we have not yet applied any untrusted gates and the conclusion is true by taking inequality (77) and multiplying by the trusted gate U 0 . Now let us suppose that (78) holds for n \u2212 1. We start by using the bound (77) with (j, k) = (1, 0) to get",
              "For each k = 0 we multiply by the bound on both sides by \u03a6 n (M \u2032 n,k ) to obtain inequalities",
              "By Lemma 1 \u03a6 n (M \u2032 n,k )\u03a6(|\u03c8 \u2032 ) = \u03a6(M \u2032 n,k |\u03c8 \u2032 ), so the state on the right above is close to |junk M n,k |\u03c8 by (77) with (j, k) = (n, k). Using the triangle inequality we find",
              "We introduce the register |\u03c6 and apply the ideal unitary W n\u22121 to both sides in the above estimation without increasing the distance. On the left, since \u03a6 n and M \u2032 n,k only operate on the nth subsystem, \u03a6 n (M \u2032 n,k ) operates only on the nth subsystem of |junk |\u03c8 (i.e. on the nth subsystem of |junk together with the nth subsystem of |\u03c8 ). Then since W n\u22121 operates only on the trusted system and the first n \u2212 1 subsystems of |\u03c8 , it commutes with \u03a6 n (M \u2032 n,k ) and M n,k so \u03a6 n (M \u2032 n,k )|junk W n\u22121 |\u03c8 |\u03c6 \u2212 |junk M n,k W n\u22121 |\u03c8 |\u03c6 \u2264 2\u03b4 (82)",
              "Now we apply the projection |k k| (used in the expression for V \u2032 n ) to both sides, again without increasing the distance. Hence",
              "Summing over all k using triangle inequality, we apply the definitions of V j and V \u2032 j to arrive at",
              "Note that it is 2m\u03b4 and not 2(m + 1)\u03b4 since the case k = 0 has no error by assumption. The state on the right above is almost what we want. Now we invoke the induction hypothesis, (78) with n \u2212 1 and multiply through by",
              "and applying the triangle inequality to the above two estimates we get",
              "Multiplying by the trusted gate U n finishes the proof.",
              "In order to use this lemma we do not need the full generality of Theorem 3. We only need it to apply for individual measurements rather than the full set X p Z q .",
              "For our purposes we do not need the full strength of the lemma. We need only know that adaptive measurements give correct outcomes.",
              "Corollary 2. Let |\u03c8 , |\u03c8 \u2032 , |junk , and \u03a6 = \u03a6 1 \u2297 . . . \u03a6 n be given along with M \u2032 j,k and M j,k (k = 0 . . . m and M \u2032 j,0 = I) such that",
              "Then for any adaptive measurement made using the M \u2032 s, the probability of a particular outcome differs from the ideal case by at most 2(2nm + 1)\u03b4.",
              "Proof. We can represent an adaptive measurement as a circuit W n as in Lemma 6. Hence",
              "To obtain the classical outcome we perform some measurement on one of the trusted subsystems. Without loss of generality this can be a projective measurement, so let \u03a0 x the projector for outcome x, which acts non-trivially only on the trusted subsystem. The probability of outcome x is then",
              "Now to estimate this probability we use (88) above in two different ways. First, multiplying on the left by \u03a6( \u03c8 \u2032 | \u03c6|W \u2020\u2032 n ) we get",
              "Second, multiplying (88) on the right by junk|W \u2020 n \u03c8| \u03c6| and then taking the adjoint of the resulting expression we obtain",
              "(91) Adding these together using the triangle inequality and invoking the fact that \u03a6 preserves inner products we find",
              "In other words, the probability of finding outcome x differs from the ideal case by at most 2(2mn + 1)\u03b4."
            ]
          },
          {
            "header": "A one-shot test",
            "paragraphs": [
              "As stated, the self-testing results are not terribly useful to us. They require knowledge of the expected value of various operators in order to draw any conclusions. The obvious solution is to take some samples and estimate, but this would require either some independence assumptions or additional work with, for example, martingales. Instead we will work with the contrapositive of the self-testing results: if the state and/or some measurements are far away from the ideal, then some measurable expected value will also be far away from the ideal. Although this is logically equivalent, instead of requiring lots of information about the various measurements, we instead are told that we just have to look for one measurement that is misbehaving. As well, we are going to arrange our measurements in a particular way as a test for honesty. For example, the stabilizer measurements will always return 1 for honest provers, so if we perform this measurement and we get a 1 the provers pass the test. If result is -1 then they fail the test. As the expected value gets close to 1, the provers will pass with probability close to 1. If the expected value is far away from 1, the provers will fail the test with some probability. Now with the R(\u03b8) measurements we do not have the same situation, but we do have something just as useful. We can build a compound test so that the ideal honest provers pass with some probability, and no other provers can pass with a higher probability. This is analogous to the CHSH test: the ideal quantum strategy passes with probability \u2248 0.85, and no other strategy achieves any higher success rate. As well, cheating provers will pass the test with a probability that is bounded away from the quantum limit, and so we obtain a gap between the ideal and cheating strategies. The honest provers will fail the test some of the time, but this is no problem: we will later do some repetition so that the ideal provers will pass with an overall probability that can be made arbitrarily close to 1. Now we give the construction for our one-shot test. First, let T be a set of triangles that covers V , i.e. each vertex in V appears in at least one triangle in T . The triangles will be specified by characteristic vectors \u03c4 . Let N G = 3|V | + |T |. Note that N G \u2264 4n since we need no more than n triangles to cover V .",
              "For a graph state computation we need only two different measurement angles per vertex, \u00b1\u03b8 v . As well, the measurement angle \u03b8 v + \u03c0 can be simulated by measuring with angles \u03b8 v and flipping the outcome. Hence there is no loss of generality by assuming that 0 \u2264 \u03b8 v \u2264 \u03c0 so that cos \u03b8 v \u2265 0.",
              "The test procedure is as follows:",
              "Procedure 2 (One-shot test for graph states and measurements).",
              "1. Randomly select either \"VERTEX\" with probability |V | N G , \"TRIANGLE\" with probability |T | N G , or \"RTHETA\" with probability 2|V |",
              "ii. Accept if the product of the replies is 1, otherwise reject",
              "Accept if the product of the replies is 1, otherwise reject.",
              "To clarify, if the basis for a prover is I then we simply ignore that prover, and its \"reply\" is taken to be 1.",
              "The test is naturally grouped in to N G different subtests. From the graph state test we have |V | subtests testing the \"physical stabilizers\", and |T | subtests testing the triangles. Additionally, there are 2|V | \"RTHETA\" subtests, one for each choice of v and t. Each of these consists of two queries chosen according to some random coin.",
              "Lemma 7. Let n non-communicating quantum provers be given that each take one of four measurement bases, labelled X, Z and R v (\u00b1\u03b8 v ) as inputs and measure joint state |\u03c8 \u2032 according to operators in",
              "Then then procedure 2 accepts with probability at most",
              "and if there exist v and",
              "then procedure 2 accepts with probability at most",
              "Proof. Honest case. First let us derive the maximum probability of passing the test. This is attained in the honest case. The \"VERTEX\" and \"TRI-ANGLE\" subtests can all be passed simultaneously with probability 1 in the honest case since the observables are all in the stabilizer group of the graph state.",
              "Let us now consider the \"RTHETA\" subtests. First, we fix a vertex v. The queries to the in the provers in the test can be seen as one large random variable taking values \u00b11 and having the expected value",
              "Note the similarity to the CHSH correlation, which is obtained for \u03b8 v = \u03c0 4 . The honest provers will attain an expected value of 1 2(cos \u03b8v+| sin \u03b8v|) . To see this, we notice that Z A1u |\u03c8 = X u |\u03c8 and X u Z A1u\u22951v = Z v |\u03c8 , which we obtain from the stabilizers S v and S u . Applying the definition of R(\u03b8), the expected value becomes",
              "Now we show that this is in fact the maximal quantum expected value. Using a standard technique introduced by Cirel'son {{120680226}}, the maximum value is the same as 1 2(cos \u03b8 v + | sin \u03b8 v |) max |\u03c8 1 ,|\u03c8 2 ,|\u03c6 1 ,|\u03c6 2",
              "where the maximization is taken over normalized states, all of dimension four. Clearly the maximum is found when |\u03c8 1 is taken to be in the direction of cos \u03b8 v |\u03c6 1 + sin \u03b8 v |\u03c6 2 and |\u03c8 2 is in the direction of cos \u03b8 v |\u03c6 1 \u2212 sin \u03b8 v |\u03c6 2 . In this case the value becomes",
              "Expanding using the definition of ||\u00b7|| we obtain",
              "We next use the identity cos \u03b8 sin \u03b8 = 1 2 sin 2\u03b8 to get",
              "which attains the value of 1 cos \u03b8v+| sin \u03b8v| when \u03c6 1 |\u03c6 2 = 0. We now have the expected value of the honest case and a matching upper bound. The expected value of any \u00b11 valued random variable X is related to the probability of obtaining 1 (i.e. the \"success\" probability) by Prob(X = 1) = X 2 + 1 2 .",
              "(103) So the probability of success for the \"RTHETA\" portion of the test for a specific v is bounded above by",
              "Combining this with the maximum probability of success for the \"VER-TEX\" and \"TRIANGLE\" subtests, the overall maximum probability of success for any set of quantum provers, attained for honest provers, is",
              "The factor 2 in front of the summation represents the fact that for each v the \"RTHETA\" subtest occurs with probability 2/N G . Dishonest case Now that we have an upper bound, we translate the probability of success into an error bound on the expectation value of each subtest.",
              "From now on fix a set of provers, which fixes the observables and state. Suppose that the provers pass the test with probability c test \u2212 \u01eb 2N G . Then each \"VERTEX\" or \"TRIANGLE\" subtest passes with probability at least 1 \u2212 \u01eb/2, which is obtained when all the error happens on a single subtest. This means that the expected value for the corresponding random variable is 1\u2212\u01eb and the conditions for Theorem 3, (20) and (21) are satisfied. Hence (94)",
              "where we have used the estimations p \u00b7 p \u2264 1, since this is all we need to apply corollary 2, and |E| \u2264 3n, since we are using a triangular cluster state which has a maximum degree of 6. For the \"RTHETA\" subtests, fix a v. As above, the expected value for the corresponding \u00b11 random variable is at least c v test \u2212 \u01eb. Hence the conditions of Lemma 5 are satisfied for each v and \u00b1\u03b8. Then equation (94)",
              "where we have used \u01eb \u2264 \u01eb 1 4 for 0 \u2264 \u01eb \u2264 1. When \u03b4 \u2264 1 the error for the R \u2032 v (\u00b1\u03b8 v ) will be larger, so we will use this value.",
              "We have just shown that if the provers pass the test with probability at least c test \u2212 \u01eb 2N G then the left side of (94) is bounded by \u03b4 as above (i.e. (94) is false). This is the contrapositive of our desired result which is that, if (94) is true, then the probability of passing is at most c test \u2212 \u01eb 2N G . So we need only solve for \u01eb in terms of \u03b4. We find",
              "Now the probability of passing is at most c test \u2212 \u01eb 2N G , which is bounded above by",
              "This one-shot test gives us an error bound on the states and measurements. Combining this with Lemma 6 we can relate the probability of passing the test to the error in an adaptive measurement, i.e. our final measurementbased quantum computation.",
              "Corollary 3. Let a set of quantum provers be given where prover v takes in-",
              "For honest provers, procedure 2 accepts with probability",
              "For general provers, if for any adaptive measurement pattern the probability of any outcome on the provers' final outcome differs from the ideal by more then \u03b4 then procedure 2 accepts with probability no more than",
              "Proof. By corollary 2, if we achieve error less than \u03b4 \u2032 = \u03b4 2(8n+1) on equation (87) then we are done, since m = 4 here. Lemma 7 says that we can achieve this if the provers pass with probability no more than",
              "\u2265 \u03b4 8 10 18.1 n 11 (119) using the pessimistic bounds N G \u2264 4n and 1 \u2264 \u221a n \u2264 n. Hence if the provers pass with probability less than",
              "we obtain our desired result."
            ]
          },
          {
            "header": "Interactive proofs",
            "paragraphs": [
              "We are now in a position to construct an interactive proof for any language in BQP. To this end, let L be a language in BQP. Then from Theorem 2 and the definition of BQP for any input x there exists an adaptive measurement 5 on a polynomially sized triangular graph state such that",
              "\u2022 If x \u2208 L then the measurement outputs \"ACCEPT\" with probability c calc \u2265 2 3 . \u2022 If x / \u2208 L then the measurement outputs \"ACCEPT\" with probability s calc \u2264 1 3 . The adaptive measurement supplies the measurements required for each vertex via angles \u03b8 v . It also supplies the functions required for the adaptation.",
              "The interactive proof is given by the following procedure."
            ]
          },
          {
            "header": "(b) Accept if the test accepts",
            "paragraphs": [
              "Now we calculate the optimal value of q.",
              "Lemma 8. Let L be a language and x in input. Suppose we are given an adaptive measurement on a triangular cluster state on n vertices which implements a measurement-based computation which decides whether x \u2208 L with error at most 1 3 (i.e. c calc \u2265 2 3 and s calc \u2264 1 6 ). Let 0 < \u03b4 < 1 3 be given and set",
              "\u2022 If x \u2208 L then for honest provers procedure 3 accepts with probability at least c ip",
              "\u2022 If x / \u2208 L then for any set of provers procedure 3 accepts with probability at most s ip where c ip \u2212 s ip \u2265 \u03b4 8 10 19.2 n 11 (122)",
              "Proof. Let c test be the probability of honest provers passing the test, and let s test be the probability of dishonest provers passing the test, given in corollary 3. Here, by dishonest we mean that the probability of a particular outcome of an adaptive measurement made using the provers differs from the honest case by more than the given \u03b4.",
              "Then we have two cases:",
              "\u2022 The input is in the language: then we only care about the honest case, in which the probability of accepting is c ip \u2265 qc calc + (1 \u2212 q)c test .",
              "\u2022 The input is not in the language: then there are two cases:",
              "-The provers pass the test with probability at least s test . Then by corollary 3 the probability of accepting on the calculation is at most s calc +\u03b4 and the probability of accepting on the test is at most c test for an overall probability of at most q(s calc + \u03b4) + (1 \u2212 q)c test .",
              "-The provers pass the test with probability less than s test . Then the probability of accepting on the calculation could be as high as 1, since we gain no information from the test. The overall probability of accepting is then less than q + (1 \u2212 q)s test .",
              "The two different cases in the x / \u2208 L case give two different gaps which are, in the first case",
              "and in the second case",
              "The overall gap is the minimum of these two. We wish to find q which gives maximizes the minimum. The two equations are just lines in q which cross each other. At the point where they are equal we find the maximum overall gap. The crossing point is easily found to be at",
              "which gives a gap of",
              "On the first line, we see that the denominator can be no larger than 2, and the first factor in the numerator is at least 1 6 (when \u03b4 = 1 6 and c calc \u2212 s calc = 1 3 ). So we can lower bound the gap by 1 12 (c test \u2212s test ), which is estimated in 3. We are now in a position to prove our main claim. This is obtained by applying a standard gap amplification procedure. 3. Otherwise reject.",
              "Proof of Theorem 1. Let L \u2208 BQP be given along with input x. Theorem 2 tells us that we can find an adaptive measurement on a triangular cluster state on n = poly(|x|) vertices whose outcome tells us whether x \u2208 L with error less than 1 3 . From Lemma 8 we have an interactive proof such that if x \u2208 L then we accept with probability at least c ip for honest provers, and if x / \u2208 L we accept with probability at most s ip . Now we amplify using procedure 4. Provided that we use fresh randomness on each run of the interactive proof, the N trials are all independent, although not necessarily identically distributed.",
              "Let us first consider the case x \u2208 L. Then we are interested in the case of honest provers, in which case we have N independent and identical Bernoulli trials with some probability of accepting p \u2265 c ip . Using Hoeffding's inequality, the probability that we mistakenly reject is bounded by",
              "Setting this equal to 1 3 we solve for N to find the minimum number of trials to achieve our desired error rate.",
              "Subbing in for c ip \u2212 s ip as estimated in Lemma 8 we obtain N \u2265 10 38.7 n 22 \u03b4 16 .",
              "The same number of repetitions also suffices to bound the probability of accepting when x / \u2208 L to below 1 3 . The analysis is similar, however if the provers are not honest they may vary their behaviour on each trial, so the trials are not necessarily identically distributed. However, since the probability p of accepting satisfies p \u2264 s ip for every trial we can still use Hoeffding's inequality.",
              "Note that there is ambiguity in procedure 4. In particular, it does not mention whether the repetition of procedure 3 should be done serially or in parallel. In fact this does not matter. We can add N sets of n provers and query each prover once, or use one set of n provers and query each one N times. Either way the fact that we use fresh randomness keeps each trial independent of other trials. If x / \u2208 L then there are no provers, whether entangled with other provers in other trials, retaining memory of past trials, or otherwise, that will force the verifier to accept with probability more than s ip . Hence we can specify that the repetition is accomplished in parallel using nN provers, each of which, in the honest case, performs a single measurement."
            ]
          }
        ]
      },
      {
        "header": "Future work",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Future work",
            "paragraphs": [
              "Our error bounds are clearly suboptimal. In many places we have made only loose estimates which suffice for our purposes of establishing a polynomial bound, but could be made more robust. Hence one avenue of future improvement is to tighten these bounds.",
              "Currently our construction uses many simple provers, providing a nice complement to Reichardt et al.'s result using a constant number provers. Much of our result could easily be adapted to the case of two provers. The most difficult part is the graph-state test. Likely it is not possible to prove a self-testing theorem for two provers if there are any odd cycles in the graph since it would be necessary at some point to test the entanglement across an edge with both vertices held by a single prover. However, bipartite graph states could yet be self-tested with two provers. helpful discussions. This work is funded by the Centre for Quantum Technologies, which is funded by the Singapore Ministry of Education and the Singapore National Research Foundation, and by the University of Otago.",
              "The first inequality is a straightforward applications of the definition of ||\u00b7||. The second inequality is an application of the first, along with the triangle inequality. The last inequality is an application of the inequality ||O|\u03c8 || \u2264 ||O|| \u221e |||\u03c8 || 2 where we use the operator O = \u03c6|M.",
              "Lemma 10. Let t be an n-bit string. Then s\u2208{0,1} n (\u22121) s\u00b7t = 2 n \u03b4 t .",
              "If t = 0 then the summand is always 1. If t = 0 then half the strings s have inner product 0 with t and the other half have inner product 1, so we get a sum with half the summands 1 and the other half -1.",
              "Lemma 11. Let u \u2208 {0, 1} n be given and let A be the adjacency matrix for a graph G = (V, E). For the first one, the average inner product of a vector with u is half the number of 1's in u. The second computes this for an average u, which has n/2 1's. For the last one, t \u00b7 At counts the number of edges in the induced subgraph on S t = {v \u2208 V |t v = 1}. Consider an edge (u, v). Then (u, v) appears in the induced subgraph on S t whenever both ends are in S t , i.e. when t u = t v = 1. This happens for a quarter of all bit strings t. Hence each edge is counted 2 n\u22122 times for a total of 2 n\u22122 |E|.",
              "This is a standard result in quantum computing, and can be shown using induction on n.",
              "B Local complementation and self-testing stabilizer states Local Complementation. Let G be a graph. We may form a new graph G \u2032 by local complementation on a vertex v \u2208 V . This operation complements all the edges in the neighbourhood of v, meaning that if a, b \u2208 V are neighbours of v and (a, b) \u2208 E then (a, b) is removed in G \u2032 and, conversely, if (a, b) / \u2208 E then (a, b) is added in G \u2032 .",
              "Local complementation is relevant to graph states because |G and |G \u2032 will be related by local Clifford operations, which simply relabel Pauli measurements and outcomes. The standard stabilizer generators for |G \u2032 are found from those of |G by replacing S u with S u S v for each neighbour u of v, and exchanging Z v with Y v , and X u with Y u .",
              "Self-testing stabilizer states. First, note that an arbitrary stabilizer state is equivalent to a graph state under local Clifford operations {{not_in_s2orc}}. The local Clifford operations just relabel the Pauli operators (up to \u00b11), so the problem of testing stabilizer states reduces to that of testing graph states.",
              "To self-test an arbitrary graph we first divide it into connected components. The graph state will a product state where the overall graph state is the product of the graph states on the connected components. We may thus test connected components individually. If a component has only one vertex then the test is trivial. If it has two vertices then the graph state on that component is an EPR pair (up to local unitaries), which can be tested using the Mayers-Yao test {{14069874}} or the CHSH test {{18467053}}{{118535156}}. Now we consider a connected component with three or more vertices. We need to show that equation (23) in Lemma 3 holds for each vertex, i.e. the X \u2032 v and Z \u2032 v operators approximately anti-commute on the state. It is shown in {{34099972}} (Lemma 2) that if equation (23) holds on vertex v then it also holds (with a larger bound) on any neighbour u of v. Hence as long as (23) holds for at least one vertex in a connected component, then it also holds for all other vertices in the component by inducting along paths.",
              "Let us return to our component with three or more vertices. If it contains a triangle then we test using Lemma 3. If not, then let u, v \u2208 V be two vertices in the component that are not adjacent. Since they are in the same component, there is a shortest path between them of length at least 2. Look at the first three vertices in this path. The first and third are not adjacent, otherwise the path would be shorter. Hence we have an induced path of length 2. We then locally complement on the middle vertex, obtaining a triangle. The corresponding graph state is equivalent to the original graph state under local Clifford operations, which can be absorbed into the definition of the local isometry \u03a6. Now we have a graph state with a triangle and we can apply Lemma 3 and {{34099972}} (Lemma 2) to obtain the anti-commuting relation for all vertices.",
              "Having tested all components and obtained local isometries \u03a6 j for each component j, we simply form \u03a6 = \u03a6 1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u03a6 k and apply triangle equalities to obtain the final isometry and bound."
            ]
          },
          {
            "header": "(",
            "paragraphs": []
          },
          {
            "header": "Figure 2 :",
            "paragraphs": []
          },
          {
            "header": "( 42 )(\u22121) t\u00b7s (\u22121) p\u00b7u (\u22121) s\u00b7A(u\u2295s)) (\u22121) 1 2",
            "paragraphs": []
          },
          {
            "header": "Lemma 12 .",
            "paragraphs": []
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "44939389",
        "6197709",
        "8839192"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": []
        },
        {
          "header": "Assumptions",
          "paragraphs": [
            "For our result to hold, we must assume that the provers behave according to quantum mechanics. This is because we model the provers using the quantum formalism. Currently this appears to be a reasonable assumption since quantum mechanics has been a very successful theory. However, we run into a problem if we wish to use this result in certain circumstances. For example, we may wish to verify that quantum mechanics generates accurate predictions for very complex systems. In this case it becomes infeasible to classically compute predictions from the quantum model. Quantumly, this is still possible, and we might be tempted to use an interactive proof in order to verify that our quantum computer has done the computation correctly. However, if we use the arguments in this paper we run into a problem of circularity since we must assume that quantum mechanics is correct in order for the argument to go through, but quantum mechanics is exactly what we want to verify! Hence it remains an important open question whether it is possible to achieve interactive proofs for problems in BQP where the prover's actions are easy for quantum computers but for which we do not assume a priori that quantum mechanics holds."
          ]
        },
        {
          "header": "Time-space trade-offs",
          "paragraphs": [
            "As discussed in the proof of Theorem 1, there are space-time trade-offs. In particular we may perform the gap amplification by repeating in parallel or serially, or some mixture of the two. Hence we could perform N repetitions on n provers, each of which then performs N measurements, or we can repeat in parallel with nN provers, each of which performs a single measurement.",
            "Another factor, which we have not mentioned, is the time required to build the necessary graph states. Graph states are built by applying CTRL-Z gates, one for each edge. At worst this can take no more than O(n 2 ) operations. For triangular cluster states, such as we use here, the degree of each vertex in bounded above by 6 so at most 3n 2-qubit operations are required (plus n single-qubit state preparations). These can be parallelized in a constant depth circuit by exploiting the localized structure of triangular cluster states. Regardless, the state preparation can be accomplished in a polynomial number of steps."
          ]
        },
        {
          "header": "Simplicity",
          "paragraphs": [
            "Our construction has a somewhat remarkable property. Since triangular cluster states are universal, the state preparation depends only on the size of the calculation. By choosing a discrete set of operations (X, Z and X \u00b1 Z measurements are sufficient for universality) the quantum provers are also constant, requiring only the ability to measure in some fixed set of bases. The self-testing portion of the interactive proof then also only depends on the size of the calculation, since it depends only on the state and the measurements. Amazingly the classical verifier is also rather simple. It can be given a circuit as its input from which it reads off what gates to perform and simply looks up what angle to measure for that gate. The remaining calculations are simply XORs. This is a clear example of where the simplicity of the measurementbased quantum computing model allows for a simple analysis.",
            "Another interesting property is that no single quantum prover has enough power to convince even itself whether the input is in the language. Indeed, all the quantum parts together, including the state preparation, still cannot perform even simple calculations since they can only prepare and measure some fixed state. They lack the capacity to perform the XORs required to perform a full measurement-based calculation. It is only when we combine the verifier, provers, and state preparation together that we obtain enough power to perform any substantial calculations."
          ]
        },
        {
          "header": "X-Y plane measurements",
          "paragraphs": [
            "It should be possible, although a bit more involved, to use the usual graph state computation model {{8839192}}{{6197709}} involving X-Y plane measurements. There is a complication since it is possible to simulate a complex measurements M by using a M * measurements instead (i.e. complex conjugate everything). In other words, the provers could complex conjugate all their states and operators, and this would preserve the expected values of all operators. However, the complex conjugation cannot be \"undone\" through an isometry. Hence the complex conjugated provers would not satisfy equation (22).",
            "As far as correctness of the calculation is concerned, complex conjugation poses no problem since the outcome of an adaptive measurement will be identical to the desired case. A workaround is possible through a suitable relaxation of (22), and details are given for the exact case in {{44939389}}. The remaining points are to make the techniques of {{44939389}} robust, and generalize to X-Y plane measurements, which can be done analogously to how X-Z plane measurements are handled here."
          ]
        }
      ]
    },
    "discussion_txt": "For our result to hold, we must assume that the provers behave according to quantum mechanics. This is because we model the provers using the quantum formalism. Currently this appears to be a reasonable assumption since quantum mechanics has been a very successful theory. However, we run into a problem if we wish to use this result in certain circumstances. For example, we may wish to verify that quantum mechanics generates accurate predictions for very complex systems. In this case it becomes infeasible to classically compute predictions from the quantum model. Quantumly, this is still possible, and we might be tempted to use an interactive proof in order to verify that our quantum computer has done the computation correctly. However, if we use the arguments in this paper we run into a problem of circularity since we must assume that quantum mechanics is correct in order for the argument to go through, but quantum mechanics is exactly what we want to verify! Hence it remains an important open question whether it is possible to achieve interactive proofs for problems in BQP where the prover's actions are easy for quantum computers but for which we do not assume a priori that quantum mechanics holds.As discussed in the proof of Theorem 1, there are space-time trade-offs. In particular we may perform the gap amplification by repeating in parallel or serially, or some mixture of the two. Hence we could perform N repetitions on n provers, each of which then performs N measurements, or we can repeat in parallel with nN provers, each of which performs a single measurement.Another factor, which we have not mentioned, is the time required to build the necessary graph states. Graph states are built by applying CTRL-Z gates, one for each edge. At worst this can take no more than O(n 2 ) operations. For triangular cluster states, such as we use here, the degree of each vertex in bounded above by 6 so at most 3n 2-qubit operations are required (plus n single-qubit state preparations). These can be parallelized in a constant depth circuit by exploiting the localized structure of triangular cluster states. Regardless, the state preparation can be accomplished in a polynomial number of steps.Our construction has a somewhat remarkable property. Since triangular cluster states are universal, the state preparation depends only on the size of the calculation. By choosing a discrete set of operations (X, Z and X \u00b1 Z measurements are sufficient for universality) the quantum provers are also constant, requiring only the ability to measure in some fixed set of bases. The self-testing portion of the interactive proof then also only depends on the size of the calculation, since it depends only on the state and the measurements. Amazingly the classical verifier is also rather simple. It can be given a circuit as its input from which it reads off what gates to perform and simply looks up what angle to measure for that gate. The remaining calculations are simply XORs. This is a clear example of where the simplicity of the measurementbased quantum computing model allows for a simple analysis.Another interesting property is that no single quantum prover has enough power to convince even itself whether the input is in the language. Indeed, all the quantum parts together, including the state preparation, still cannot perform even simple calculations since they can only prepare and measure some fixed state. They lack the capacity to perform the XORs required to perform a full measurement-based calculation. It is only when we combine the verifier, provers, and state preparation together that we obtain enough power to perform any substantial calculations.It should be possible, although a bit more involved, to use the usual graph state computation model {{8839192}}{{6197709}} involving X-Y plane measurements. There is a complication since it is possible to simulate a complex measurements M by using a M * measurements instead (i.e. complex conjugate everything). In other words, the provers could complex conjugate all their states and operators, and this would preserve the expected values of all operators. However, the complex conjugation cannot be \"undone\" through an isometry. Hence the complex conjugated provers would not satisfy equation (22).As far as correctness of the calculation is concerned, complex conjugation poses no problem since the outcome of an adaptive measurement will be identical to the desired case. A workaround is possible through a suitable relaxation of (22), and details are given for the exact case in {{44939389}}. The remaining points are to make the techniques of {{44939389}} robust, and generalize to X-Y plane measurements, which can be done analogously to how X-Z plane measurements are handled here.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "Title: Interactive Proofs for BQP Languages with Polynomial Number of Quantum Provers",
          "",
          "Abstract:",
          "In this paper, we propose a measurement-based quantum computation model for constructing interactive proofs with non-communicating quantum provers and a classical verifier. Our construction enables interactive proofs for all languages in BQP with a polynomial number of quantum provers, each of which performs only a single measurement in the honest case. We achieve this by utilizing self-tested graph states and introducing two significant improvements over previous work: (1) deriving polynomial error bounds scaling with the size of the graph, compared to exponential dependence in previous work, and (2) extending the self-testing error bounds to a general set of measurements, including adaptive measurements used in measurement-based quantum computation as a special case.",
          "",
          "Introduction:",
          "Interactive proofs are a fundamental tool in quantum computing, enabling the verification of quantum computations without compromising their security. In the context of measurement-based quantum computation, we consider a setting where a classical verifier interacts with non-communicating quantum provers, each performing a single measurement. Our goal is to construct interactive proofs for all languages in BQP (Bounded-Error Quantum Polynomial-time) with a polynomial number of quantum provers.",
          "",
          "Previous Work:",
          "Several works have explored the construction of interactive proofs in measurement-based quantum computation. However, these constructions were limited to specific classes of languages or required an exponential number of quantum provers. In this work, we aim to overcome these limitations by deriving polynomial error bounds and extending the self-testing error bounds to a broader set of measurements.",
          "",
          "Our Contributions:",
          "Our main contributions can be summarized as follows:",
          "",
          "1. Deriving polynomial error bounds: We derive error bounds for our interactive proof protocols that scale polynomially with the size of the graph, rather than exponentially. This enables the construction of interactive proofs for a broader range of languages in BQP.",
          "2. Extending self-testing error bounds: We extend the self-testing error bounds to a general set of measurements, including adaptive measurements used in measurement-based quantum computation. This allows for the application of self-testing techniques to a broader range of quantum computations.",
          "",
          "Discussion:",
          "The measurement-based quantum computation model provides a promising avenue for constructing interactive proofs in quantum computing. By leveraging self-tested graph states and deriving polynomial error bounds, we are able to construct interactive proofs for all languages in BQP with a polynomial number of quantum provers. Our work demonstrates the power of this model for achieving efficient and secure interactive proofs in quantum computing.",
          "",
          "However, there are several open questions and future directions that arise from this work. For instance, the current construction relies on the availability of self-tested graph states, which may not be practical in current quantum technology. Future work may focus on developing alternative constructions that do not rely on these states. Additionally, the current bounds on the number of quantum provers required for interactive proofs are still exponential in the size of the graph, which may limit the practicality of the construction. Future work may aim to improve these bounds, enabling the construction of more efficient and practical interactive proofs.",
          "",
          "In conclusion, our work demonstrates the potential of the measurement-based quantum computation model for constructing interactive proofs in quantum computing. While there are still open questions and future directions, our contributions provide a significant step towards the development of practical and secure interactive proofs in this field."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.5838524103164673,
            "R": 0.6087294220924377,
            "F1": 0.5960314273834229
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.5266470909118652,
            "R": 0.5509724617004395,
            "F1": 0.5385352373123169
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          ""
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "]",
          "",
          "Discussion:",
          "",
          "The measurement-based quantum computation model allows for a new paradigm for quantum computation where the measurement"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.3608064353466034,
            "R": 0.5308445692062378,
            "F1": 0.42961230874061584
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.3195165693759918,
            "R": 0.5407067537307739,
            "F1": 0.4016742408275604
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10051468",
    "title": "Graph-based Clustering of Synonym Senses for German Particle Verbs",
    "externalids": {
      "ACL": "W16-1805",
      "ArXiv": "",
      "CorpusId": "10020161",
      "DBLP": "conf/mwe/WittmannMW16",
      "DOI": "10.18653/v1/W16-1805",
      "MAG": "2513477335",
      "PubMed": "",
      "PubMedCentral": ""
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "X",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "X",
            "paragraphs": [
              "h before killing. Tissue was processed for t-#m sections as previously described {{not_in_s2orc}}. In addition, the sections were coated with a silver emulsion and incubated for 2 wk before Giemsa staining and microscopic examination. Autoradiography of [aH]thymidine incorporation into cell nuclei was used to quantitate endothelial cell proliferation. Labeled endothelial cells had >5 grains of precipitated silver superimposed on their nuclei. The percentage of labeled endothelial cells was determined within a 0.5-ram swath along the wound edge from the epidermis to the panniculus carnosis and within the adjacent swath 0.5-1.0 mm from the wound edge.",
              "Reciprocal Species-specific Anti-Fibronectin Antibodies. Rat and mouse fibronectin was isolated from normal rat-and mouse-citrated plasma using gelatin-affinity chromatography {{19792028}}. Fibronectin from each animal was mixed with complete Freund's adjuvant and injected into the footpads of reciprocal animals. This procedure produced species-specific antibodies. Antisera from each species were purified by adsorption on insolubilized fibronectin-depleted plasma of that species. Antibodies were isolated by passage of the antisera over a protein A affinity column {{not_in_s2orc}} and then conjugated with either fluorescein or rhodamine by the dialysis method {{not_in_s2orc}}. Immunofluorescence was performed as previously described {{not_in_s2orc}}."
            ]
          }
        ]
      },
      {
        "header": "Results",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Results",
            "paragraphs": [
              "To test whether blood vessel fibronectin was produced in situ, we used rat ear skin transplanted onto the flanks of immunosuppressed mice and reciprocal species-specific anti-fibronectin antibodies. Previous studies have shown that donor (rat) blood vessels persisted in the xenograft for at least 4 wk after transplantation {{17786725}}{{14452609}}; therefore, within this time period mouse anti-rat fibronectin antisera would identify fibronectin produced in situ by donor (rat) blood vessel cells, while rat anti-mouse fibronectin antisera would identify plasma derived fibronectin deposited in the vessels from the recipient (mouse) circulation. Because the graft sites were healed within 2 wk after surgery, it was possible to extirpate a 3-ram punch biopsy from the center of each graft site at that time.",
              "2 d after excisional wounding, the endothelial cells of the microvasculature along the wound edge appeared markedly activated as characterized by swollen nuclei with an open chromatin pattern, swollen cytoplasm, and bulging of the entire cell into the vessel lumen. Some endothelial cells in blood vessels within 0.5 mm of the wound edge demonstrated mitotic figures by day 4. These cytologic findings correlated with the greatly increased numbers of [aH]thymidine-labeled endothelial cells near the wound edge demonstrated with 1-/tin section autoradiography at 2 and 4 d after injury (Fig. 1).",
              "When the microvasculature near the wound was examined with species-specifc, fluorescein-labeled, anti-rat fibronectin antibodies, an increased fluorescence of fibronectin was noted in the walls of blood vessels that coursed within 0.5 mm of the wound edge the second day after injury (Fig 2. A and B) compared with the staining of fibronectin in walls of rat blood vessels within the graft before injury (Fig 2. D). When the rat vessels near the wound edge were stained with anti-mouse fibronectin no staining was apparent (Fig 2. C). The lack of vessel fluorescence with anti-mouse fibronectin was not secondary to a failure of the anti-mouse fibronectin antibodies to stain mouse fibronectin in tissue sections, because the microvasculature of normal mouse skin fluoresced with the same anti-mouse antibodies (Fig 2. E) but not with anti-rat fibroneetin antibodies (Fig 2. F). Specificity of the antisera was confirmed by the demonstration that anti-rat, but not anti-mouse, fibronectin antibody-staining activity could be abolished by passage over a rat-fibronectin Sepharose 4B affinity column; conversely, anti-mouse, but not anti-rat, fibronectin staining could be abrogated by absorption with insolubilized mouse fibronectin. At the interface between transplanted skin and host skin, the microvasculature from the two animals might intermingle; therefore, we examined the graft/host interface with our reciprocal anti-rat and anti-mouse fibronectin fluorescent antibody probes. At this location, at 2 wk after transplantation, the blood vessels stained with either anti-rat fibronectin (Fig. 3 A) or anti-mouse fibronectin (Fig. 3 B) but not with both antibody preparations (Fig. 3 A and B). Thus, these data further support the hypothesis that fibronectin is synthesized in situ in blood vessels and that fibronectin production is stimulated during microvasculature proliferation in response to injury."
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "13161383"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": [
            "These studies provide the first clear indication that blood vessels synthesize fibronectin in vivo in association with endothelial proliferation. Whereas plasma flbronectin can deposit in other sites and bathes the endothelium, we could detect no significant recruitment of circulating fibronectin into the matrix of these proliferating vessels, as has been shown in fibroblast matrices in vitro {{13161383}}. This suggests there might be a direct relationship between the observed endothelial proliferation and the increased production of fibronectin by cells in the vessel wall. Perhaps the increase in vessel wall fibronectin is an essential part of the reparative process of blood vessel basement membrane during wound healing or neovascularization."
          ]
        },
        {
          "header": "Summary",
          "paragraphs": [
            "During the time of tissue repair that ensues subsequent to tissue injury, blood vessel wall fibronectin increases concomitantly with endothelial proliferation and angiogenesis. However, the source of this blood vessel fibronectin had not been delineated. In this report we have demonstrated that microvascular fibronectin is produced in situ by the proliferating vessels surrounding excisional wounds. This finding was established by extirpating 3 mm of skin from the center of a well-healed rat xenograph on the flanks of immunosuppressed mice, harvesting the injured skin sites at various stages during the healing process, and staining the specimens with reciprocal speciesspecific anti-fibronectin. The proliferating donor vessels that surrounded the wounded graft had increased fluorescence staining with FITC conjugated mouse anti-rat fibronectin and no staining with rat anti-mouse fibronectin. This finding was taken as direct evidence tha[~ the fibronectin was produced in situ by the rat vessels and not derived from circulating mouse plasma."
          ]
        },
        {
          "header": "FIG. 1 .",
          "paragraphs": []
        },
        {
          "header": "Fie. 2 .",
          "paragraphs": []
        }
      ]
    },
    "discussion_txt": "These studies provide the first clear indication that blood vessels synthesize fibronectin in vivo in association with endothelial proliferation. Whereas plasma flbronectin can deposit in other sites and bathes the endothelium, we could detect no significant recruitment of circulating fibronectin into the matrix of these proliferating vessels, as has been shown in fibroblast matrices in vitro {{13161383}}. This suggests there might be a direct relationship between the observed endothelial proliferation and the increased production of fibronectin by cells in the vessel wall. Perhaps the increase in vessel wall fibronectin is an essential part of the reparative process of blood vessel basement membrane during wound healing or neovascularization.During the time of tissue repair that ensues subsequent to tissue injury, blood vessel wall fibronectin increases concomitantly with endothelial proliferation and angiogenesis. However, the source of this blood vessel fibronectin had not been delineated. In this report we have demonstrated that microvascular fibronectin is produced in situ by the proliferating vessels surrounding excisional wounds. This finding was established by extirpating 3 mm of skin from the center of a well-healed rat xenograph on the flanks of immunosuppressed mice, harvesting the injured skin sites at various stages during the healing process, and staining the specimens with reciprocal speciesspecific anti-fibronectin. The proliferating donor vessels that surrounded the wounded graft had increased fluorescence staining with FITC conjugated mouse anti-rat fibronectin and no staining with rat anti-mouse fibronectin. This finding was taken as direct evidence tha[~ the fibronectin was produced in situ by the rat vessels and not derived from circulating mouse plasma.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "As a scientific research assistant, I must first commend the authors on their rigorous and well-designed study to investigate the source of fibronectin in blood vessels during tissue repair. The study provides valuable insights into the mechanisms of tissue repair and angiogenesis, and sheds light on the role of microvascular fibronectin in these processes.",
          "",
          "The study begins by highlighting the well-established observation that fibronectin levels in blood vessel walls increase during tissue repair following injury. However, the source of this fibronectin had not been previously identified. To address this question, the authors used a rat xenograph model in which 3 mm of skin is excised from the center of immunosuppressed mice. The authors then harvested the injured skin sites at various stages during the healing process and stained the specimens with reciprocal species-specific antibodies against fibronectin.",
          "",
          "The findings of the study are fascinating. The authors discovered that the proliferating donor vessels surrounding the wounded graft had increased fluorescence staining with FITC-conjugated mouse anti-rat fibronectin, but no staining with rat anti-mouse fibronectin. This suggests that the fibronectin is produced in situ by the rat vessels and not derived from circulating mouse plasma.",
          "",
          "The authors note that the increased fibronectin expression in the proliferating vessels is likely to play a role in the regulation of endothelial proliferation and angiogenesis during tissue repair. This is supported by previous studies that have shown fibronectin to be a key regulator of endothelial cell behavior and angiogenesis.",
          "",
          "Overall, the study provides strong evidence that microvascular fibronectin is produced in situ by the proliferating vessels surrounding excisional wounds. These findings have important implications for our understanding of the mechanisms of tissue repair and angiogenesis, and may lead to the development of new therapeutic strategies for promoting tissue repair and regeneration.",
          "",
          "However, it is important to acknowledge some limitations of the study. For example, the authors only studied the expression of fibronectin in the context of tissue repair following excision injury. Further studies are needed to determine whether similar mechanisms are at play in other forms of tissue injury, such as those caused by inflammation or ischemia.",
          "",
          "In conclusion, the study by the authors provides valuable insights into the mechanisms of tissue repair and angiogenesis, and highlights the importance of microvascular fibronectin in these processes. Further research is needed to fully understand the role of fibronectin in tissue repair and regeneration, but the findings of this study provide a promising starting point for future studies."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.7303750514984131,
            "R": 0.7072145938873291,
            "F1": 0.7186082601547241
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.729216456413269,
            "R": 0.6622745990753174,
            "F1": 0.694135308265686
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          "This study suggests that microvascular fibronectin is produced in situ by the proliferating vessels"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.47871679067611694,
            "R": 0.7833317518234253,
            "F1": 0.5942625403404236
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.45277276635169983,
            "R": 0.715377926826477,
            "F1": 0.5545579791069031
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "]",
          "",
          "The production of fibronectin in microvascular endothelial cells has been demonstrated in situ by"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.45088547468185425,
            "R": 0.7042888402938843,
            "F1": 0.5497934222221375
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.439057320356369,
            "R": 0.6535658836364746,
            "F1": 0.5252549648284912
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "1005265",
    "title": "Effectiveness of biomarker-based exclusion of ventilator-acquired pneumonia to reduce antibiotic use (VAPrapid-2): study protocol for a randomised controlled trial",
    "externalids": {
      "ACL": "",
      "ArXiv": "",
      "CorpusId": "10037358",
      "DBLP": "",
      "DOI": "10.1186/s13063-016-1442-x",
      "MAG": "2473068687",
      "PubMed": "27422026",
      "PubMedCentral": "4947254"
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "Introduction",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Introduction",
            "paragraphs": [
              "Although quantitative trait locus (QTL) mapping is a very useful approach for identifying regions on the genome associated with a phenotype of interest, the mapped intervals are often very broad and contain many genes. The number of genes within the QTL must then be narrowed using genetic and bioinformatic approaches {{not_in_s2orc}}. These methods yield promising results, substantially reducing the candidate gene number. However, the steps are tedious and time-consuming, and the work required is commensurate with the starting number of genes. Thus, any method that narrows the QTL confidence interval, thereby excluding many less likely candidate genes, saves time and money. A recent advance in QTL mapping involves combining QTL cross data; in this process the data from different crosses are combined when a QTL is identified in the same chromosomal region and it is believed that the same gene underlies the QTL.",
              "In a QTL located using genotype data from F2 mice from a cross between inbred strains, a parental strain is determined to be the \"high-allele strain\" for that QTL by the effect plot for the peak marker; the homozygous genotype with the higher average phenotype carries the high allele. When genotype data from different crosses are combined, the high alleles are coded identically on a per-chromosome basis and the original cross is used as a covariate in analysis. Combining the datasets adds power to the QTL analysis, and if the underlying gene is the same, the resulting confidence interval is reduced. This method has been used in other species like the pig {{35047277}}, and it complements other recent advancements in QTL mapping, such as the meta-analysis of QTL LOD values {{11582607}}{{23997292}} or the pooling of assigned P-values along an entire QTL based on localized LOD scores {{14076080}}.",
              "The level of homology between genomes of different species is variable at both the species and genomic level, depending both on the overall relatedness of the two species at hand and on the underlying physiology and genomic architecture which define the species. Closely related species may share a similar genomic structure with contiguous patches of nearly identical sequence and shared clusters of genes. In the last decade, the sequencing of the mouse {{3835668}} and the rat {{4415600}} has allowed for a detailed analysis of the genomic divergence of the two species, revealing areas of both high and low sequence conservation {{37373525}}.",
              "For many phenotypes, QTL are concordant among different species. High Density Lipoprotein (HDL) cholesterol QTL are homologous between humans and mice {{11053934}}{{16028727}}, and kidney disease and hypertension QTL are homologous among rat, mouse, and human {{207257232}}{{1777782}}. This QTL concordance suggests an underlying shared contiguity of genetically mapped loci, which opens up the possibility to expand the combining of crosses beyond the use of one species. A successful combination is perhaps most likely using data from the mouse and the rat, as the two species are closely related and research involving both in the laboratory employs the use of inbred strains and similar crossing strategies.",
              "We explored the possibility of narrowing HDL cholesterol QTL by combining data from one rat cross {{44296026}}{{28604142}} and two mouse crosses {{28727880}}{{18939187}}{{15345967}}{{34891486}}. Our results show that in parallel with the combination of QTL datasets from the same species, it is possible to both increase the statistical significance of a QTL and narrow the confidence interval of the homologous QTL region using the combined data from two different species."
            ]
          }
        ]
      },
      {
        "header": "Materials and methods",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Materials and methods",
            "paragraphs": []
          },
          {
            "header": "QTL datasets",
            "paragraphs": [
              "The WxDA dataset. WOKW and DA rats were reciprocally crossed to produce two F2 crosses of 76 (WOKW\u00d7DA) F2 and 74 (DA\u00d7WOKW) F2 male and 72 (WOKW\u00d7DA) F2 and 68 (DA\u00d7WOKW) F2 female rats. Animals received a standard chow diet. Blood samples were taken at 28, 30, and 32 weeks, and HDL cholesterol levels were determined using a Roche Cobas Mira Plus auto analyzer. Values did not differ between the two crosses, and the crosses were therefore combined. Details of this dataset were previously published by {{28604142}} and {{not_in_s2orc}}. In these papers 126 microsatellite markers were used for genome-wide genotyping; since the publication, 19 additional markers have been genotyped, for a total of 145 markers.",
              "The PxD2 dataset. PERA/EiJ and DBA/2J mice were reciprocally crossed to produce 324 F2 progeny (166 females and 158 males). All animals were fed a chow diet until 6-8 weeks of age, followed by an 8-week atherogenic diet {{5496588}}. Ninety-seven microsatellite markers were used for genome-wide genotyping. Details of this dataset were previously published by {{34891486}}.",
              "The BxD2 dataset. This dataset has been described by {{18939187}}{{28727880}}{{15345967}}. C57BL/6J female mice were crossed to DBA/2J males, and the F1 mice were crossed to produce 111 female F2 progeny. The F2 females were fed a chow diet for 12 months and then fed an atherogenic diet for 16 weeks prior to phenotypic measurements. The mice were genotyped using 139 microsatellite markers."
            ]
          },
          {
            "header": "Placing rat markers on the mouse genome",
            "paragraphs": [
              "To determine the version 3.4 (November 2004 update) base pair positions of the microsatellite markers used in the WxD cross, marker IDs were used as input using the batch version of UCSC genome browser's Table Browser (http://genome.ucsc.edu/cgi-bin/ hgTables?command=start). Markers that were not available through the UCSC genome browser were looked up individually in Ensembl (www.ensembl.org); markers without basepair positions at that point were discarded from the dataset. Of the 145 markers, 138 were assigned updated positions. UCSC's web-based version of the batch coordinate conversion tool LiftOver (http://genome.ucsc.edu/ cgi-bin/hgLiftOver) was used to convert the rat genome positions to homologous mouse genome positions (NCBI build 37). Using this method, all but 10 of the 138 markers were converted; the remaining markers were positioned in Ensembl, and using Ensembl's Comparative Genomics tool, genes adjacent to the rat marker and both homologous and contiguous to the mouse genome were determined. The mouse position of the nearest homologous gene in a contiguous sequence of genes was used as the rat marker's homologous position. For example, using Liftover, base pair positions for rat marker D10Mgh12 are not homologous to the mouse genome. However, the gene Lcp2 is adjacent to D10Mgh12 at 19,019,978-19,066,754 bp and contiguous to the other homologous mouse positions on chromosome (Chr) 11 at 33,947,144-33,992,281. Using the above methods, all but three rat markers-D2Wox32, D8Mit6 and D10Mgh2were aligned to the mouse genome."
            ]
          },
          {
            "header": "Assigning genetic map positions to the rat markers",
            "paragraphs": [
              "Rat genome genetic positions for the rat markers were determined by interpolation to the SNP map recently published by the STAR Consortium {{12007275}}; the version 3.4 base-pair positions and STAR map cM positions are publicly available online at http:// www.well.ox.ac.uk/ rat_mapping_resources/ SNPmaps.html. Marker base pair positions were interpolated to the STAR map using MATLAB. The homologous mouse positions of the rat markers were interpolated to the Revised Shifman genetic map of the mouse {{13195015}}) using the Center for Genome Dynamic's online Mouse Map Converter (http:// cgd.jax.org/mousemapconverter/). Mouse chromosome and base pair positions were used as input, and sex-averaged cM positions were selected as output."
            ]
          },
          {
            "header": "Single dataset QTL analysis",
            "paragraphs": [
              "Individual QTL analyses were performed on the WxD rat dataset using rat positions, and then on each single mouse dataset using mouse positions. QTL analyses were performed using R version 2.8.1 and R/qtl version 1.11-12 {{2102099}}. X-chromosome genotyping data were omitted. Genome scans were performed using the EM algorithm {{15540652}} with 2 cM resolution, and significance thresholds were determined by permutation testing (1000 permutations). To determine which sex contributed more to a QTL, sex vs. HDL effect plots were created. Then, the datasets were separated by sex and reanalyzed to determine the adjusted QTL peak and confidence interval positions. For all analyses, 95% confidence intervals were determined by Bayesian analysis using the bayesint function in R/qtl, which calculates an approximate interval (endpoints around the maximum LOD) for a given chromosome using the genome scan output. Allele effects were determined using the effect plot function in R/qtl using the QTL peak marker or marker nearest to the peak as the reference marker."
            ]
          },
          {
            "header": "Combining the rat and mouse QTL datasets and multi-species QTL analysis",
            "paragraphs": [
              "The rat marker names were listed in chromosome and base-pair order in an Excel spreadsheet, along with the rat genetic position, the homologous mouse position, and the mouse genetic position. In the spreadsheet, the QTL peaks, confidence intervals and allele assignments for both the rat and mouse individual crosses were shaded. This allowed for the visualization of homologous and contiguous rat and mouse markers within a mouse QTL. Data were chosen for combination if the following criteria were met: 1) A set of markers within a rat HDL QTL significantly overlapped a mouse QTL based on homology with at least half of the markers from each species-specific QTL overlapping; 2) The markers were contiguous in both genomes; and 3) The peaks of the homologous QTL were close enough to suggest that the QTL from each species were caused by the same gene. In each case, the homologous peak positions were either adjacent or aligned in the same row in the Excel spreadsheet. In some cases, markers within a rat QTL were not included in the combinedspecies analysis. For example, a rat QTL marker was excluded if it was within a rat QTL but outside of a homologous mouse QTL.",
              "The original rat HDL cholesterol values were converted to the same unit as was used for mice (mg/dL). Prior to combining datasets, the HDL phenotype was log-transformed and then standardized by Z-score in R within each cross. The datasets were combined by mouse chromosome as in {{34891486}}, by coding high and low allele strains the same and then combining the datasets into one file.",
              "The data were combined and analyzed one chromosome at a time. We followed the linear models and analyses described in ) (see equations 1-5) for all QTL scans. R/qtl version 1.11-12 was used for QTL analyses, and the EM algorithm with a resolution of 2 cM was used for the scan and for determining significance thresholds with 1000 permutations. For the combined plots, the significance thresholds were based on the additive model. Only one covariate was used in the analysis, representing either cross, species or sex, as there were no nested factors to consider within each covariate."
            ]
          },
          {
            "header": "Determining the number of genes within the QTL confidence interval",
            "paragraphs": [
              "NCBI build 37 gene lists for Mus Musculus for chromosomes 11 and 12 were downloaded via Ensembl's BioMart (www.ensembl.org/biomart/martview); gene attributes selected for download were Ensembl Gene ID, Ensembl Transcript ID, Associated Gene Name, Gene Start (bp), Gene End (bp), and Description. The list of genes was downloaded as a csv file and then opened in MS Excel. The gene list was sorted by Gene Start base pair position and all gene repeats were removed (reflecting multiple transcripts per gene); such repeats were detected as duplicate Ensembl Gene IDs. To determine the number of genes within a QTL confidence interval, any gene with a starting base pair position within the confidence interval was counted. To convert from cM to Build 37 base pair positions, we used the Mouse Map Converter available on The Jackson Laboratory's Center for Genome Dynamics Website (http://cgd.jax.org/mousemap converter). We chose the sex-averaged cM position as input. In comparing the gene lists within the QTL confidence intervals prior to and after species combination, the confidence interval resulting from analysis with the speciesadditive covariate model was chosen for the post-analysis interval."
            ]
          }
        ]
      },
      {
        "header": "Results",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Results",
            "paragraphs": [
              "The results of the rat WxDA genome scan are shown in Figure 1. Running the first genome scan with sex as a covariate revealed the presence of multiple sex-specific QTL on Chrs 2, 4, 6, 7 and 9. Plotting genome scans using male and female data separately revealed which sex contributes to each sex-specific QTL detected in the covariate analysis and also unveiled any sex effects not detected in the first scan. Figures 1C and 1D show additional sex differences on Chrs 1, 3, 5, 8, 10 through 14, and 17. Due to the high incidence of sexspecific or sex-influenced QTL, the male and female data from the rat and mouse datasets were separated for individual species analysis; only QTL from these analyses were considered for multi-species combination. The separation of sexes also facilitates the analysis of the combined data, as the influence of the sex, species and cross covariates may be assessed as one covariate. The genome scan plots for the separate mouse crosses are not shown, as they were previously published {{28727880}}{{18939187}}{{15345967}}{{34891486}} The markers listed for rat Chrs 6 and 10 from the WxDA dataset, along with their rat genetic map positions and mouse genetic map positions on Chrs 12 and 11, are shown in Figure 2. The following rat-mouse QTL HDL data combinations were tested: Of these individual QTL, the PxD2 Chr 11 QTL is the only one that was previously published . The rat chromosomes 6 and 10 QTL were suggestive (LOD = 2.05 and LOD = 2.2, respectively) and not published previously.",
              "The phenotypes were standardized by Z-score in the individual QTL datasets and combined by identically coding the high-and low-allele strains. The results of the QTL analyses for the combinations are shown in Figure 3 and Figure 4. Table 1 lists the peaks, confidence intervals and LOD scores for the original mouse QTL, and for the additive and interactive QTL resulting from the combination of data from different species. The numbers of genes within the original and species-additive confidence intervals are shown in Table 2. Figure 3 shows the result of combining rat Chr 6 data with mouse Chr 12 data. The interactive, additive and non-covariate plots are all identical and overlaid, indicating that the QTL is probably not species-specific. The LOD is substantially higher than the original mouse QTL and the confidence interval is narrowed from 44 cM to 22 cM. The number of genes in the confidence interval is reduced from 613 to 304 genes. Figure 4 shows the result of combining rat Chr 10 data with the mouse PxD Chr 11 female QTL data resulted in a narrowing of the confidence interval and an increase in LOD from 2.3 to 3.7 (additive model). The additive and non-covariate plots are identical and overlaid. Although the interaction LOD score is somewhat higher than the additive model, the difference between them is not significant (\u0394LOD=0.8). Thus, we conclude that the QTL is not species-specific. The confidence interval was reduced from 41 cM to 19 cM and the number of genes in the confidence interval was reduced from 1343 to 761 genes."
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "14076080"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": [
            "Previously, genotype data from multiple mouse QTL datasets were successfully combined; if the QTL genes are the same, or close to each other with the same mode of inheritance and the same direction of the allele effect, the combined analysis results in higher LOD scores and narrower QTL. Using the same methodology, we combined HDL QTL datasets from two different species, the rat and mouse. Both combinations resulted in a successful increase in statistical significance and a narrowing of the QTL confidence interval. More importantly than the positional effects, however, this narrowing reduced the number of underlying candidate genes. The mouse QTL on Chr 11 was narrowed from a confidence interval spanning 41 cM to one spanning 17 cM, and the QTL on Chr 12 was reduced from 44 to 22 cM. The numbers of underlying candidate genes were reduced by 43 and 54 percent, respectively.",
            "These results are the first report of a successful combination of QTL data from two different species. While the methodology involved in combining cross data was not novel, the steps required to prepare the datasets for combination were thoroughly researched and tested by us and may now be repeated. It is notable that in clarifying the steps required for aligning rat and mouse QTL at the marker level, several factors normally considered in QTL analysis were omitted for simplicity and for the value of testing by assumption. We designed the data combinations so that sex, diet or any other condition were cofactors only in addition to species, i.e., the datasets may have been from different species and different sexes, but one dataset never included both male and female mice. Diet significantly contributes to lipid metabolism, and the rats and mice from the different crosses were not fed identical diets or for the same time periods prior to phenotyping (see materials and methods), but its effects were not considered in this study. In implementing this procedure for actual positional cloning, a model with multiple cofactors should be used.",
            "If QTL for the same trait are found in more than one species, it is common when narrowing the QTL to eliminate regions within the QTL that are not homologous to QTL regions for the same trait in the other species. This is especially true when the two species in comparison are extensively studied, as in humans and mice, or when the two species are very closely related, as among rodents. We recommend that in parallel or in conjunction with combining QTL data from multiple crosses within a species, combining QTL data from different species maybe be used along with all other QTL narrowing techniques, including cross-specific haplotyping and genome-wide association studies based on all strains. As previously suggested {{14076080}}, combining crosses complements the meta-analysis of QTL significance values.",
            "Although rat and mouse are closely related rodents, this study represents a useful approach for combining data from other species, even from less related species, like mouse and human -if the species are extensively studied and if there is an appropriate arrangement of contiguous loci, as required in the selection of the datasets combined in this analysis. In attempting a mouse-human study, we would need to address the differences in overall metabolism in theoretical speculation as well as in data analysis. For example, in humans, females on average have higher HDL cholesterol than males, but in mice the opposite relationship is true. Such sex or other covariate differences in physiology are usually considered after one-species analyses are complete, when individual candidate genes are investigated. In combining QTL data within a species, we assume that allelic effects are parallel, which allows us to combine the data. A successful combination, i.e., the achievement of a higher LOD and narrower confidence interval, provides substantial evidence that the two QTL share the same underlying gene. We have shown that such success is possible with two closely related species, and because the rat and mouse are both closely related and physiologically similar, it is not a far stretch to draw the same conclusion. If the two species were different regarding HDL metabolism, as in mice and humans, narrowing the QTL by combined data analysis suggests the same underlying gene. However, because the physiological differences are not considered in the combined QTL analysis, any successful combination is based on a multi-dimensional assumption. The underlying differences in physiology are so complex in their polygenicity and dependent pathways that the data analysis would possibly need to coincide with physiological modeling. It may be possible to quantify metabolic differences between species based on such modeling and then standardize the phenotype prior to combining QTL data accordingly.",
            "In addition to metabolic differences, genomic function at the level of recombination must be considered. In the analysis presented here, homologous QTL with an underlying contiguous sequence of loci were combined. Because differences in recombination rate between the rat and mouse were not taken into account during the analysis, a likely result is the skewing of the genetic map created in the context of the mouse. Recombination rates should be consulted as data from more divergent species are combined.",
            "In summary, sex, diet, species, physiology and genomic differences are all factors to consider when analyzing combined QTL datasets.",
            "In the examples we present, we use a simplified one-covariate model, encompassing all possible differences between the two datasets. Most importantly, we have described detailed methods for combining datasets from two different species. The successful combinations we achieved reveal a promising addition to the process of QTL narrowing. Had a combination not been successful, we would not have been able to declare that the individual QTL were species-specific as they could also be diet-, physiology-, sex-(in one case) or genome structure -specific. We recommend using a multi-covariate model when analyzing combined datasets for QTL-narrowing.",
            "Advances in comparative genomics contribute greatly to the understanding of animal development and physiology. Incorporating such knowledge is essential for the construction of genomes, proteomes and metabolic networks, and for theorizing and elucidating the mechanisms of molecular and phenotypic evolution. Now, the near completion of entire genome homology maps, like the one available between the rat and mouse, allows for the ability to combine data in a way that is useful both intellectually and statistically. The methods introduced here recognize the decade-long explosion of empirical advancements and the resulting field of bioinformatics.",
            "Beyond the alignment of homologous markers and genes, the integration of the QTL data for analysis adds a new level to species homology because theories of shared physiology may be tested mathematically. While statistical significance is both arbitrary and necessary for experimental validity, the increase in statistical significance (LOD) seen in these combinations is overshadowed by the potential insight gained into the underlying shared genome organization. The candidate genes underlie the QTL, but the replication and expression of those genes are dictated by the underlying DNA sequence and chromosome mechanics.  Rat-mouse concordance between cholesterol QTL on two different chromosomes. Genetic maps for rat chromosomes are shown on the left and for mouse chromosomes on the right. Rat markers names are shown in alignment with their genetic positions on the rat genome, and with their homologous positions on the mouse genome. The grey boxes show the original (pre-combination) QTL confidence interval for each species; the black bar represents the QTL peak. The LOD score for each QTL is shown adjacent to the peak. (A) Rat WxDA chr 6 male (most left) and female QTL are aligned with mouse BxD2 Chr 12 QTL. The most proximal rat marker (D6Pas1) is homologous to mouse Chr 17. The remaining markers align to mouse chromosome 12 contiguously. The female rat data was not chosen for this combination as the rat peak positions are not within the rat/mouse overlapping regions. (B) Rat WxDA Chr 10 female QTL is aligned with the mouse PxD2 Chr 11 female QTL. The region immediately surrounding the most distal rat marker, D10Mgh2, is not homologous to the mouse genome; the other markers align to mouse Chr 11 contiguously. Combining rat chromosome 6 data with mouse chromosome 12 data. All cM positions are with respect to mouse chromosome 12. Confidence intervals for the original and speciesadditive QTL are depicted by the black boxes; the box closest to the bottom represents the original 95% confidence interval in the mouse. The interactive and additive plots are indistinguishable, suggesting the QTL is caused by the same gene in both the rat and the mouse.  Combining rat chromosome 10 data with mouse chromosome 11 data. All cM positions are with respect to mouse chromosome 11. Confidence intervals for the original and speciesadditive QTL are depicted by the black boxes; the box closest to the bottom represents the original 95% confidence interval in the mouse. The interactive plot has a higher LOD than the additive plot, but not by a significant amount (\u0394LOD = 0.8). Overall, the LOD score is increased and the QTL is substantially narrowed, suggesting that the underlying gene is shared between the rat and mouse.  Table 1 Peaks, confidence intervals, and LOD scores for the original mouse QTL, and for the additive and interactive QTL resulting from the combination of data from different species."
          ]
        },
        {
          "header": "Figure 1 .",
          "paragraphs": []
        },
        {
          "header": "Figure 2 .",
          "paragraphs": []
        },
        {
          "header": "Figure 3 .",
          "paragraphs": []
        },
        {
          "header": "Figure 4 .",
          "paragraphs": []
        }
      ]
    },
    "discussion_txt": "Previously, genotype data from multiple mouse QTL datasets were successfully combined; if the QTL genes are the same, or close to each other with the same mode of inheritance and the same direction of the allele effect, the combined analysis results in higher LOD scores and narrower QTL. Using the same methodology, we combined HDL QTL datasets from two different species, the rat and mouse. Both combinations resulted in a successful increase in statistical significance and a narrowing of the QTL confidence interval. More importantly than the positional effects, however, this narrowing reduced the number of underlying candidate genes. The mouse QTL on Chr 11 was narrowed from a confidence interval spanning 41 cM to one spanning 17 cM, and the QTL on Chr 12 was reduced from 44 to 22 cM. The numbers of underlying candidate genes were reduced by 43 and 54 percent, respectively.These results are the first report of a successful combination of QTL data from two different species. While the methodology involved in combining cross data was not novel, the steps required to prepare the datasets for combination were thoroughly researched and tested by us and may now be repeated. It is notable that in clarifying the steps required for aligning rat and mouse QTL at the marker level, several factors normally considered in QTL analysis were omitted for simplicity and for the value of testing by assumption. We designed the data combinations so that sex, diet or any other condition were cofactors only in addition to species, i.e., the datasets may have been from different species and different sexes, but one dataset never included both male and female mice. Diet significantly contributes to lipid metabolism, and the rats and mice from the different crosses were not fed identical diets or for the same time periods prior to phenotyping (see materials and methods), but its effects were not considered in this study. In implementing this procedure for actual positional cloning, a model with multiple cofactors should be used.If QTL for the same trait are found in more than one species, it is common when narrowing the QTL to eliminate regions within the QTL that are not homologous to QTL regions for the same trait in the other species. This is especially true when the two species in comparison are extensively studied, as in humans and mice, or when the two species are very closely related, as among rodents. We recommend that in parallel or in conjunction with combining QTL data from multiple crosses within a species, combining QTL data from different species maybe be used along with all other QTL narrowing techniques, including cross-specific haplotyping and genome-wide association studies based on all strains. As previously suggested {{14076080}}, combining crosses complements the meta-analysis of QTL significance values.Although rat and mouse are closely related rodents, this study represents a useful approach for combining data from other species, even from less related species, like mouse and human -if the species are extensively studied and if there is an appropriate arrangement of contiguous loci, as required in the selection of the datasets combined in this analysis. In attempting a mouse-human study, we would need to address the differences in overall metabolism in theoretical speculation as well as in data analysis. For example, in humans, females on average have higher HDL cholesterol than males, but in mice the opposite relationship is true. Such sex or other covariate differences in physiology are usually considered after one-species analyses are complete, when individual candidate genes are investigated. In combining QTL data within a species, we assume that allelic effects are parallel, which allows us to combine the data. A successful combination, i.e., the achievement of a higher LOD and narrower confidence interval, provides substantial evidence that the two QTL share the same underlying gene. We have shown that such success is possible with two closely related species, and because the rat and mouse are both closely related and physiologically similar, it is not a far stretch to draw the same conclusion. If the two species were different regarding HDL metabolism, as in mice and humans, narrowing the QTL by combined data analysis suggests the same underlying gene. However, because the physiological differences are not considered in the combined QTL analysis, any successful combination is based on a multi-dimensional assumption. The underlying differences in physiology are so complex in their polygenicity and dependent pathways that the data analysis would possibly need to coincide with physiological modeling. It may be possible to quantify metabolic differences between species based on such modeling and then standardize the phenotype prior to combining QTL data accordingly.In addition to metabolic differences, genomic function at the level of recombination must be considered. In the analysis presented here, homologous QTL with an underlying contiguous sequence of loci were combined. Because differences in recombination rate between the rat and mouse were not taken into account during the analysis, a likely result is the skewing of the genetic map created in the context of the mouse. Recombination rates should be consulted as data from more divergent species are combined.In summary, sex, diet, species, physiology and genomic differences are all factors to consider when analyzing combined QTL datasets.In the examples we present, we use a simplified one-covariate model, encompassing all possible differences between the two datasets. Most importantly, we have described detailed methods for combining datasets from two different species. The successful combinations we achieved reveal a promising addition to the process of QTL narrowing. Had a combination not been successful, we would not have been able to declare that the individual QTL were species-specific as they could also be diet-, physiology-, sex-(in one case) or genome structure -specific. We recommend using a multi-covariate model when analyzing combined datasets for QTL-narrowing.Advances in comparative genomics contribute greatly to the understanding of animal development and physiology. Incorporating such knowledge is essential for the construction of genomes, proteomes and metabolic networks, and for theorizing and elucidating the mechanisms of molecular and phenotypic evolution. Now, the near completion of entire genome homology maps, like the one available between the rat and mouse, allows for the ability to combine data in a way that is useful both intellectually and statistically. The methods introduced here recognize the decade-long explosion of empirical advancements and the resulting field of bioinformatics.Beyond the alignment of homologous markers and genes, the integration of the QTL data for analysis adds a new level to species homology because theories of shared physiology may be tested mathematically. While statistical significance is both arbitrary and necessary for experimental validity, the increase in statistical significance (LOD) seen in these combinations is overshadowed by the potential insight gained into the underlying shared genome organization. The candidate genes underlie the QTL, but the replication and expression of those genes are dictated by the underlying DNA sequence and chromosome mechanics.  Rat-mouse concordance between cholesterol QTL on two different chromosomes. Genetic maps for rat chromosomes are shown on the left and for mouse chromosomes on the right. Rat markers names are shown in alignment with their genetic positions on the rat genome, and with their homologous positions on the mouse genome. The grey boxes show the original (pre-combination) QTL confidence interval for each species; the black bar represents the QTL peak. The LOD score for each QTL is shown adjacent to the peak. (A) Rat WxDA chr 6 male (most left) and female QTL are aligned with mouse BxD2 Chr 12 QTL. The most proximal rat marker (D6Pas1) is homologous to mouse Chr 17. The remaining markers align to mouse chromosome 12 contiguously. The female rat data was not chosen for this combination as the rat peak positions are not within the rat/mouse overlapping regions. (B) Rat WxDA Chr 10 female QTL is aligned with the mouse PxD2 Chr 11 female QTL. The region immediately surrounding the most distal rat marker, D10Mgh2, is not homologous to the mouse genome; the other markers align to mouse Chr 11 contiguously. Combining rat chromosome 6 data with mouse chromosome 12 data. All cM positions are with respect to mouse chromosome 12. Confidence intervals for the original and speciesadditive QTL are depicted by the black boxes; the box closest to the bottom represents the original 95% confidence interval in the mouse. The interactive and additive plots are indistinguishable, suggesting the QTL is caused by the same gene in both the rat and the mouse.  Combining rat chromosome 10 data with mouse chromosome 11 data. All cM positions are with respect to mouse chromosome 11. Confidence intervals for the original and speciesadditive QTL are depicted by the black boxes; the box closest to the bottom represents the original 95% confidence interval in the mouse. The interactive plot has a higher LOD than the additive plot, but not by a significant amount (\u0394LOD = 0.8). Overall, the LOD score is increased and the QTL is substantially narrowed, suggesting that the underlying gene is shared between the rat and mouse.  Table 1 Peaks, confidence intervals, and LOD scores for the original mouse QTL, and for the additive and interactive QTL resulting from the combination of data from different species.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "Title: Combining QTL Data from Different Species: A Novel Approach for Narrowing QTL Intervals",
          "",
          "Abstract:",
          "",
          "Quantitative trait locus (QTL) analysis is a powerful tool for identifying genetic regions associated with complex traits. However, the process of narrowing QTL intervals can be challenging, particularly when working with multiple species. Here, we present a novel approach for combining QTL data from different species, using a combination of standardized high-density lipoprotein phenotype values and genotype data from the rat and mouse. By combining data within homologous regions from the rat onto the mouse genome and vice versa, we were able to narrow QTL intervals and identify fewer candidate genes encompassed by the QTL. This approach promises to be a useful tool for narrowing QTL intervals and identifying the underlying genetic causes of complex traits.",
          "",
          "Introduction:",
          "",
          "Quantitative trait locus (QTL) analysis is a widely used method for identifying genetic regions associated with complex traits. Once a QTL is detected, the region is narrowed by positional cloning in the hope of determining the underlying candidate gene. However, the process of narrowing QTL intervals can be challenging, particularly when working with multiple species. Here, we present a novel approach for combining QTL data from different species, using a combination of standardized high-density lipoprotein phenotype values and genotype data from the rat and mouse.",
          "",
          "Methods:",
          "",
          "We combined standardized high-density lipoprotein phenotype values and genotype data for the rat and mouse using information from one rat cross and two mouse crosses. We successfully combined data within homologous regions from rat Chr 6 onto mouse Chr 12, and from rat Chr 10 onto mouse Chr 11. The combinations and analyses resulted in QTL with smaller confidence intervals and increased logarithm of the odds ratio scores.",
          "",
          "Results:",
          "",
          "The numbers of candidate genes encompassed by the QTL on mouse Chr 11 and 12 were reduced from 1343 to 761 genes and from 613 to 304 genes, respectively. This is the first time that QTL data from different species were successfully combined, demonstrating the power of this approach for narrowing QTL intervals.",
          "",
          "Discussion:",
          "",
          "Our findings demonstrate the potential of combining QTL data from different species to narrow QTL intervals and identify fewer candidate genes encompassed by the QTL. This approach can be particularly useful when working with complex traits that are likely to be influenced by multiple genetic factors. However, it is important to note that the success of this approach depends on the quality and quantity of the available data, as well as the choice of species and genetic markers used.",
          "",
          "Conclusion:",
          "",
          "In conclusion, combining QTL data from different species is a promising approach for narrowing QTL intervals and identifying the underlying genetic causes of complex traits. By combining standardized high-density lipoprotein phenotype values and genotype data from the rat and mouse, we were able to successfully narrow QTL intervals and identify fewer candidate genes encompassed by the QTL. This approach has the potential to be a valuable tool for identifying genetic causes of complex traits in a variety of species."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.63428795337677,
            "R": 0.6627936363220215,
            "F1": 0.6482275724411011
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.5410599708557129,
            "R": 0.5733168125152588,
            "F1": 0.556721568107605
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          ""
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "**Discussion**",
          "",
          "Combining data from two different species to narrow a quantitative trait locus (QTL"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.38969627022743225,
            "R": 0.558433473110199,
            "F1": 0.45904991030693054
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.2586982250213623,
            "R": 0.4709506034851074,
            "F1": 0.3339526653289795
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10054248",
    "title": "Graph-based Clustering of Synonym Senses for German Particle Verbs",
    "externalids": {
      "ACL": "W16-1805",
      "ArXiv": "",
      "CorpusId": "10020161",
      "DBLP": "conf/mwe/WittmannMW16",
      "DOI": "10.18653/v1/W16-1805",
      "MAG": "2513477335",
      "PubMed": "",
      "PubMedCentral": ""
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "Introduction",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Introduction",
            "paragraphs": [
              "Collaborative filtering (CF) has been successfully used in a large number of applications, such as e-commerce web-sites {{262736977}} and on-line communities in a series of domains {{2455777}}{{2616594}}{{5546580}}.However, some challenges are still offered.Most of these systems deal with very large amounts of data and frequently suffer from scalability problems.CF systems should be able to efficiently process data on-line as it arrives, in order to keep the system up-to-date.This poses two problems:",
              "-Scalability: as new users and items enter the system, time and memory requirements increase.At some point, the data processing rate may fall below the data arrival rate.-Accuracy: as new data elements add up, the weight of each individual data element decreases.This causes the system to become less and less sensitive to recent information.",
              "In order to overcome these problems, forgetting mechanisms can be implemented.When forgetting older data, it is possible to reduce processing time and memory usage and maintain the system's sensitivity to recent data.",
              "In this work, we present two forgetting mechanisms: sliding windows and fading factors.We look at user activity as a data stream {{34783548}} in which data elements consist of individual user sessions, each containing a set of implicitly binaryrated items-seen items.Then we implement and evaluate forgetting mechanisms in nonincremental and incremental versions of CF algorithms.",
              "The remainder of this paper is organized as follows.Section 2 refers to related work.In Sect.3, we introduce the forgetting approaches to CF. Section 4 describes the four algorithms used in the experiments.Evaluation and results are presented in Sect. {{not_in_s2orc}}. Conclusions and future work are presented in Sect.6."
            ]
          }
        ]
      },
      {
        "header": "Related work",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Related work",
            "paragraphs": [
              "Collaborative filtering has been an active research field in recent years, with many enriching advances.However, few work has focused on the study of temporal effects in CF.In {{1487925}} and {{not_in_s2orc}}, Ding et al. use time-weighted ratings to predict new ratings in an item-based CF algorithm.The authors incorporate a time function in the rating prediction function, thus giving more weight to recent ratings and less weight to older ratings.In {{15280943}}, Nasraoui et al. use their TECNO-STREAMS {{6513417}} stream clustering algorithm to learn from evolving usage data streams.Koren {{not_in_s2orc}} addresses the problem of time-varying usage data using a model that is able to separately deal with multiple changing concepts.",
              "In the field of data stream processing {{34783548}}, several methods have been proposed to provide algorithms with mechanisms able to deal with concept drifts.Most of these are based on the idea of \"forgetting\" older data elements, whether by using sliding windows or decay functions based on fading factors.The FLORA system {{15558554}} tries to deal with concept drifts by using an adaptive time window, controlled by heuristics that track system performance.In {{31439485}}, techniques are proposed to maintain sequence-based windows-where the number of data elements in the window is fixed-and time-based windows-where data elements belong to a determined time interval.Gradual forgetting is studied in {{15355119}}, where the author uses a recommender system to study the proposed method in the context of drifting user preferences.",
              "Incremental CF has been presented in {{15396909}}, where a userbased algorithm incrementally updates user similarities every time new data is available.In {{206867959}} and {{15830350}}, item-based and user-based incremental algorithms that use implicit binary ratings are proposed and evaluated.",
              "The algorithms proposed in this article are based on the ones presented in {{206867959}} and {{15830350}}.We propose, implement, and evaluate forgetting mechanisms in incremental and nonincremental algorithms using binary usage data.Our research suggests that forgetting mechanisms have the potential to improve both the scalability and the predictive ability of user-based and item-based CF."
            ]
          },
          {
            "header": "Collaborative filtering with forgetting mechanisms",
            "paragraphs": [
              "Neighborhood-based collaborative filtering works by calculating similarities between users-user-based-or itemsitem-based.Users are similar if they share many preferred items.Similarity between two items is determined by the number of users that simultaneously share interest in both items.This information is obtained by inspecting user sessions.A user session s by user u contains a list of items i.These session items are the ones for which u has given positive feedback during a well defined, usually short, period in time.Based on this information, a similarity matrix S is built, containing pairwise user or pairwise item similarities."
            ]
          },
          {
            "header": "Sliding windows",
            "paragraphs": [
              "Forgetting can be performed using a sequence-based sliding window of size n that retains information about the n most recent user sessions.One direct way to implement sliding windows in a user-based or item-based CF system is to rebuild the similarity matrix using data of a fixed size sequence-based window holding the w latest sessions.Each time a new session s i is available, the window moves one session forward discarding session s i\u2212n\u22121 and including s i .Then the new window is used as learning data to build a new similarity matrix.",
              "Nonincremental algorithms can be easily adapted to use sliding windows, since they recalculate the whole similarity matrix each time new session data is available.The additional task is to move the window forward and use that window to rebuild the matrix.It is important to note that nonincremental algorithms process individual sessions several times-as many as the length of the window-which is not an ideal way to deal with data streams {{34783548}}.",
              "Traditional nonincremental algorithms use all of the past data to recalculate S.This is strategy is hereby referred to as a growing window approach, since the data window continuously grows as it incorporates more sessions.",
              "For incremental algorithms the adaptation is not so simple because the similarity matrix is not rebuilt from scratch {{15830350}}{{15396909}}.The similarity values corresponding to the items in each new session are updated, while other values are kept.In order to implement a sliding window approach in incremental algorithms it is necessary, at each update, to remove from the similarity matrix the information that was added by the oldest session in the window.This requires that every session is processed twice, and as in the nonincremental case, memory is required to hold session data for all sessions in the window.",
              "One possible alternative to sequence-based windows is to use time-based windows {{31439485}}.In this case sessions are timestamped so that the system knows which sessions it should discard.This approach has the disadvantage to make the window size variable, depending on the data rate.The number of sessions in the window increases for fast data rates and decreases with slow rates.Large windows carry more information but require more time and memory to be processed.Small windows are easy to process but may contain insufficient information.",
              "The sliding window approach basically considers the n most recent sessions to build the similarity matrix S. To illustrate, consider a sequence of the first n user sessions {s 1 , s 2 , . . ., s n }, each containing a set of items rated by one user.First, S is built from data in sessions {s 1 , . . ., s n }.",
              "Then, for each new session s a , the model is rebuilt with data from sessions {s a\u2212n , . . ., s a }, creating a window that slides through data as it arrives.Algorithm 1 (UBSW) is a classical user-based nonincremental algorithm adapted for using sliding windows.This algorithm takes in a sequence of user sessions L = {s 1 , s 2 , . ..}.The first n sessions are used to build an initial user vs. user similarity matrix S.Then, for each new user session in L, S is recalculated using a new window consisting of the latest n sessions.Item activation weights are then calculated and the Nrecs items with the highest weights are recommended.",
              "IBSW (Algorithm 2) is an item-based version of the nonincremental algorithm, also using sliding windows.This algorithm is based on the item-based nonincremental algo-",
              "-Recommend the Nrec items with the highest activation weight rithm in {{15830350}}.As with UBSW, the algorithm takes in a sequence of user sessions L and, for each new session, recalculates an item vs. item similarity matrix S using a window with the latest n sessions.Then the item weights are calculated and recommendations are provided accordingly."
            ]
          },
          {
            "header": "Fading factors",
            "paragraphs": [
              "Sliding windows provide an effective but abrupt way to forget older data.In many cases, however, past data is not necessarily obsolete, and can contain valuable information {{34783548}}{{15355119}}.Fading factors {{5767782}} provide a mechanism to gradually forget past data.Fading factors with incremental algorithms can be implemented by multiplying the similarity matrix by a factor \u03b1 < 1 before each update.In user-based algorithms, the similarity matrix contains similarities measured between every pair of users in the system.In item-based algorithms, similarities are measured between every pair of items.In both cases, the fading factor causes similarities to continuously decrease through time unless they are reinforced by recent session data.If the similarity reaches a lower threshold value, it can be assumed to be zero.This method is simple to implement and requires a single scan of each session.",
              "Figure 1 illustrates the session weight curve that is obtained at session index 500 using different factors.We can observe that the decay for recent sessions is higher than for older sessions.Using forgetting curves with different shapes requires a more complex approach.Because session history is not kept, there is no way to know how to apply forgetting to each similarity value.Keeping ordered session data in memory would allow us to use different decay curves, however, it would also introduce complexity in the update process.Each session would have to be processed at every update until its weight is zero.",
              "Fading factors can also be implemented in nonincremental algorithms if all considered sessions are kept in memory in the same order by which they arrived.A function of the session index can then be applied when rebuilding the similarity matrix, giving less weight to older sessions.However, this poses the same problem of using sliding windows: each session is processed several times, which is undesirable.",
              "Whereas with sliding windows old data is abruptly forgotten, the idea of fading factors is to slowly decrease the importance of sessions as they grow old.This can be achieved by manipulating the similarity matrix S. Incremental algorithms using fading factors simply multiply S by a factor \u03b1 \u2264 1 before updating them with the active session data.With \u03b1 < 1, at each new session, older sessions become less important.With \u03b1 = 1, older data weight is maintained.To incrementally update S we also maintain a frequency matrix F with the number of items corated by each pair of users (user-based) or the number of users that corated each pair of items (item-based).The principal diagonal in F gives us the number of items evaluated by each user-in the user-based case-and the number of users that evaluated each itemin the item-based case.The matrix F contains all necessary data to calculate any similarity in S. The values in F are incremented by 1 for every pair of items that are contained in the same session (item-based), or for every pair of users that have seen the same item (user-based).Then only the similarities in S that are affected by changes in F are recalculated.",
              "Forgetting is obtained by multiplying matrices S and F by a fading factor \u03b1 < 1.When using \u03b1 = 1 no forgetting occurs.It is important that both matrices S and F are multiplied by \u03b1.Because similarities in S are calculated directly from values in F , forgetting must be reflected also in F .Otherwise, every time rows and columns in S were updated, no forgetting would occur for them.Also, if only F is multiplied by \u03b1, nonupdated rows and columns in S would not be forgotten.",
              "UBFF (Algorithm 3) is a modified version-using fading factors-of the user-based incremental algorithm originally",
              "-Update the row/column of S corresponding to user u a :",
              "-Determine the activation weight W i of each item i never seen before by u a (Eq.( 3)) -Recommend the Nrec items with the highest activation weight to u a described in {{15830350}}.A cache matrix F maintains the number of items covisited by every pair of users.Additionally, the database D of user sessions and session weights needs to be maintained.This database is required in the matrices update step in order to reflect the forgetting of user sessions and in the recommendation step to retrieve recommendable items from the nearest neighbors.Each element s, w in D is a pair containing session data s and session weight w.The initial weight of each new session is set to 1. Session weights are then multiplied by the fading factor \u03b1 every time a new session is processed.This way, sessions loose weight as they grow older.",
              "Values in F are calculated as the number of items simultaneously present in the active session and every other (past) session.In order to reflect the forgetting of the older sessions, this number needs to be multiplied by the weight of the oldest of the two sessions at each cell in the active session row/column in F .For example, let the first session s 1 of user u 1 be composed of 2 items i and j .Also, let the tenth session s 10 (of user u 10 ) be composed of the same two items i and j .This would make F u 1 ,u 10 = 2.However, at session s 10 , previous session weights are already lower.Specifically, the weight of s 9 is W D 9 = \u03b1, the weight of s 8 is W D 8 = \u03b1 2 , the weight of s 7 is W D 7 = \u03b1 3 , and so on.The first session Algorithm 4 IBFF Input: L, Nrecs, \u03b1, n Output: recommendation list -Initialize matrices S and F using sessions {s 1 , . . ., s n } \u2282 L -For each new session s a \u2208 L, a > n (by user u a )",
              "-Determine the activation weight W i of each item i never seen before by u a (Eq.( 4)) -Recommend the Nrec items with the highest activation weight to u a -Update S and F :",
              "-Let I a be the set of items in session s a -Multiply all values in S and F by a fading factor \u03b1 S = \u03b1S, F = \u03b1F (",
              "-For each new item, add a row and column to F and to S -For each pair of items in (i, j ) in I a , update the corresponding row/column in F :",
              "-For each item i a in I a update the corresponding row (column) of S:",
              "F i a ,i a \u00d7 F .,. {{not_in_s2orc}} s 1 has weight W D 1 = \u03b1 9 , so this must be reflected in the cache matrix as",
              "The incremental item-based algorithm with fading factors (IBFF-Algorithm 4) is based on the incremental itembased algorithm in {{15830350}}.In order to incrementally update S we also need save in memory the auxiliary cache matrix F with the number of users that evaluate each pair of items.The principal diagonal gives us the number of users that evaluate each item.Forgetting is obtained the same way as in UBFF, but in this case, the user session database is not required.",
              "One difference between UBFF and IBFF is that with the first, recommendations are performed after updating the model, while with the latter, recommendations are provided before updating the model.With UBFF, the session belonging to the active user needs to be processed before the recommendation step because similarities between the active user and other users may not yet be present in S. With IBFF, S already contains enough information to compute the recommendations before performing the update.UBFF and IBFF are the same algorithms used in previous work {{15830350}}, with only the changes that are strictly necessary to function with fading factors.This allows us to make comparisons between past and present results."
            ]
          },
          {
            "header": "Algorithms",
            "paragraphs": [
              "All algorithms take binary usage data as input.This data contains the set of items visited by each user, grouped in sessions and ordered by the session end time.A session is considered to be the set of items visited or rated by a single user in a certain time frame.For the purpose of this work, datasets contain only anonymous users, so each session corresponds to a unique user.Sessions containing a single item are removed.Datasets are processed to build a similarity matrix S that contains the similarities between all pairs of usersin the user-based version-or items-in the item-based version.Similarity between a pair of users (or items) is calculated using a simplified version of the cosine measure for binary ratings {{206867959}}{{15830350}}.If U and V are the sets of items that users u and v evaluated, then the user-based similarity between u and v is given by",
              "For the item-based version, if I and J are the sets of users that evaluated items i or j , the similarity between i and j is given by"
            ]
          },
          {
            "header": "Evaluation and results",
            "paragraphs": [
              "In this section, we present results obtained in experiments conducted to evaluate the impact of forgetting mechanisms in CF algorithms.Our main goal is to assess the potential of forgetting mechanisms to improve scalability and predictive ability.We also implement an evaluation methodology that is able to deal with usage data streams.This approach allows us to continuously monitor the behavior of the algorithms."
            ]
          },
          {
            "header": "Datasets",
            "paragraphs": [
              "Four datasets are used in the experiments.Table 1 describes each dataset.Sessions with less than 2 items were removed.In all datasets, every user performs exactly one session, meaning that each session corresponds to a different unique user.Datasets ART1 and ART2 are synthesized datasets with an abrupt change.Both ART1 and ART2 consist of identical sessions with 4 items.These sessions contain the items {a, b, c, d} at the beginning and then the item d is replaced by a new item e.This change occurs at session index 400 in ART1 and session 500 in ART2.",
              "ELEARN and MUSIC are natural datasets extracted from web usage logs of an e-learning website (ELEARN) and listened tracks from a social network1 dedicated to nonmainstream music (MUSIC)."
            ]
          },
          {
            "header": "Evaluation methodology",
            "paragraphs": [
              "In all experiments we have used the all-but-one protocol as described in {{2885948}}, but following a chronological ordering for sessions.First, the dataset is split in a training set and a test set.Sessions are not selected randomly to the training and test sets, but rather according to their order.This means that for a split of 0.2, for example, the training set is composed of the first 20 % sessions and the test set is composed of the remaining 80 %.For IBSW and UBSW, the training set is considered to be the first window.For IBFF and UBFF, an initial training set containing the first 10 % of sessions is used to build the initial matrices S and F .This initial training set is required in order to avoid cold-start problems {{not_in_s2orc}}.After splitting the dataset, an item is randomly hidden from each session in the test set.Then recommendations made to each user are evaluated as a hit if the hidden item is among the recommended items.",
              "To evaluate recommendations, we use Precision and Recall, with the following definition: Precision = # hits # recommended items {{15355119}} Recall = # hits # hidden items {{262736977}} One other possible measure, that combines Precision and Recall, is the F1 measure:",
              "Since one single item from each session is hidden, Recall is either 1 (hit) or 0 (miss), and Precision is obtained dividing Recall by the number of recommended items, which is a predefined parameter (see Sect. 5.2.1).For this reason, we present predictive ability using Recall only.Precision and/or F1 scores can be easily calculated from Recall and the number of recommended items.",
              "Recall is calculated sequentially for each user.At the end, we obtain a sequence of hits and misses, and an overall average can be calculated.However, as this average may hide different behaviors through time, we study the evolution of Recall values through time for each experiment.A moving average of Recall is used to obtain values and graphics that illustrate how accuracy varies with time, as new sessions arrive.It is important to use this approach because we want to study how Recall evolves with and without implementing forgetting mechanisms.In the Recall graphics, a moving average consisting of arithmetic mean of the previous 40 Recall values (0 or 1) is used to draw the graphics.In practice, this represents the proportion of hits in the previous 40 recommendation requests.",
              "Computational time spent building or updating the matrices is provided for natural datasets ELEARN and MUSIC.Time measurements allow us to empirically study the scalability of algorithms using different datasets and parameters.",
              "Reaction to sudden changes in data, using natural datasets ELEARN and MUSIC, is studied introducing artificial changes in these datasets.For the purpose of our experiments, we randomly chose 50 % of the existing items and change their names from a certain session onwards, causing a sudden drift.The algorithms are then evaluated using these modified datasets.These new datasets keep all the characteristics of a natural dataset, only with a drift of 50 % of the items."
            ]
          },
          {
            "header": "Evaluation parameters",
            "paragraphs": [
              "The following parameters must be set to conduct the tests:",
              "k: the maximum number of neighbors (users or items) to consider in S when computing recommendations;",
              "-Nrec: the number of items to recommend at each recommendation request; w p : the window size in percentage of total sessions in the dataset (for IBSW and UBSW); \u03b1: the fading factor.In IBFF and UBFF, S and F are multiplied by this factor before updating with new data.",
              "In the experiments with synthesized datasets (ART1 and ART2), values k = 2 and Nrec = 1 are used.These low values are chosen because synthesized datasets have a low number (4) of items.With all other datasets values k = 5 and Nrec = 5 are used.These values are chosen taking into account results obtained in {{not_in_s2orc}} and the computational resources required to run the experiments.",
              "For the incremental algorithms, four values of \u03b1 are tested.Values close to 1 are chosen so that the forgetting is not too abrupt.The nonforgetting factor \u03b1 = 1 is used as reference to measure and compare the impact of forgetting with different factors.",
              "To study the impact of forgetting, we compare UBSW and IBSW, which use sliding windows, with their nonforgetting versions that use growing windows.These are called UBGW and IBGW.With growing windows, at each session s i , all past sessions {s 1 , . . ., s i\u22121 } are used to build S. For UBGW and IBGW, w p is the percentage of sessions used to build the initial matrix S.",
              "Implementation and hardware details All algorithms were implemented using R version 2.11.0 with the package spam version 0.21-0 to handle sparse matrices.The hardware used in the experiments was a machine with a dual 2.67 GHz core processor and 2Gb of RAM, running the Ubuntu 8.04 Linux OS."
            ]
          },
          {
            "header": "Experiments with synthesized datasets",
            "paragraphs": [
              "It is possible to observe in Fig. 2 that, on the synthesized datasets, the algorithms with forgetting mechanisms tend to recover faster from abrupt changes than nonforgetting algorithms.As older data is forgotten, the initial conditions are not considered, providing a faster adaptation to new situations.Results for IBFF in Fig. 2(b) illustrate the behavior of the algorithm using different fading factors: recovery is faster for lower values of \u03b1 and slower for higher values of \u03b1.Without forgetting, none of the algorithms recover completely until the end of the test.For \u03b1 = 0.97, Recall = 1 is recovered about 400 sessions after the change.For \u03b1 = 0.98 the recovery occurs after 600 sessions, and for \u03b1 = 0.99, almost 1500 sessions are necessary to recover.Without forgetting (\u03b1 = 1), recovery does not happen during the 2,000 sessions in the dataset.With IBSW and IBGW, shown in Fig. 4, the experiments with the ELEARN dataset show a better recovery from change with IBSW.This only happens right after change occurs.Then, shortly after session 200, IBGW recovers and remains better than IBSW.At the end of the experiment, IBSW drops rapidly to values close to 0.4, while IBGW holds on to values around 0.7."
            ]
          },
          {
            "header": "Incremental with fading factors",
            "paragraphs": [
              "Figure 5 shows the reaction of UBFF to change.With the ELEARN dataset, there is a better reaction of the algorithm with lower fading factor.The best results are obtained with \u03b1 = 0.97, from session 200 to around session 450.The second best results are achieved with \u03b1 = 0.98.Analyzing the lines in Fig. 5(a), from session 200-where change is introduced-to around session 350, Recall is generally higher for lower fading factors.In that interval, the lower recall values are obtained with \u03b1 = 1 (without forgetting).",
              "With the MUSIC dataset (Fig. 5(b)) results are very similar for all 4 fading factors.UBFF with \u03b1 = 1 seems to have a slightly better performance most of the time.",
              "With the item-based version (IBFF), shown in Fig. 6, recall is generally lower with \u03b1 < 1 than with \u03b1 = 1, although it is possible to see a better response to change with the ELEARN dataset with \u03b1 = 0.98 and \u03b1 = 0.99 between sessions 200 and 350.With the MUSIC dataset, the highest recall is obtained without forgetting (\u03b1 = 1).case.In the case of matrix rebuild time-for IBSW and UBSW-the similarity matrix S is rebuilt from scratch every time a new session is available.With IBFF and UBFF values in matrices S and F are selectively updated.In any case, these matrices are typically very large and tend to grow very fast as new users and items enter the system.In this section, we study the scalability of the algorithms by measuring the time needed to rebuild or update the similarity matrix.",
              "Incremental CF algorithms, instead of rebuilding the entire similarity matrix, only update the similarity values that can potentially change with a specific session.As shown in {{15830350}}, this has a considerably lower complexity than a complete rebuild.By looking at the time scales in Figs. 9 (UBFF) and 10-IBFF, and comparing them with those of nonincremental algorithms (Figs. 7 and 8), we can verify that incremental algorithms-UBFF and IBFF-update times are much shorter than rebuild times by UBSW/UBGW and IBSW/IBGW.",
              "Analyzing UBFF update times in Fig. 9, it is clear that time increases as more data is available.Additionally, there seem to be very little differences between runs with different fading factors, including \u03b1 = 1 (no forgetting).With IBFF (Fig. 10), results are also very similar between different fading factors.",
              "Looking at the combination between user-based/itembased and the ELEARN/MUSIC datasets, we observe that with the ELEARN dataset, IBFF performs better than UBFF, while with the MUSIC dataset the opposite occurs.This is an effect similar to the one observed with nonincremental al-Fig.7 Matrix rebuild time with nonincremental algorithms (ELEARN dataset).A moving average (n = 40) is used to smoothen the lines gorithms, and again is caused by the proportion of the number of items and users in the datasets."
            ]
          },
          {
            "header": "Nonincremental with sliding windows",
            "paragraphs": [
              "Nonincremental algorithms need to recalculate the whole similarity matrix S every time a new session occurs.Figure 7 illustrates the time required to rebuild the matrix as the ELEARN dataset is processed.Comparing the sliding window algorithms with their growing window versions, it is clear that both user-based and item-based versions using growing windows (UBGW and IBGW) time to recalculate S grows super-linearly with the number of sessions.The sliding window versions tend to maintain time.Comparing the user-based algorithms with the item-based ones, the first have a more stable record than the latter.This happens because with user-based algorithms, S has exactly the same number of rows and columns as the number of sessions in the window.With item-based algorithms, the number of items in the window is not fixed-with sliding window-nor it grows one by one-with growing windows-since the order of appearance of new items is not sequential.This leads to memory allocation and garbage collection processes to run frequently, causing extra time consumption in some iterations.",
              "Figure 8 shows the time to rebuild S with the MUSIC dataset.As with the ELEARN dataset, growing window algorithms take increasingly more time to rebuild S while the sliding window algorithms tend to maintain the time required to rebuild S.However, two main differences between ELEARN and MUSIC are clear.First, the difference between item-based and user-based algorithms is much higher with MUSIC.Second, the item-based versions take longer than the user-based versions with MUSIC, which is the opposite behavior of ELEARN.This happens because the number of items in MUSIC (3121) is much higher than the number of users (785), leading to much larger matrices when using item-based algorithms.The item-based matrices are large enough to cause memory swapping, as the available RAM is not enough to store them.This causes very high fluctuations (Fig. 8(b))."
            ]
          },
          {
            "header": "Update times with Netflix data",
            "paragraphs": [
              "When using fading factors, the matrices S and F become sparser as similarities and cooccurrence frequencies are forgotten.The algorithms actually take advantage of this sparsity to optimize the data structures where S and F are stored, leading to a better scalability.This is not visible in ELEARN and MUSIC because these datasets are not large enough to completely forget similarities and frequencies.However, using a longer dataset it is possible to observe that fading factors improve scalability.To verify this, we calculate update times of IBFF with a dataset consisting of 5,000 sessions sampled the well-known Netflix Prize {{not_in_s2orc}} dataset.",
              "Because this dataset holds ratings given by users to movies within a discrete scale of 1 (worse) to 5 (better), we eliminated all ratings below 4, retaining only ratings of 4 or 5.This ensures that only positive ratings are considered.Then we organized the dataset in sessions.Because timestamps in the dataset only contain the date (not the time), we consider one session to be the list of rated items by one user in the same day.We then sampled 5,000 random sessions from the whole data, maintaining the chronological order.With \u03b1 < 1, the time spent updating S and F shows a similar behavior until around session 1600, and then it starts to deviate downward from the nonforgetting configuration.Furthermore, there seems to be a relation between values of \u03b1 and time gains-lower values require less time.The downsize of this experiment was the overall poor accuracy of the algorithm, with an average recall of 0.021 with \u03b1 = 1 and 0.01 with \u03b1 = 0.99.We believe this is caused by the removal of information (ratings <4) and low representativeness of the sample."
            ]
          }
        ]
      },
      {
        "header": "Conclusions",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "Conclusions",
            "paragraphs": [
              "We have implemented and evaluated the impact of forgetting mechanisms in nonincremental and incremental collaborative filtering algorithms.Our results suggest that nonincremental algorithms that use sliding windows, when compared to their nonforgetting versions using a growing window, reduce computational requirements while not negatively affecting-and in some situations improvingpredictive ability.Results also suggest that incremental algorithms benefit from the use of fading factors, although the fading factor approach has more subtle improvements in time requirements.It is also confirmed that incremental algorithms are more scalable than non-incremental algorithms.",
              "This work studies the impact of forgetting mechanisms in an abrupt change scenario.Our experimental results were limited with respect to gradual drifts, either due to the data sets we have used or due to limitations of our approach.In the future, we intend to further evaluate forgetting mechanisms under both abrupt and gradual drifts.This will require researching how forgetting mechanisms relate to dataset intrinsic characteristics.A better understanding of datasets will allow the implementation of algorithms that are able to automatically adjust forgetting parameters to different situations.This will allow the implementation of dynamic forgetting, only when useful.We are also working on the implementation of the algorithms to allow larger scale experiments and the use of fading factors more sporadicallyevery k sessions.",
              "PONORTE and by the ERDF-European Regional Development Fund through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT-Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia (Portuguese Foundation for Science and Technology) within project \u00abFCOMP-01-0124-FEDER-022701\u00bb."
            ]
          },
          {
            "header": "Fig. 1",
            "paragraphs": [
              "Fig. 1 Session weights at 500th update with fading factors"
            ]
          },
          {
            "header": "Algorithm 3",
            "paragraphs": [
              "UBFF Input: L, Nrecs, \u03b1, n Output: recommendation list -Initialize D with sessions s i \u2208 L and weights set to 1: D = { s 1 , 1 , . . ., s n , 1 } (W D denotes the weights in D) -Initialize matrices S and F using {s 1 , . . ., s n } \u2282 L -For each new session s a \u2208 L, a > n (by user u a ) -Update D, S and F : -Let I a be the set of items in session s a -Multiply all values in S and F and past session weights in D by fading factor \u03b1: S = \u03b1S, F = \u03b1F, W D = \u03b1W D (5) -Add s a to D with weight set to 1: D a = s a , 1 -If u a is a new user, add a row and a column to F and to S -Update the row/column of F corresponding to u a , using the new D:"
            ]
          },
          {
            "header": "Fig. 2",
            "paragraphs": [
              "Fig. 2 Predictive ability of IBSW and IBFF with ART2.A moving average (n = 40) is used to smoothen the lines.For IBSW, wp represents the percentage of sessions used as learning data.The vertical dashed line indicates the point where change occurs"
            ]
          },
          {
            "header": "Fig. 3",
            "paragraphs": [
              "Fig. 3 Comparison between recall of UBSW and UBGW with w = 0.2 and 50 % drift.A moving average (n = 40) is used to smoothen the lines.The vertical dashed line marks the point where changes are introduced"
            ]
          },
          {
            "header": "Fig. 4",
            "paragraphs": [
              "Fig. 4 Comparison between recall of IBSW and IBGW with w p = 0.2 and 50 % drift.A moving average (n = 40) is used to smoothen the lines.The vertical dashed line marks the point where changes are introduced"
            ]
          },
          {
            "header": "Fig. 5",
            "paragraphs": [
              "Fig. 5 Recall of UBFF with 50 % drift.A moving average (n = 40) is used to smoothen the lines.The vertical dashed line marks the point where changes are introduced"
            ]
          },
          {
            "header": "Fig. 6",
            "paragraphs": [
              "Fig. 6 Recall of IBFF with 50 % drift.A moving average (n = 40) is used to smoothen the lines.The vertical dashed line marks the point where changes are introduced"
            ]
          },
          {
            "header": "Fig. 8",
            "paragraphs": [
              "Fig. 8 Matrix rebuild time with nonincremental algorithms (MUSIC dataset).A moving average (n = 40) is used to smoothen the lines"
            ]
          },
          {
            "header": "Fig. 9",
            "paragraphs": [
              "Fig. 9 Matrix update time with UBFF.A moving average (n = 40) is used to smoothen the lines"
            ]
          },
          {
            "header": "Fig. 10 Fig. 11",
            "paragraphs": [
              "Fig. 10 Matrix update time with IBFF.A moving average (n = 40) is used to smoothen the lines"
            ]
          },
          {
            "header": "Table 1",
            "paragraphs": [
              "Description of the datasets used"
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "Discussion",
      "papers_cited_discussion": [
        "44515378"
      ],
      "subsections": [
        {
          "header": "Discussion",
          "paragraphs": []
        },
        {
          "header": "Predictive ability",
          "paragraphs": [
            "Using the ELEARN dataset, forgetting mechanisms provide clear improvements in recall immediately after a sudden drift, except with IBFF, where slight improvements are only obtained with high fading factors (slow forgetting).However, the same is not observable in any case with the MUSIC dataset.With this dataset, immediately after the occurrence of a drift, improvements are not observed, but relative degradation is also not present.This contradicts results obtained with synthesized datasets that suggest a better capability to adapt to sudden drifts.A number of factors can influence the behavior of the algorithms, namely the presence of natural drifts and the natural variability of the datasets.This motivates further research to understand how dataset inherent features such as natural variability and the occurrence of sudden and gradual drifts relate to forgetting parameters such as window length and fading factor values."
          ]
        },
        {
          "header": "Update time",
          "paragraphs": [
            "With nonincremental algorithms (UBSW/UBGW and IBSW/IBGW), the first observation is that the sliding window algorithms tend to maintain an approximately constant time to rebuild the matrix, while with growing window algorithms time increases throughout the experiment.This is a natural consequence of the use of fixed length windows.The number of sessions to process, in the case of UBSW and IBSW, is fixed, which leads to approximately constant time.Gains in scalability are the most evident motivation for the use of sliding windows.",
            "Also, with nonincremental algorithms, it is noticeable that time is steadier with the user-based versions than with the item-based versions.One explanation for this is that both datasets have a sequential unique session per user, which Time with the use of fading factors in incremental algorithms seems to be unaffected with ELEARN and MU-SIC, but using a longer dataset, it becomes more evident that fading factors also have the potential to improve scalability.With fading factors, older similarity values tend to zero, but it takes a large amount of sessions for similarities to actually become zero.The package spam optimizes sparse matrix storage by storing only values greater than 2.220446 \u00d7 10 \u221216 {{44515378}}.For example, with \u03b1 = 0.97, a similarity value of 0.5, if never updated by recent data, only gets completely \"forgotten\" (i.e., becomes zero) after 1,170 sessions, which is more than the number of sessions in both ELEARN and MUSIC.",
            "User-based algorithms produce smaller matrices when the number of users is lower than the number of items and larger matrices when the number of users exceeds the number of items.Time to rebuild and/or update these matrices is directly affected by the amount of data to process.This explains why user-based algorithms perform better with the MUSIC dataset, while item-based algorithms have better results with ELEARN."
          ]
        }
      ]
    },
    "discussion_txt": "Using the ELEARN dataset, forgetting mechanisms provide clear improvements in recall immediately after a sudden drift, except with IBFF, where slight improvements are only obtained with high fading factors (slow forgetting).However, the same is not observable in any case with the MUSIC dataset.With this dataset, immediately after the occurrence of a drift, improvements are not observed, but relative degradation is also not present.This contradicts results obtained with synthesized datasets that suggest a better capability to adapt to sudden drifts.A number of factors can influence the behavior of the algorithms, namely the presence of natural drifts and the natural variability of the datasets.This motivates further research to understand how dataset inherent features such as natural variability and the occurrence of sudden and gradual drifts relate to forgetting parameters such as window length and fading factor values.With nonincremental algorithms (UBSW/UBGW and IBSW/IBGW), the first observation is that the sliding window algorithms tend to maintain an approximately constant time to rebuild the matrix, while with growing window algorithms time increases throughout the experiment.This is a natural consequence of the use of fixed length windows.The number of sessions to process, in the case of UBSW and IBSW, is fixed, which leads to approximately constant time.Gains in scalability are the most evident motivation for the use of sliding windows.Also, with nonincremental algorithms, it is noticeable that time is steadier with the user-based versions than with the item-based versions.One explanation for this is that both datasets have a sequential unique session per user, which Time with the use of fading factors in incremental algorithms seems to be unaffected with ELEARN and MU-SIC, but using a longer dataset, it becomes more evident that fading factors also have the potential to improve scalability.With fading factors, older similarity values tend to zero, but it takes a large amount of sessions for similarities to actually become zero.The package spam optimizes sparse matrix storage by storing only values greater than 2.220446 \u00d7 10 \u221216 {{44515378}}.For example, with \u03b1 = 0.97, a similarity value of 0.5, if never updated by recent data, only gets completely \"forgotten\" (i.e., becomes zero) after 1,170 sessions, which is more than the number of sessions in both ELEARN and MUSIC.User-based algorithms produce smaller matrices when the number of users is lower than the number of items and larger matrices when the number of users exceeds the number of items.Time to rebuild and/or update these matrices is directly affected by the amount of data to process.This explains why user-based algorithms perform better with the MUSIC dataset, while item-based algorithms have better results with ELEARN.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "As a scientific research assistant, I would like to provide a brief discussion on the paper titled \"Forgetting Mechanisms for Collaborative Filtering with Implicit Binary Ratings\" by [Author's Name]. The paper proposes two forgetting mechanisms, sliding windows and fading factors, to address the challenge of dealing with old and potentially obsolete data in collaborative filtering (CF) algorithms.",
          "",
          "The authors evaluate the impact of these mechanisms on user-based and item-based CF algorithms with implicit binary ratings under a scenario of abrupt change. Their results indicate that forgetting mechanisms can reduce the time and space requirements of the algorithms, improving their scalability, without significantly affecting their predictive ability.",
          "",
          "The use of forgetting mechanisms in CF is a promising approach to address the challenge of dealing with old data. The sliding windows mechanism involves replacing a portion of the historical data with new data, while the fading factors mechanism gradually reduces the weight of older data in the recommendation process. By incorporating these mechanisms into CF algorithms, the authors demonstrate that the algorithms can adapt to changes in user preferences and item popularity more efficiently.",
          "",
          "One of the key findings of the study is that the forgetting mechanisms do not significantly affect the predictive ability of the CF algorithms. This suggests that the mechanisms are effective in reducing the impact of obsolete data without compromising the accuracy of the recommendations. The authors also observe that the sliding windows mechanism is more effective than the fading factors mechanism in reducing the time and space requirements of the algorithms.",
          "",
          "Overall, the paper provides valuable insights into the use of forgetting mechanisms in CF algorithms. The findings suggest that incorporating forgetting mechanisms into CF can improve the scalability of the algorithms without significantly affecting their predictive ability. However, further research is needed to explore the applicability of these mechanisms in real-world scenarios and to compare the performance of different forgetting mechanisms.",
          "",
          "In conclusion, the paper provides a valuable contribution to the field of CF by proposing and evaluating two forgetting mechanisms for improving the scalability of CF algorithms. The findings suggest that incorporating forgetting mechanisms into CF can be an effective approach to address the challenge of dealing with old and potentially obsolete data, while maintaining the accuracy of the recommendations."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.5518767833709717,
            "R": 0.6102322340011597,
            "F1": 0.5795893669128418
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.4866367280483246,
            "R": 0.5299921631813049,
            "F1": 0.5073899626731873
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          "I'll start with a discussion based on the abstract, and then you can correct/clarify"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.3068491220474243,
            "R": 0.4386112093925476,
            "F1": 0.36108553409576416
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.2949580252170563,
            "R": 0.48265087604522705,
            "F1": 0.3661525845527649
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "]",
          "",
          "Forgetting mechanisms can be applied to both user-based and item-based collaborative filtering algorithms to mitigate the"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.3403027355670929,
            "R": 0.5912959575653076,
            "F1": 0.43198782205581665
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.33663809299468994,
            "R": 0.5380585789680481,
            "F1": 0.41415733098983765
          }
        ]
      }
    ]
  },
  {
    "corpus_id": "10060157",
    "title": "Effectiveness of biomarker-based exclusion of ventilator-acquired pneumonia to reduce antibiotic use (VAPrapid-2): study protocol for a randomised controlled trial",
    "externalids": {
      "ACL": "",
      "ArXiv": "",
      "CorpusId": "10037358",
      "DBLP": "",
      "DOI": "10.1186/s13063-016-1442-x",
      "MAG": "2473068687",
      "PubMed": "27422026",
      "PubMedCentral": "4947254"
    },
    "year": "2016",
    "level": "section",
    "sections": [
      {
        "header": "X",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "X",
            "paragraphs": [
              "Neurons and their target cells appear to interact in a complex reciprocal manner. These interactions have been most readily established in the neuromuscular system, where denervation results in numerous changes in muscle cell physiology {{45013459}}{{not_in_s2orc}}, in the visual system, where spatial and temporal relationships influence retinotectal innervation patterns {{36539243}}{{not_in_s2orc}}, and in the sympathetic nervous system, where the postganglionic axonal pathways appear to subserve bidirectionally significant regu-latory processes {{not_in_s2orc}}{{30230624}}{{43891205}}. While these types of interactions may be mediated by neurotransmitters such as acetylcholine, or by larger molecules, such as nerve growth factor, the constraints imposed in vivo by the close physical association between cells whose interactions are demonstrable, militate against characterization of the trophic molecules involved. In contrast, the study of interacting cell systems in vitro obviates this difficulty and, as a result, a variety of techniques-circumfu- {{81726630}} TrtE JOURNAL OF CELL BIOLOGY 9 VOLUME 74, 1977 9 pages {{81726630}}{{84653251}}{{19178054}}{{not_in_s2orc}}{{43891205}}{{4148984}}{{45817370}}{{not_in_s2orc}}{{not_in_s2orc}}{{not_in_s2orc}}{{not_in_s2orc}}{{not_in_s2orc}}{{not_in_s2orc}}{{not_in_s2orc}} sion {{84653251}}, extract addition {{not_in_s2orc}}{{11925495}}{{24070142}}, and coculture {{20829354}}{{85966459}}{{4148984}}-have been successfully applied to cultured cells. The objectives of such approaches have included the identification of significant interactions between different cell types and the characterization of the molecular events mediating these interactions. Because of the possibility that responses observed in vitro result from requirements peculiar to the cell culture conditions, the ultimate significance of such studies may be derived from an extension of these observations, once defined, to in vivo systems.",
              "Using the co-culture approach to study the interactions between murine spinal cord (SC) cells and skeletal muscle, we have observed (4) an increase with time in the number of detectable synaptic interactions between nerve and muscle cells and marked increases in the activity of choline acetyltransferase (CAT, EC 2.3.1.6), the enzyme which mediates the synthesis of the neuromuscular transmitter, acetylcholine. The increased CAT activity was shown not to be due to a general facilitation of cell survival since the activity of the four other enzymes assayed as well as total culture protein and DNA were relatively unaffected by the co-culture conditions. The survival of morphologically identified presumptive neurons was similar whether the cells were grown alone or with muscle {{22465705}}. The objective of the present study was to examine this interaction between cultured SC and muscle in terms of (a) the quantitative relationships between muscle and nerve, (b) the effects of co-culture on another neurotransmitterrelated enzyme, glutamic acid decarboxylase (GAD), and (c) the stimulation of a similar increase in SC cell CAT activity by a factor(s) released by muscle into its culture medium."
            ]
          }
        ]
      },
      {
        "header": "MATERIALS AND METHODS",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "MATERIALS AND METHODS",
            "paragraphs": []
          },
          {
            "header": "Cultures",
            "paragraphs": [
              "Pregnant C57B1/6 mice were obtained from the National Institutes of Health colony or from Microbiological Associates (Bethesda, Md.). SC cultures were produced from mouse embryos at 12-14 days in utero; heart (atrium) and skeletal muscle cell cultures were made from term embryos. The adult mice were anesthetized with CO2 or chloroform and sacrificed by cervical dislocation. Uteri containing the embryos were removed, placed in sterile 60-ram plastic petri dishes (Falcon) containing DISGH, a mixture of 50 ml of D1 concentrate (Colorado Serum Co., Denver, Colo.), 20 g sucrose, and 1 g glucose with 10 mM N-hydroxyethylpiperazine N'-2-ethane sulfonic acid (HEPES, Calbiochem, San Diego, Calif.) in 1 liter (320-330 mosmol, adjusted to pH 7.2 with NaOH). The embryos were removed under a dissecting microscope and transferred to a fresh dish containing DISGH. The desired tissue was quickly dissected, washed in fresh DISGH, and transferred to an empty 35-ram plastic petri dish. Unless otherwise stated, about one-half of the meninges and dorsal root ganglia (DRGs) were left attached to the spinal cords. Complete hindlimbs minus skin, bone, and feet were used for skeletal muscle cultures. The tissue from either source was finely minced with iridectomy scissors and treated at 37~ for 20-30 min (SC) or 40 min (muscle) with 0.25% crude trypsin (Grand Island Biological Co., Grand Island, N. Y.) in DISGH. The tissue pieces from four or five cords were transferred to a tube containing 1.5 ml of complete medium (see below) and disrupted by trituration (about 10 passages through a 9-inch transfer pipette, the tip bore of which had been narrowed by flaming). In some experiments, minced SC tissue was dissociated by trituration alone. The supernatant fluid with cells in suspension was removed and replaced with 1 ml of fresh complete medium and the remaining undissociated tissue was triturated again. This process was repeated until the combined cell suspensions contained 1 ml of medium per SC, heart, or hindlimb.",
              "The hindlimb suspension was enriched for myoblasts by preplating for 20 min in 60-mm diameter plastic culture dishes. Unattached cells were then filtered through 130-tzm-pore nylon mesh (Nitex, Silk Screen Supplies, Inc., Brooklyn, N. Y.) to remove undissociated tissue. At this stage of preparation the tissue suspensions contained predominantly single cells with rare 2-to 10-cell dumps. The cells were more than 90% viable as determined by nigrosin stain exclusion. Except as noted in the individual experiments, 3 \u2022 10 e SC cells or 0.7-0.8 x 108 skeletal muscle cells were inoculated per 60-mm dish, while 1 x 10 e SC cells or 0.25 x l0 s muscle cells were plated on 35-ram dishes. Heart cells were not counted; 3 ml of the suspension (equivalent to three embryonic hearts) were plated per 60-mm dish. All culture dishes were coated with a thin, air-dried film of acid-soluble, calf skin collagen (A grade, Caibiochem).",
              "Myoblast-flbroblast cultures were prepared from muscle cultures by multiple subculturings (using trypsin) to remove the myotubes. In some experiments, cells from established muscle cultures were added to SC cultures or pooled to produce cultures with more myotubes per dish. For these experiments, the myotube cultures were removed from the dishes by incubation for I/e h at 37~ in DISGH containing 10% heat-inactivated horse serum and 1 mg/ml collagenase (Worthington Biochemical Corp., Freehold, N. J., 449 U/mg). This procedure, in contrast to subculture with trypsin, did not destroy the myotubes. All cultures used for a given experiment came from the same cell suspension."
            ]
          },
          {
            "header": "Medium",
            "paragraphs": [
              "The medium used for a given set of cultures was either DMEM: the high glucose formation (Grand Island Biological Co.) of the Dulbecco-Vogt modification of Ea-GILLER ET AL. Choline Acetyltransferase Activity of Spinal Cord Cell Cultures gle's medium with 10% heat-inactivated (56~ for 30 min) horse serum (selected lots from Grand Island Biological Co. or Colorado Serum Co.), or MEM: Eagle's minimal essential medium with final concentrations of 30.3 mM glucose and 44 mM NaHCO3 similarly supplemented with horse serum. The media also included 10% fetal calf serum (Grand Island Biological Co. or Colorado Serum Co.) for the first 6 days of culture. The cultures received 20 ~g/ml of 5-fluoro-2'-deoxyuridine (FUdR) (a gift from Hoffman-LaRoche) and 50 p.g/ml uridine from day 6 to day 8 to prevent overgrowth by background cells. The culture medium was changed every 2-3 days. The cultures were maintained at 36~ in a 10% CO~-90% air, water-saturated atmosphere. Muscle cell cultures were 10 days-2 wk old before nerve ceils were added, and the medium of the muscle cultures was changed immediately before the addition of neuronal cells.",
              "Conditioned medium (CM) was taken from the appropriate cultures after 2-3 days of incubation and transferred to the experimental cultures. The CM was left on the recipient cultures until the next medium change, when the process was repeated. In some experiments, muscle CM was dialyzed at 6~ for 36 h (three changes) against 12 vol of medium with serum and then filtered through a 0.2-t~m-pore diameter membrane (Nalge Co., Rochester, N. Y.) before use on SC cell cultures. To estimate the molecular weight of the active factor, we filtered muscle CM through Diaflow XM-50 membrane filters under nitrogen pressure. The filtrate and an equal volume of CM concentrated above the filter were tested for activity on SC cell cultures."
            ]
          },
          {
            "header": "Harvest and Assay",
            "paragraphs": [
              "At appropriate intervals after their initiation, the cultures were rinsed 3 x with Hanks' balanced salt solution and stored at -70~ until harvest. Cells were scraped from dishes, pooled in some instances, sonicated, and diluted for radiochemical assay of CAT and GAD activities {{19178054}}. Enzyme activities were determined in triplicate. Protein was assayed according to a modification of the method of Lowry et al. {{677129}}. Both Nomarski differential interference and phase contrast optics were employed for photomicrography of SC cells grown on collagen-coated glass cover slips."
            ]
          }
        ]
      },
      {
        "header": "RESULTS",
        "papers_cited_discussion": null,
        "subsections": [
          {
            "header": "RESULTS",
            "paragraphs": []
          },
          {
            "header": "SC and Muscle Cell Cultures",
            "paragraphs": [
              "Cell cultures of dissociated mouse hindlimb muscle closely resembled those obtained from chick {{24682673}}{{not_in_s2orc}} and rat {{40389102}}{{45817370}}. Myoblast division and fusion resulted in the development of multinucleate myotubes within a few days of inoculation. Spontaneous contractile activity was seen by day 7. The myotubes rested on a layer of flat undiffer-entiated cells which continued to divide until the time of FUdR treatment.",
              "Cells cultures of dissociated mouse SC contained phase-bright neuronal cells with extensively branching processes. The neurons could be grouped into three broad morphologic categories: (a) large multipolar cells (Fig. 1 a and b and Fig.  2); (b) large, spherical, often pseudo-unipolar cells (Fig. 1 c) which were observed only when the dorsal sensory ganglia were included in the culture; and (c) small, less well-defined cells which were often observed in clusters (Fig. l b). The cultures also contained a variety of other cell types, including fibroblast-like cells in a basal layer, phase-dark cells with extensive branching processes ( Fig. 1 d), and other less common morphologic types. The development and physiologic characteristics of SC cultures, similar to those used in these experiments, were previously described in work from this laboratory {{1501785}}{{81726630}}. Neurons in these cultures have been found to innervate skeletal muscle. Quite similar morphologic and enzymatic characteristics were observed in cultures from the same inoculum of pooled, dissociated spinal cords. This reliability within each experiment was in contrast to the large variation in CAT activity of control SC cultures often observed between different cell preparations. The results in this report are accordingly presented as comparisons of the effects of various co-culture or CM treatments within groups of cultures obtained from the same inoculum."
            ]
          },
          {
            "header": "SC-Muscle Cell Co-cultures",
            "paragraphs": [
              "Morphological maturation of SC ceils during 2 wk of culture without muscle was accompanied by a moderate increase in CAT activity (Fig. 3). When replicate inocula of these SC ceils were cocultured with hindlimb muscle, a marked stimulation of CAT activity was apparent, beginning on the second day; at 14 days, the activity in coculture exceeded that of SC cultured alone by 20fold and the difference was still increasing. In other co-cultures the CAT activity continued to increase for at least 3 wk, the longest interval tested. These data suggest that both the timecourse of CAT development and the total activity achieved were affected by culture with skeletal muscle cells. Because of our inability to assess reliably the relative contribution of SC and muscle to the DNA or protein content of co-cultures, the activities in most instances are expressed per culture. This is justified because the range of total the increase continued at a slower rate in SC cultures but a marked decrease occurred in the cocultures. The C A T activity of these same co-cultures increased steadily through day 21, at which point it was 10-fold greater than in the SC cells cultured alone. This finding supported our previous conclusion (4) that there was considerable specificity in the stimulation of C A T activity when compared to that of other muscle and nervous system enzymes.",
              "Cells from dorsal sensory ganglia were included in most SC cultures because of the extra dissection time required to remove the ganglia which were closely associated with the spinal cords of 12-to 13-day embryos. In one experiment, however, these ganglia were carefully removed during the dissection and the SC cells from the spinal cords alone were cultured with or without muscle cells. After 21 days of culture, SC cell cultures contained C A T activities of 35 pmol/min per dish while the co-cultures contained activities of 498 pmol/min per dish. From this result we conclude that the presence of dorsal sensory ganglion cells is not necessary for the muscle-mediated stimulation of C A T activity.",
              "The data presented in Figs. 5 and 6 were oh-FIGURE 2 An optical section through a large motoneuron-like SC cell obtained with the Nomarski differential -500 interference technique. The culture was grown for 14 days on a collagen-coated glass cover slip. Note the intricate network of fine processes which course through ~ 400 the field. The neuronal nucleus is apparent in the plane of focus. Bar, 50 ttm.",
              "a. 300 C A T activity in replicate SC cell cultures was consistently less than \"*15% under similar treatment conditions and, as previously shown (4), normalization of the C A T activity to total culture protein or D N A confirmed the dramatic nature of the co-culture effect. In the experiment presented in Fig. 3, the cultures were also assayed for the activities of acetylcholinesterase, creatine phosphokinase, phosphoglucomutase, myokinase, and phosphorylase. As we have found previously (4), these enzyme activities were not substantially affected by the co-culture condition relative to the magnitude of the C A T activity increase.",
              "The effects of co-culture on SC cell C A T activity contrasted sharply with those on the activity of G A D (EC 4.1.1.15), the enzyme which mediates the synthesis of gamma-aminobutyric acid (Fig.  4). G A D activity increased at similar rates in control and co-cultures through day 7, after which  . Proportionality between dose and response was obtained when the inoculum was increased from 1 x l0 s to 2 x 106 cells. At lower SC cell densities, the increase in CAT activity was greater than proportional; above 2 \u2022 l0 s SC cells, the increase was markedly diminished. These data are consistent with the conclusion that the capacity of muscle cell cultures to stimulate SC cell CAT activity was saturable with SC cells. In these same cultures (Fig. 6), it was found that the capacity of muscle cells to decrease the GAD activity could also be affected by the dose of SC cells. The GAD activity of SC cells cultured alone increased with the number of cells inoculated. The co-cultures, at all SC cell doses except the lowest one, contained substantially lower GAD activity than the con-trols. It appeared that the ability of muscle cells to decrease GAD activity was limited and could be partially overcome by large inocula of SC cells.",
              "The contrasting results between the CAT and GAD activities in this experiment (Figs. 5 and 6) argue against nonspecific facilitation either of SC cell plating efficiency or of long-term survival as the source of the increase in co-culture CAT activity. The decrease in GAD activity during co-culture with muscle occurred in three of the five preparations in which GAD assays were performed. Analysis of the records from these five experiments revealed that GAD decreases were found in co-cultures for which SC cells were prepared from 12-to 13-day embryos, but not in cocultures which were prepared from 14-to 15-day embryonic spinal cords.",
              "The degree of stimulation of CAT activity, for replicate SC cell inocula, also varied with the dose of muscle cells used for co-culture (Fig. 7). The relationship was essentially linear when co-cultures were made in the usual manner by inoculation of SC cells onto muscle cultures of different densities (Fig. 7a). After 2 wk of co-culture, CAT activities in the cultures with the largest dose of muscle were 10-fold greater than in cultures without muscle. There was no evidence that the capacity for stimulation of CAT activity in these SC cells had been saturated by this amount of muscle. This muscle dose experiment was also done in a different way, with similar results. Myotubes were removed intact with collagenase treatment from 2wk-old muscle cultures and placed on SC cell cultures 2 days after SC cell inoculation. In such co-cultures, after 1-2 days, the myotubes lay heneath the SC cells and the cultures were indistinguishable from the standard co-cultures. The assays (Fig. 7b) showed that CAT activity was stimulated nearly 10-fold at the highest muscle dose, and that lesser doses of muscle gave less stimulation. Since SC cells normally were adherent to the dishes before day 2, when the myotubes were added, this experiment demonstrated that the CAT stimulation effect was not dependent upon an increase in initial plating efficiency of SC cells due to the presence of muscle cells."
            ]
          },
          {
            "header": "CM Effects",
            "paragraphs": [
              "The CAT activity of SC cell cultures could also be stimulated as much as 10-fold by chronic treatment with culture medium which had been conditioned by muscle cell cultures for 2-3 days. The data of Table I   of muscle CM and co-culture. The \"myoblastfibroblast\" cells of this experiment were obtained by repeated trypsinization and sub-culture of muscle cultures which had not been treated with FUdR; this resulted in the growth of morphologically undifferentiated cells (myoblasts or fibroblasts) in cultures devoid of mature myotubes. Medium conditioned by these cells caused the CAT activity of SC cell cultures to increase twofold, substantially less than the increase in CAT activity in replicate SC cell cultures which was produced by CM from muscle cultures of similar cell density. The extent to which these undifferentiated cells possessed the chemical characteristics of muscle and, alternatively, the degree to which cells and tissues other than muscle may affect SC cell CAT activity remain to be determined. In this experiment (Table I, Exp A), the SC cell GAD activity was not affected by treatment with CM, although co-culture of these SC cells with muscle resulted in substantially reduced GAD activity. The data on protein specific activities of CAT in this experiment and others presented here often indicate a lesser effect on CAT activity than do the data when expressed as enzyme activity per culture. This is due to the fact that the CM treatment often resulted in increases in the protein content of SC cell cultures. This was presumably related to the resumption of cell division by some cells as well as to a continued increase in cell size after the completion of FUdR treatment. Since neurons presumably did not increase in number after inoculation, increases in the protein content of control or experimental cultures appeared to reflect primarily an increase in the number of non-neuronal cells. SC cells co-cultured with skeletal muscle or cardiac muscle had a substantially increased CAT activity which was greater than that of SC cells cultured alone (Table I, Exp B). In these cultures, both GAD and acetylcholinesterase (ACHE) were relatively unaffected. When these SC cells were maintained for 8 days in muscle CM, both the total and specific activities of CAT were increased nearly threefold. No increases in GAD or AChE were detected in the SC cells treated with muscle CM. Growth of SC cells for 12 days in medium conditioned by other SC cultures resulted in increased CAT activity and small increases in GAD and ACHE. The use of different treatment intervals in this experiment makes difficult a direct comparison of the effectiveness of these CM sources, as data below will show that early application of CM is an important determinant in the degree of CAT activity stimulation. However, the CM data in this table show that ceils other than muscle cells probably can condition medium with a factor or factors which accomplish this stimulation.",
              "We found that the increase in CAT activity in SC cell-muscle co-cultures which were also treated with muscle CM was almost precisely equal to the sum of the increases produced by the two treatments administered separately (Table  II, Exp A). If the muscle CM and co-culture were acting to increase CAT activity by the same mechanism, such additivity might have been the result of failure to saturate that mechanism by either treatment. Alternatively, these data are also consistent with the possibility that the two treatments were working through different mechanisms. Since we have as yet been unable to saturate the effect of either CM or muscle coculture treatments, a common mechanism cannot be ruled out. Consistent with a similarity in the mode of action of these treatments was the finding that functional neuromuscular transmission was not required in co-cultures in order to obtain stimulation of CAT activity. Although we had previously shown that there was a temporal relationship between the formation of functional neuromuscular junctions and the development of CAT activity in co-cultures, we found (Table II, Exp B) that the increase in enzyme activity occurred equally well in co-cultures which were chronically maintained in (8.5 \u2022 10 -~ M) abungarotoxin, a snake venom polypeptide which binds to and blocks the muscle acetylcholine receptors. We found that this treatment blocked neuromuscular transmission in these cultures.",
              "The increase in CAT activity was dependent upon the concentration of CM with which the cultures were treated. When SC cells were treated with muscle CM diluted variably with fresh medium (Fig. 8), the increase in enzyme activity which resulted was linearly related to the dose of CM. As mentioned above, we have not yet been able to saturate the CM effect. The increase in CAT activity was also dependent on the length of treatment and the time after initiation of SC cultures at which the CM was first applied (Table III, Exp A). Cultures treated from day 3 to day 10 and given regular medium from day 11 to day 13 had lower CAT activity than did cultures treated with CM continuously from day 3 to day 13. Starting the CM treatment later, after culture inoculation, results in a substantial decrease in the responsiveness of the SC cells, as measured by the increase in CAT activity per day of treatment. Similarly, when either muscle cells or muscle CM was applied to SC cells after several weeks in culture (Table III, Exp B), no increase in CAT activity was detected above that of SC cells cultured alone.",
              "Attempts were made to determine some of the physical characteristics of the factor(s) present in muscle CM which was responsible for increased SC cell CAT activity. Since the horse serum present in the culture medium was routinely heated to 58~ before its use, this temperature was selected to test the thermal stability of the factor(s) in muscle CM. A 20-min treatment of muscle  {{not_in_s2orc}} Effect of muscle cell density on CAT activity increase. In each experiment the amount of muscle in the dishes was calculated by subtracting the total protein in control dishes (no muscle) from that of the combined cultures. Panel a: culture dishes (60 mm) were inoculated with varying amounts of muscle inoculum and grown to myotubes for 9 days before SC cells (0.95 x 105 per cm 2 surface area) were added. The combined cultures, as well as SC and muscle alone controls, were grown for 21 days more and then harvested and assayed for CAT and protein content. Protein content of the SC control dish was 0.60 mg of protein. Each datum represents a single homogenate made from the contents of a single dish. Panel b: mouse myotubes, after 2 wk of culture were removed from dishes by treatment with collagenase, and reinoculated in varying amounts onto 60-mm dishes which contained 2-day-old cultures of SC cells (1.4 \u2022 105 cells/cm z surface area) from 14-day mouse embryos. Combination cultures were then grown for an additional 12 days, harvested, and assayed for CAT and GAD activities and protein content of the dishes. In this experiment GAD activity was not decreased in cultures with muscle. Each datum represents the activity in one homogenate made from the contents of three replicate dishes. The SC control dishes contained an average of 0.64 mg of protein.",
              "CM at that temperature before its application to SC cell cultures did not destroy the activity (Table IV, Exp A); 69% of the effect obtained with unheated CM was still present after heating. Myoblast-fibroblast CM (data not shown) gave similar results. Dialysis of muscle CM for 48 h against three changes of fresh medium also failed to reduce substantially its effectiveness relative to otherwise similarly treated, but nondialyzed, muscle CM (Table IV, Exp B). In this experiment, both the dialyzed and nondialyzed CM were filtered through a membrane with a pore size of 0.2/.tin to ensure postdialysis sterility. We have observed in other experiments that such filtration reduced the effectiveness of the CM for increasing CAT activity; this effect was most significant when the CM was obtained from very dense muscle cultures.",
              "In the experiments reported to this point, SC cultures had been maintained for 11-21 days with and without several changes of CM. Although this approach produced impressive differences between control and CM-treated cultures, it represented a slow, expensive, and inefficient assay method for continued studies of the factor(s) in CM. To obtain a more efficient experimental procedure for bioassay of muscle CM, we began to assay the SC cells after one treatment of 4 days with or without CM. Thus, the data in Table IV show much smaller increases than do those obtained in previous experiments. Using this more rapid assay method, we found that CM which would produce an increase in CAT activity could be obtained from muscle cultures maintained in the absence of horse serum. We had previously determined that horse serum could be omitted from muscle culture medium for at least   Alone  34  -57  On muscle cells  557  16  229  On muscle cells +  515  15 238 a-Btx * SC cells, cultured as described in Materials and Methods, were harvested after 14 days of culture, and each datum represents a determination, in triplicate, of the activity from pooled contents of two cultures in 60-mm dishes. Bungarotoxin (a-Btx) in Exp B was included in the culture medium at 8.5 \u2022 10 -6 M.",
              "tioning, resulted in a 50% increase in the SC CAT activity per culture (Table IV, Exp C). This effect equalled that obtained with medium conditioned in the presence of serum and suggests that conditioning of the medium is not due to a direct detoxification of serum by muscle cell cultures. The fact that CM (no serum) was effective in the assay enabled us to fractionate such CM by filtration on membranes of known pore size without concomitant concentration of serum macromolecules. The muscle CM components which passed through a filter with an approximate molecular weight exclusion limit of 50,000 daltons were inactive in increasing SC cell CAT activity. The CM components which did not pass through this filter were approximately twofold concentrated for high molecular weight molecules and produced an increase in CAT activity per culture that was nearly twice that obtained with untreated muscle CM. The SC cells in this experiment were not treated with FUdR, in contrast to our normal protocol, and, as a result, nonneuronal cell division continued in all of these cultures. Cell growth was apparently stimulated by the CM treatments since the total protein content of these treated cultures was increased. Despite this increase in protein, the CAT specific activity was significantly higher in cultures treated once with CM concentrated for >50,000-dalton macromolecules than in cultures treated with CM from which the high molecular weight components had been removed. These results demonstrate the efficacy of both the single dose bioassay for muscle CM factor(s) and the fractionation Cultures were harvested on day 14 and assayed for CAT activity and protein. Each datum represents CAT activity per dish (circles; presented as equivalent activities for 60-mm diameter dishes) and per milligram of protein (squares) in single homogenates from the contents of 4, 2, 3, or 3 dishes containing 0, 25, 50, and 100% CM, respectively.",
              "of medium conditioned in the absence of serum as useful approaches to the identification of the trophic substance(s) released by muscle which increases CAT activity in SC cell cultures."
            ]
          }
        ]
      }
    ],
    "discussion": {
      "header": "DISCUSSION",
      "papers_cited_discussion": [
        "85966459"
      ],
      "subsections": [
        {
          "header": "DISCUSSION",
          "paragraphs": [
            "We have found that growth of SC cells with cultured muscle cells resulted in markedly increased CAT activity. We have previously re-ported (4) that co-culture of SC and muscle cells did not increase either the number of identifiable neurons or the activity of several other enzymes. Indeed, the transmitter-related enzyme, GAD, was decreased under some conditions of co-culture. The increase in SC cell CAT activity was directly dependent on the muscle and SC cell density in culture, as well as the duration of coculture. We did not obtain a saturation of the response by increasing the quantity of muscle in co-culture with SC. Muscle CM also increased SC cell CAT activity by as much as 10-fold within 3 wk, while it did not affect the activity of GAD or acetylcholinesterase. The CAT activity increase produced by muscle CM was directly dependent upon the CM concentration and the interval of exposure. The factor(s) in muscle CM responsible for increasing the SC cell CAT activity was heat stable at 58~ and nondialyzable; membrane filtration data were consistent with the factor's having a molecular weight greater than 50,000 daltons. It seems reasonable to suppose that muscle grown in co-culture conditioned the medium as did muscle grown alone; and, thus, a substantial part of the increase in CAT activity observed in co-cultures was probably produced by mechanisms similar to those in the experiments with CM. It is valid to ask whether any additional factors are provided by the contiguity of SC cells and muscle in co-cultures. Physiologically effective synaptic interaction between nerve and muscle is not involved in the increased CAT activity in co-cultures, since the increase was not affected by the presence of c~-bungarotoxin. Our experiments provide no evidence for any effect on CAT activity requiring some other aspect of nervemuscle contact, but such an effect cannot be excluded as a basis for some component of the increase seen with nerve and muscle co-cultures.",
            "The experiments with CM demonstrated that one or more macromolecules released by myogenic cells act to increase the CAT activity of SC cells. Several questions naturally arise with regard to this phenomenon. (a) Are myogenic cells the only source for the active molecules or can more than one tissue serve as a source? (b) Is CAT activity of the nerve cell culture affected in a relatively specific manner or is the effect a general one on culture growth? (c) Is the stimulation of CAT activity the effect of a single interaction between CM factor and recipient cell, or is it due to multiple small additive effects? (d) Does * SC cells were cultured alone in freshly prepared medium, in medium conditioned for 2-3 days by muscle cells, or co-cultured with muscle. Cultures were harvested at the end of the treatment intervals and the CAT activity was assayed independently in cells pooled from 2-4 dishes (35 mm) in Exp A (expressed as mean -+ SD) or from two 60mm culture dishes (Exp B). The increases in CAT activity were calculated as in Table I. :~ Differ from 3-13 at P > 0.02. w Differ from 3-13 at P > 0.001.",
            "a similar regulatory interaction occur in vivo?",
            "The present work provides some information with regard to the first two questions. We have found that medium conditioned by SC cells, differentiated skeletal muscle, and undifferentiated cells of myogenic origin increased CAT activity of SC cell cultures. Because of the differences between these cultures in cell type, size, and density, and because of the uncertainty as to the amount of effective protein secreted by the cultures into the medium during conditioning, it is difficult to assess the relative efficacy of the different cell cultures in the production of the CM factor. It seems clear, however, that skeletal muscle is not the only source of material that can produce an increase in CAT activity in SC cell cultures.",
            "The co-culture and CM effects exhibited considerable specificity with regard to the enzyme changes produced in the SC cultures. In the experiment in Fig. 3, we assayed for protein, DNA, CAT, acetylcholinesterase, creatine phosphokinase, phosphoglucomutase, myokinase, and phosphorylase. Of these, only CAT activity was substantially increased, while in that experiment and some others, GAD activity was reduced in co-cultures (Figs. 4 and 6 and Table I). When SC cells were treated with muscle CM, CAT activity was substantially increased while the activities of GAD and acetylcholinesterase were not. It is this specificity in enzyme response of SC cells to coculture or treatment with CM which distinguishes the interaction reported here from a relatively nonspecific promotion of cell survival {{85966459}}. The data on enzyme activities, total culture DNA, and culture protein indicate neither a substantial nonspecific increase in plating efficiency with coculture nor a facilitation of long-term survival with CM. The specific increase in CAT activity produced by both of these conditions indicates either that survival of a population of CAT-producing cells is selectively increased or that cell survival is unchanged and CAT production per cell is increased. Some combination of these processes might of course occur. The fact that CAT activity in co-culture continued to increase well after the activity in control cultures had become constant (Fig. 3 and Reference 4), argues against an increased survival in co-culture of a population of neurons in which the CAT activity increased at the same rate as that of surviving control cells. We previously reported that differences in the number of identifiable neuron-like cells were not detected between cultures of SC cells grown alone and SC grown with muscle, while 20-fold more CAT activity was present in the co-cultures (4). Thus, our results suggest that the increase in CAT activity that occurred in co-GILLER El' AL. Choline Acetyltransferase Activity of Spinal Cord Cell Cultures cultures resulted from an increase in the net production of this enzyme per cell. Until the cell of origin of the CAT activity is established, however, other mechanisms cannot be excluded. The specificity of the effect of both co-culture and CM on CAT activity suggests some relevance to trophic interactions in the neuromuscular or other cholinergic systems. Since this enzyme mediates the synthesis of acetylcholine at the synaptic junction between spinal motoneurons and skeletal muscle, the interaction in culture may reflect a significant neuromuscular regulatory process. The nature of the regulatory material(s) and its mechanism of action merit further investigation."
          ]
        },
        {
          "header": "FIGURE 1",
          "paragraphs": []
        },
        {
          "header": "FIGURE 4",
          "paragraphs": []
        },
        {
          "header": "FIGURE 5",
          "paragraphs": []
        },
        {
          "header": "F~GUl~ 6",
          "paragraphs": []
        },
        {
          "header": "FIGURE 8",
          "paragraphs": []
        },
        {
          "header": "TABLE I",
          "paragraphs": []
        },
        {
          "header": "TABLE II",
          "paragraphs": []
        },
        {
          "header": "TABLE III",
          "paragraphs": []
        },
        {
          "header": "TABLE IV",
          "paragraphs": []
        }
      ]
    },
    "discussion_txt": "We have found that growth of SC cells with cultured muscle cells resulted in markedly increased CAT activity. We have previously re-ported (4) that co-culture of SC and muscle cells did not increase either the number of identifiable neurons or the activity of several other enzymes. Indeed, the transmitter-related enzyme, GAD, was decreased under some conditions of co-culture. The increase in SC cell CAT activity was directly dependent on the muscle and SC cell density in culture, as well as the duration of coculture. We did not obtain a saturation of the response by increasing the quantity of muscle in co-culture with SC. Muscle CM also increased SC cell CAT activity by as much as 10-fold within 3 wk, while it did not affect the activity of GAD or acetylcholinesterase. The CAT activity increase produced by muscle CM was directly dependent upon the CM concentration and the interval of exposure. The factor(s) in muscle CM responsible for increasing the SC cell CAT activity was heat stable at 58~ and nondialyzable; membrane filtration data were consistent with the factor's having a molecular weight greater than 50,000 daltons. It seems reasonable to suppose that muscle grown in co-culture conditioned the medium as did muscle grown alone; and, thus, a substantial part of the increase in CAT activity observed in co-cultures was probably produced by mechanisms similar to those in the experiments with CM. It is valid to ask whether any additional factors are provided by the contiguity of SC cells and muscle in co-cultures. Physiologically effective synaptic interaction between nerve and muscle is not involved in the increased CAT activity in co-cultures, since the increase was not affected by the presence of c~-bungarotoxin. Our experiments provide no evidence for any effect on CAT activity requiring some other aspect of nervemuscle contact, but such an effect cannot be excluded as a basis for some component of the increase seen with nerve and muscle co-cultures.The experiments with CM demonstrated that one or more macromolecules released by myogenic cells act to increase the CAT activity of SC cells. Several questions naturally arise with regard to this phenomenon. (a) Are myogenic cells the only source for the active molecules or can more than one tissue serve as a source? (b) Is CAT activity of the nerve cell culture affected in a relatively specific manner or is the effect a general one on culture growth? (c) Is the stimulation of CAT activity the effect of a single interaction between CM factor and recipient cell, or is it due to multiple small additive effects? (d) Does * SC cells were cultured alone in freshly prepared medium, in medium conditioned for 2-3 days by muscle cells, or co-cultured with muscle. Cultures were harvested at the end of the treatment intervals and the CAT activity was assayed independently in cells pooled from 2-4 dishes (35 mm) in Exp A (expressed as mean -+ SD) or from two 60mm culture dishes (Exp B). The increases in CAT activity were calculated as in Table I. :~ Differ from 3-13 at P > 0.02. w Differ from 3-13 at P > 0.001.a similar regulatory interaction occur in vivo?The present work provides some information with regard to the first two questions. We have found that medium conditioned by SC cells, differentiated skeletal muscle, and undifferentiated cells of myogenic origin increased CAT activity of SC cell cultures. Because of the differences between these cultures in cell type, size, and density, and because of the uncertainty as to the amount of effective protein secreted by the cultures into the medium during conditioning, it is difficult to assess the relative efficacy of the different cell cultures in the production of the CM factor. It seems clear, however, that skeletal muscle is not the only source of material that can produce an increase in CAT activity in SC cell cultures.The co-culture and CM effects exhibited considerable specificity with regard to the enzyme changes produced in the SC cultures. In the experiment in Fig. 3, we assayed for protein, DNA, CAT, acetylcholinesterase, creatine phosphokinase, phosphoglucomutase, myokinase, and phosphorylase. Of these, only CAT activity was substantially increased, while in that experiment and some others, GAD activity was reduced in co-cultures (Figs. 4 and 6 and Table I). When SC cells were treated with muscle CM, CAT activity was substantially increased while the activities of GAD and acetylcholinesterase were not. It is this specificity in enzyme response of SC cells to coculture or treatment with CM which distinguishes the interaction reported here from a relatively nonspecific promotion of cell survival {{85966459}}. The data on enzyme activities, total culture DNA, and culture protein indicate neither a substantial nonspecific increase in plating efficiency with coculture nor a facilitation of long-term survival with CM. The specific increase in CAT activity produced by both of these conditions indicates either that survival of a population of CAT-producing cells is selectively increased or that cell survival is unchanged and CAT production per cell is increased. Some combination of these processes might of course occur. The fact that CAT activity in co-culture continued to increase well after the activity in control cultures had become constant (Fig. 3 and Reference 4), argues against an increased survival in co-culture of a population of neurons in which the CAT activity increased at the same rate as that of surviving control cells. We previously reported that differences in the number of identifiable neuron-like cells were not detected between cultures of SC cells grown alone and SC grown with muscle, while 20-fold more CAT activity was present in the co-cultures (4). Thus, our results suggest that the increase in CAT activity that occurred in co-GILLER El' AL. Choline Acetyltransferase Activity of Spinal Cord Cell Cultures cultures resulted from an increase in the net production of this enzyme per cell. Until the cell of origin of the CAT activity is established, however, other mechanisms cannot be excluded. The specificity of the effect of both co-culture and CM on CAT activity suggests some relevance to trophic interactions in the neuromuscular or other cholinergic systems. Since this enzyme mediates the synthesis of acetylcholine at the synaptic junction between spinal motoneurons and skeletal muscle, the interaction in culture may reflect a significant neuromuscular regulatory process. The nature of the regulatory material(s) and its mechanism of action merit further investigation.",
    "evaluations": [
      {
        "model": "meta-llama/Llama-2-7b-chat-hf",
        "llm_discussion": [
          "The abstract describes an experiment investigating the effect of muscle cells on the activity of choline acetyltransferase (CAT), an enzyme involved in the synthesis of the neurotransmitter acetylcholine, in spinal cord (SC) cells grown in culture. The study found that co-culture of SC cells with muscle cells for 2 weeks resulted in a 20-fold increase in CAT activity in SC cells. This increase was found to be directly related to the duration of co-culture and the cell density of both the SC and muscle cells, and was not affected by the presence of the acetylcholine receptor blocking agent, \u03b1-bungarotoxin. Additionally, the study found that glutamic acid decarboxylase (GAD) activity was decreased in SC-muscle cultures, while the activities of acetylcholinesterase and other enzymes were little changed. The study also found that conditioned medium (CM) from muscle cells, which had been exposed to serum or not, could increase CAT activity in SC cells, and that the active material(s) in CM had a molecular weight in excess of 50,000 daltons.",
          "",
          "Discussion:",
          "",
          "The results of this study suggest that muscle cells can significantly increase the activity of CAT in SC cells through the release of large molecular weight material. This increase in CAT activity is directly related to the duration of co-culture and the cell density of both the SC and muscle cells, indicating that there is a direct communication between these cell types. The finding that GAD activity is decreased in SC-muscle cultures while acetylcholinesterase activity is little changed suggests that the increase in CAT activity is not due to a general increase in cholinergic activity, but rather to the specific increase in acetylcholine synthesis.",
          "",
          "The use of conditioned medium from muscle cells to increase CAT activity in SC cells is an interesting observation, as it suggests that there may be specific molecules in muscle cells that are able to communicate with SC cells and increase their acetylcholine synthesis. The fact that the active material(s) in CM have a molecular weight in excess of 50,000 daltons suggests that these molecules are likely to be large and complex, and may include proteins or other macromolecules.",
          "",
          "The implications of these findings are significant, as they suggest that muscle cells may play a role in regulating neural activity in the spinal cord, and that this regulation may involve the release of specific molecules that can modulate acetylcholine synthesis. This could have important implications for our understanding of the neural mechanisms underlying muscle control and movement, and may lead to new strategies for the treatment of muscle-related disorders.",
          "",
          "However, it is important to note that this study was conducted in vitro, and further research is needed to determine whether similar effects occur in vivo. Additionally, the exact mechanism by which muscle cells increase CAT activity in SC cells is not fully understood, and further investigation is needed to determine the specific molecules involved and how they interact with SC cells.",
          "",
          "In conclusion, the present study provides evidence that muscle cells can increase the activity of CAT in SC cells through the release of large molecular weight material. This increase in CAT activity is directly related to the duration of co-culture and the cell density of both the SC and muscle cells, and may have important implications for our understanding of the neural mechanisms underlying muscle control and movement. Further research is needed to fully understand the mechanisms involved and to determine the potential therapeutic applications of these findings."
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.6586524248123169,
            "R": 0.6941017508506775,
            "F1": 0.675912618637085
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.614891529083252,
            "R": 0.6472861766815186,
            "F1": 0.6306731700897217
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-1B-Instruct",
        "llm_discussion": [
          ""
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.0,
            "R": 0.0,
            "F1": 0.0
          }
        ]
      },
      {
        "model": "meta-llama/Llama-3.2-3B-Instruct",
        "llm_discussion": [
          "The authors of this study suggest that large molecular weight material released by muscle cells is capable of increasing"
        ],
        "accuracy_scores": [
          {
            "model": "bert-base-uncased",
            "P": 0.36279791593551636,
            "R": 0.6315277814865112,
            "F1": 0.46084892749786377
          },
          {
            "model": "microsoft/deberta-large-mnli",
            "P": 0.292305052280426,
            "R": 0.5485765933990479,
            "F1": 0.3813894987106323
          }
        ]
      }
    ]
  }
]